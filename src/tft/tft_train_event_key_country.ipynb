{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c39ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import time\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.models import TiDEModel, NaiveMovingAverage, TFTModel, NHiTSModel, TSMixerModel\n",
    "from darts.metrics import mae, mse, smape\n",
    "from darts.utils.losses import MAELoss, MapeLoss, SmapeLoss\n",
    "import darts\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04aea7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # CuDNN 연산을 deterministic하게 만들어 주지만, 약간 느려질 수 있음\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 값 고정\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ec71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_darts_time_series_group(\n",
    "    dataset: pd.DataFrame,\n",
    "    target: Union[List[str],str],\n",
    "    time_col: str,\n",
    "    group_cols: Union[List[str],str],\n",
    "    static_cols: Union[List[str],str]=None,\n",
    "    past_cols: Union[List[str],str]=None,\n",
    "    future_cols: Union[List[str],str]=None,\n",
    "    freq: str=None,\n",
    "    encode_static_cov: bool=True,\n",
    ")-> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], List[TimeSeries]]:\n",
    "\n",
    "    series_raw = TimeSeries.from_group_dataframe(\n",
    "    dataset,\n",
    "    time_col    =   time_col,\n",
    "    group_cols  =   group_cols,  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "    static_cols =   static_cols,  # also extract these additional columns as static covariates (without grouping)\n",
    "    value_cols  =   target,  # optionally, specify the time varying columns\n",
    "    n_jobs      =   -1,\n",
    "    verbose     =   False,\n",
    "    freq        =   freq,\n",
    "    )\n",
    "\n",
    "    if encode_static_cov:\n",
    "        static_cov_transformer = StaticCovariatesTransformer()\n",
    "        series_encoded = static_cov_transformer.fit_transform(series_raw)\n",
    "    else: series_encoded = []\n",
    "\n",
    "    if past_cols:\n",
    "        past_cov = TimeSeries.from_group_dataframe(\n",
    "            dataset,\n",
    "            time_col    =   time_col,\n",
    "            group_cols  =   group_cols,\n",
    "            value_cols  =   past_cols,\n",
    "            n_jobs      =   -1,\n",
    "            verbose     =   False,\n",
    "            freq        =   freq,\n",
    "            )\n",
    "    else: past_cov = []\n",
    "\n",
    "    if future_cols:\n",
    "        future_cov = TimeSeries.from_group_dataframe(\n",
    "            dataset,\n",
    "            time_col    =   time_col,\n",
    "            group_cols  =   group_cols,\n",
    "            value_cols  =   future_cols,\n",
    "            n_jobs      =   -1,\n",
    "            verbose     =   False,\n",
    "            freq        =   freq,\n",
    "            )\n",
    "    else: future_cov = []\n",
    "\n",
    "    return series_raw, series_encoded, past_cov, future_cov\n",
    "\n",
    "def split_grouped_darts_time_series(\n",
    "    series: List[TimeSeries],\n",
    "    split_date: Union[str, pd.Timestamp],\n",
    "    min_date: Union[str, pd.Timestamp]=None,\n",
    "    max_date: Union[str, pd.Timestamp]=None,\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
    "\n",
    "    if min_date:\n",
    "       raw_series = series.copy()\n",
    "       series = []\n",
    "       for s in raw_series:\n",
    "        try: series.append(s.split_before(pd.Timestamp(min_date)-timedelta(1))[1])\n",
    "        except: series.append(s)\n",
    "\n",
    "    if max_date:\n",
    "       raw_series = series.copy()\n",
    "       series = []\n",
    "       for s in raw_series:\n",
    "        try: series.append(s.split_before(pd.Timestamp(max_date))[0])\n",
    "        except: series.append(s)\n",
    "\n",
    "    split_0 = [s.split_before(pd.Timestamp(split_date))[0] for s in series]\n",
    "    split_1 = [s.split_before(pd.Timestamp(split_date))[1] for s in series]\n",
    "    return split_0, split_1\n",
    "\n",
    "def eval_forecasts(\n",
    "    pred_series: Union[List[TimeSeries], TimeSeries],\n",
    "    test_series: Union[List[TimeSeries], TimeSeries],\n",
    "    error_metric: darts.metrics,\n",
    "    plot: bool=False\n",
    ") -> List[float]:\n",
    "\n",
    "    errors = error_metric(test_series, pred_series)\n",
    "    print(errors)\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.hist(errors, bins=50)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Error\")\n",
    "        plt.title(f\"Mean error: {np.mean(errors):.3f}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return errors\n",
    "\n",
    "def fit_mixed_covariates_model(\n",
    "    model_cls,\n",
    "    common_model_args: dict,\n",
    "    specific_model_args: dict,\n",
    "    model_name: str,\n",
    "    past_cov: Union[List[TimeSeries], TimeSeries],\n",
    "    future_cov: Union[List[TimeSeries], TimeSeries],\n",
    "    train_series: Union[List[TimeSeries], TimeSeries],\n",
    "    val_series: Union[List[TimeSeries], TimeSeries]=None,\n",
    "    max_samples_per_ts: int=None,\n",
    "    save:bool=False,\n",
    "    path:str=\"\",\n",
    "):\n",
    "\n",
    "    # Declarare model\n",
    "    model = model_cls(model_name=model_name,\n",
    "                    **common_model_args,\n",
    "                    **specific_model_args)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(\n",
    "                    # TRAIN ARGS ===================================\n",
    "                    series                = train_series,\n",
    "                    past_covariates       = past_cov,\n",
    "                    future_covariates     = future_cov,\n",
    "                    max_samples_per_ts    = max_samples_per_ts,\n",
    "                    # VAL ARGS ======================================\n",
    "                    val_series            = val_series,\n",
    "                    val_past_covariates   = past_cov,\n",
    "                    val_future_covariates = future_cov,\n",
    "                )\n",
    "\n",
    "    if save: model.save(path)\n",
    "\n",
    "def backtesting(model, series, past_cov, future_cov, start_date, horizon, stride):\n",
    "  historical_backtest = model.historical_forecasts(\n",
    "    series, past_cov, future_cov,\n",
    "    start=start_date,\n",
    "    forecast_horizon=horizon,\n",
    "    stride=stride,  # Predict every N months\n",
    "    retrain=False,  # Keep the model fixed (no retraining)\n",
    "    overlap_end=False,\n",
    "    last_points_only=False\n",
    "  )\n",
    "  maes = model.backtest(series, historical_forecasts=historical_backtest, metric=mae)\n",
    "\n",
    "  return np.mean(maes)\n",
    "\n",
    "def process_predictions(\n",
    "    preds: List[TimeSeries],\n",
    "    series_raw: List[TimeSeries],\n",
    "    group_cols: List[str]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    list_df = [serie.pd_dataframe() for serie in preds]\n",
    "    for i in range(len(list_df)):\n",
    "      list_df[i]['Date'] = preds[i].time_index\n",
    "      for j in range(len(group_cols)):\n",
    "        list_df[i][group_cols[j]] = series_raw[i].static_covariates[group_cols[j]].values[0]\n",
    "    processed_preds =  pd.concat(list_df, ignore_index=True)\n",
    "    return processed_preds\n",
    "\n",
    "def price_weighted_mae(predictions, targets, prices):\n",
    "    \"\"\"\n",
    "    Compute the price-weighted Mean Absolute Error (MAE).\n",
    "\n",
    "    :param predictions: A list or 1D NumPy array of predicted values.\n",
    "    :param targets: A list or 1D NumPy array of actual (ground truth) values.\n",
    "    :param prices: A list or 1D NumPy array of prices corresponding to the targets.\n",
    "    :return: The price-weighted MAE as a float.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    predictions = np.array(predictions, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "    prices = np.array(prices, dtype=np.float32)\n",
    "\n",
    "    # Compute absolute error\n",
    "    error = np.abs(targets - predictions)\n",
    "\n",
    "    # Compute price-weighted error\n",
    "    weighted_error = error * prices\n",
    "\n",
    "    # Compute and return the mean of the weighted error\n",
    "    return np.mean(weighted_error)\n",
    "\n",
    "def local_iqr_clip(series, window=30, q1=0.25, q3=0.75, m=2.5):\n",
    "    roll_q1 = series.rolling(window, center=True).quantile(q1)\n",
    "    roll_q3 = series.rolling(window, center=True).quantile(q3)\n",
    "    iqr = roll_q3 - roll_q1\n",
    "    upper = roll_q3 + m * iqr\n",
    "    return series.clip(0, upper)\n",
    "\n",
    "class MultiTaskLossModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskLossModule, self).__init__()\n",
    "        self.alpha = 1.0\n",
    "        self.beta = 1.0\n",
    "        self.gamma = 1.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # 내부에 바로 구현\n",
    "        reg_pred   = y_pred[..., :2]\n",
    "        reg_true   = y_true[..., :2]\n",
    "        # cls_pred   = y_pred[..., 2]\n",
    "        # cls_true   = y_true[..., 2]\n",
    "\n",
    "        loss_demand   = F.mse_loss(reg_pred[..., 0], reg_true[..., 0])\n",
    "        loss_discount = F.mse_loss(reg_pred[..., 1], reg_true[..., 1])\n",
    "        # loss_cls      = F.binary_cross_entropy_with_logits(cls_pred, cls_true)\n",
    "\n",
    "        return self.alpha*loss_demand + self.beta*loss_discount #+ self.gamma*loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686151eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventWeightedMultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Darts 프레임워크에 최적화된 이벤트 가중 멀티태스크 손실 함수.\n",
    "\n",
    "    이 손실 함수는 is_event 정보를 타겟 텐서의 일부로 받아,\n",
    "    이벤트 기간 동안의 수요(demand) 및 할인율(discount_pct) 예측 오차에\n",
    "    더 높은 페널티를 부여하도록 설계되었습니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, demand_weight: float = 1.0, discount_weight: float = 0.1, event_penalty: float = 500.0):\n",
    "        \"\"\"\n",
    "        손실 함수의 가중치를 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            demand_weight (float): 수요 예측 손실에 대한 기본 가중치.\n",
    "            discount_weight (float): 할인율 예측 손실에 대한 기본 가중치.\n",
    "            event_penalty (float): 이벤트 기간의 오차에 적용할 추가 페널티 배수.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.demand_weight = demand_weight\n",
    "        self.discount_weight = discount_weight\n",
    "        self.event_penalty = event_penalty\n",
    "        \n",
    "        # 이상치에 강건한 Huber Loss를 사용 (reduction='none'으로 설정)\n",
    "        # self.loss_fn = nn.HuberLoss(reduction='none')\n",
    "        self.loss_fn = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        가중 손실을 계산합니다.\n",
    "\n",
    "        Darts 모델 학습 시 이 함수가 호출되며, y_pred와 y_true는 \n",
    "        모델의 출력 차원과 타겟 시계열의 컴포넌트 수에 맞춰 전달됩니다.\n",
    "\n",
    "        Args:\n",
    "            y_pred (torch.Tensor): 모델의 예측값 텐서.\n",
    "                                   - y_pred[..., 0]: 예측된 demand\n",
    "                                   - y_pred[..., 1]: 예측된 discount_pct\n",
    "                                   - y_pred[..., 2]: (무시할) 예측된 is_event\n",
    "            y_true (torch.Tensor): 실제값 텐서.\n",
    "                                   - y_true[..., 0]: 실제 demand\n",
    "                                   - y_true[..., 1]: 실제 discount_pct\n",
    "                                   - y_true[..., 2]: 실제 is_event 플래그 (0 또는 1)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: 최종 계산된 스칼라 손실 값.\n",
    "        \"\"\"\n",
    "        # --- 1. 텐서에서 각 변수 분리 ---\n",
    "        pred_demand = y_pred[..., 0]\n",
    "        pred_discount = y_pred[..., 1]\n",
    "        # 모델의 is_event 예측값(y_pred[..., 2])은 사용하지 않으므로 무시합니다.\n",
    "\n",
    "        true_demand = y_true[..., 0]\n",
    "        true_discount = y_true[..., 1]\n",
    "        true_is_event_flag = y_true[..., 2]\n",
    "\n",
    "        # --- 2. 이벤트 가중치 텐서 생성 ---\n",
    "        # is_event가 1인 위치는 event_penalty 값을, 아닌 곳은 1.0 값을 가짐\n",
    "        weights = torch.ones_like(true_is_event_flag)\n",
    "        weights[true_is_event_flag == 1] = self.event_penalty\n",
    "\n",
    "        # --- 3. 각 태스크의 가중 손실 계산 ---\n",
    "        # 3.1. 수요(demand) 손실 계산\n",
    "        loss_demand_samples = self.loss_fn(pred_demand, true_demand)\n",
    "        weighted_demand_loss = (loss_demand_samples * weights).mean()\n",
    "\n",
    "        # 3.2. 할인율(discount) 손실 계산\n",
    "        loss_discount_samples = self.loss_fn(pred_discount, true_discount)\n",
    "        weighted_discount_loss = (loss_discount_samples * weights).mean()\n",
    "        \n",
    "        # --- 4. 최종 손실 조합 ---\n",
    "        total_loss = (self.demand_weight * weighted_demand_loss + \n",
    "                      self.discount_weight * weighted_discount_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc6f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATE = pd.Timestamp('2023-01-01')\n",
    "VAL_DATE_OUT = pd.Timestamp('2022-01-01')\n",
    "VAL_DATE_IN = pd.Timestamp('2021-01-01')\n",
    "# MIN_TRAIN_DATE = pd.Timestamp('2015-06-01')\n",
    "\n",
    "dataset = pd.read_csv(\"data/all_master.csv\", parse_dates=[\"date\"])\n",
    "dataset = dataset.sort_values(by=[\"city\", \"sku\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90033d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  6.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. 이벤트 기간 정의 (기간 및 미래 예측 시나리오)\n",
    "event_periods = {\n",
    "    2018: {\"country\": \"KOR\", \"start_date\": \"2018-02-05\", \"end_date\": \"2018-03-26\"},\n",
    "    2019: {\"country\": \"JPN\", \"start_date\": \"2019-01-09\", \"end_date\": \"2019-03-08\"},\n",
    "    2020: {\"country\": \"USA\", \"start_date\": \"2020-01-07\", \"end_date\": \"2020-04-28\"},\n",
    "    2021: {\"country\": \"USA\", \"start_date\": \"2021-02-19\", \"end_date\": \"2021-06-10\"},\n",
    "    2022: {\"country\": \"KOR\", \"start_date\": \"2022-01-17\", \"end_date\": \"2022-04-18\"},\n",
    "    # --- 미래 예측 시나리오 ---\n",
    "    2023: {\"country\": \"AUS\", \"start_date\": \"2023-07-25\", \"end_date\": \"2023-08-13\"},\n",
    "    2024: {\"country\": \"DEU\", \"start_date\": \"2024-02-13\", \"end_date\": \"2024-04-15\"}\n",
    "}\n",
    "\n",
    "# 3. 동적 피쳐(Dynamic Features) 생성\n",
    "# 컬럼 초기화\n",
    "dataset['is_event'] = 0\n",
    "dataset['days_since_event_start'] = -1\n",
    "dataset['event_countdown'] = -1\n",
    "dataset['event_progress_normalized'] = 0.0\n",
    "dataset['event_peak_proximity'] = 0.0\n",
    "\n",
    "# 이벤트 기간을 순회하며 피쳐 생성 (모든 sku, city에 일괄 적용)\n",
    "for year, event_info in tqdm(event_periods.items()):\n",
    "    country = event_info['country']\n",
    "    start_date = pd.to_datetime(event_info['start_date'])\n",
    "    end_date = pd.to_datetime(event_info['end_date'])\n",
    "\n",
    "    # 카운트다운 피쳐\n",
    "    countdown_start_date = start_date - pd.Timedelta(days=14)\n",
    "    countdown_mask = (\n",
    "        (dataset['country'] == country) &\n",
    "        (dataset['date'] >= countdown_start_date) &\n",
    "        (dataset['date'] < start_date)\n",
    "    )\n",
    "    dataset.loc[countdown_mask, 'event_countdown'] = (start_date - dataset.loc[countdown_mask, 'date']).dt.days\n",
    "\n",
    "    # 이벤트 기간 내 피쳐\n",
    "    event_mask = (\n",
    "        (dataset['country'] == country) &\n",
    "        (dataset['date'] >= start_date) &\n",
    "        (dataset['date'] <= end_date)\n",
    "    )\n",
    "    dataset.loc[event_mask, 'is_event'] = 1\n",
    "    dataset.loc[event_mask, 'days_since_event_start'] = (dataset.loc[event_mask, 'date'] - start_date).dt.days\n",
    "    \n",
    "    event_duration = (end_date - start_date).days\n",
    "    if event_duration > 0:\n",
    "        days_from_start = dataset.loc[event_mask, 'days_since_event_start']\n",
    "        dataset.loc[event_mask, 'event_progress_normalized'] = days_from_start / event_duration\n",
    "        \n",
    "        midpoint = event_duration / 2.0\n",
    "        dataset.loc[event_mask, 'event_peak_proximity'] = 1 - np.abs(days_from_start - midpoint) / midpoint\n",
    "\n",
    "\n",
    "# 4. 정적 피쳐(Static Features) 생성\n",
    "# 4.1. is_key_market 피쳐\n",
    "key_markets = ['KOR', 'USA', 'JPN']\n",
    "dataset['is_key_market'] = dataset['country'].apply(lambda x: 1 if x in key_markets else 0)\n",
    "\n",
    "# 4.2. event_responsiveness_score 피쳐 (국가별 집계 데이터로 계산 후 매핑)\n",
    "df_agg = pd.read_csv('aggregated_df.csv') # 국가별 집계 데이터 로드\n",
    "df_agg['date'] = pd.to_datetime(df_agg['date'])\n",
    "train_agg = df_agg[df_agg['date'] < '2023-01-01'].copy()\n",
    "train_agg['is_event'] = 0\n",
    "\n",
    "# 집계 데이터에 이벤트 기간 표시\n",
    "for year, event_info in event_periods.items():\n",
    "    if year < 2023:\n",
    "        event_mask_agg = (\n",
    "            (train_agg['country'] == event_info['country']) &\n",
    "            (train_agg['date'] >= event_info['start_date']) &\n",
    "            (train_agg['date'] <= event_info['end_date'])\n",
    "        )\n",
    "        train_agg.loc[event_mask_agg, 'is_event'] = 1\n",
    "\n",
    "# 국가별 반응성 점수 계산\n",
    "responsiveness = {}\n",
    "for country in dataset['country'].unique():\n",
    "    if country in key_markets:\n",
    "        event_demand = train_agg[(train_agg['country'] == country) & (train_agg['is_event'] == 1)]['demand'].mean()\n",
    "        non_event_demand = train_agg[(train_agg['country'] == country) & (train_agg['is_event'] == 0)]['demand'].mean()\n",
    "        responsiveness[country] = event_demand / non_event_demand if non_event_demand > 0 else 1.0\n",
    "    else:\n",
    "        responsiveness[country] = 1.0 # 이벤트가 없었던 국가는 1\n",
    "\n",
    "dataset['event_responsiveness_score'] = dataset['country'].map(responsiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "572f1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = 'none'\n",
    "# preprocess = 'iqr'\n",
    "dataset = pd.get_dummies(dataset, columns=['season'], prefix='season')\n",
    "# dataset = dataset[dataset['days_since_launch'] > 0]\n",
    "condition = (dataset['days_since_launch'] < 0) & (dataset['date'] < '2023-01-01')\n",
    "dataset.loc[condition, 'discount_pct'] = 0.0\n",
    "\n",
    "dataset['month'] = dataset['date'].dt.month\n",
    "dataset['day']   = dataset['date'].dt.day \n",
    "dataset['year']  = dataset['date'].dt.year\n",
    "dataset['day_of_week'] = dataset['date'].dt.dayofweek\n",
    "\n",
    "dataset['time_index'] = (dataset['date'] - pd.Timestamp('2018-01-01')).dt.days\n",
    "dataset['time_index'] = dataset['time_index'].astype(np.float32)\n",
    "\n",
    "\n",
    "target_col = ['demand', 'discount_pct', 'is_event' ]\n",
    "time_col = 'date'\n",
    "group_cols = ['sku','city']\n",
    "\n",
    "drop_cols = [\n",
    "    # static\n",
    "    # 'country', \n",
    "    'category', 'family', 'storage_gb', 'colour', \n",
    "    # numeric\n",
    "    # 'season_Fall', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday',\n",
    "    'avg_temp', 'humidity', 'precip_mm',\n",
    "    'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover',\n",
    "    'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode',\n",
    "    'shortwave_rad_MJ', 'vpd', 'cdd18', 'delta_temp',\n",
    "    'delta_humidity',\n",
    "]\n",
    "drop_cols = []\n",
    "# past_cols = ['EMA_30', 'MA_30']\n",
    "# future_cols <=> past_cols N-HiTS\n",
    "past_cols= []\n",
    "future_cols = ['season_Fall', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday','avg_temp', 'min_temp', 'max_temp', 'dewpoint', 'humidity', 'precip_mm',\n",
    "       'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover',\n",
    "       'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode',\n",
    "       'shortwave_rad_MJ', 'vpd', 'hdd18', 'cdd18', 'delta_temp',\n",
    "       'delta_humidity', 'brent_usd', 'local_fx', 'spend_usd', 'days_since_launch',\n",
    "       'month', 'day', \"year\", \"day_of_week\", \"time_index\",\n",
    "       'is_event', 'event_countdown', 'days_since_event_start', 'event_progress_normalized', 'event_peak_proximity',\n",
    "]\n",
    "static_cols = ['country', 'category', 'family', 'storage_gb', 'colour', 'unit_price', 'life_days', 'is_key_market', 'event_responsiveness_score']\n",
    "\n",
    "dataset = dataset.drop(columns=drop_cols)\n",
    "future_cols = [col for col in future_cols if col not in drop_cols]\n",
    "static_cols = [col for col in static_cols if col not in drop_cols]\n",
    "\n",
    "\n",
    "if preprocess == 'clip':\n",
    "    print('clip')\n",
    "    low, high = dataset['demand'].quantile([0.00,0.95])\n",
    "    dataset['demand'] = dataset['demand'].clip(low, high)\n",
    "elif preprocess == 'iqr':\n",
    "    print('iqr')\n",
    "    dataset['demand'] = local_iqr_clip(dataset['demand'])\n",
    "dataset['demand'] = np.log1p(dataset['demand'])\n",
    "dataset['discount_pct'] = dataset['discount_pct']/100\n",
    "\n",
    "series_raw, series, past_cov, future_cov = to_darts_time_series_group(\n",
    "    dataset=dataset,\n",
    "    target=target_col,\n",
    "    time_col=time_col,\n",
    "    group_cols=group_cols,\n",
    "    past_cols=past_cols,\n",
    "    future_cols=future_cols,\n",
    "    static_cols=static_cols,\n",
    "    freq='D', # daily\n",
    "    encode_static_cov=True, # so that the models can use the categorical variables (Agency & Product)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc90eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = split_grouped_darts_time_series(\n",
    "    series=series,\n",
    "    split_date=TEST_DATE\n",
    ")\n",
    "\n",
    "train, _ = split_grouped_darts_time_series(\n",
    "    series=train_val,\n",
    "    split_date=VAL_DATE_OUT\n",
    ")\n",
    "\n",
    "_, val = split_grouped_darts_time_series(\n",
    "    series=train_val,\n",
    "    split_date=VAL_DATE_IN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 100,\n",
    "    \"accelerator\": \"gpu\", \n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    # \"output_chunk_length\": 28,\n",
    "    # \"input_chunk_length\": 365,\n",
    "    \"output_chunk_length\": 7,\n",
    "    \"input_chunk_length\": 30,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.001},\n",
    " 'lr_scheduler_cls':ReduceLROnPlateau,   # <<<--- 사용할 스케줄러 클래스 직접 지정\n",
    " 'lr_scheduler_kwargs':{                 # <<<--- 스케줄러에 전달할 파라미터 딕셔너리\n",
    "     'monitor': 'val_loss',\n",
    "     'patience': 5,\n",
    "     'factor': 0.2, \n",
    "     'threshold': 1e-4,\n",
    " },   \n",
    " 'loss_fn': EventWeightedMultiTaskLoss(),\n",
    " 'use_layer_norm': True,\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2080d588",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m## COMMENT TO LOAD PRE-TRAINED MODEL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mfit_mixed_covariates_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTiDEModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecific_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTiDE_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train_series=train_val,\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# val_series=None,\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m time_tide = time.time() - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfit_mixed_covariates_model\u001b[39m\u001b[34m(model_cls, common_model_args, specific_model_args, model_name, past_cov, future_cov, train_series, val_series, max_samples_per_ts, save, path)\u001b[39m\n\u001b[32m    114\u001b[39m model = model_cls(model_name=model_name,\n\u001b[32m    115\u001b[39m                 **common_model_args,\n\u001b[32m    116\u001b[39m                 **specific_model_args)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TRAIN ARGS ===================================\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# VAL ARGS ======================================\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save: model.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:922\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\n\u001b[32m    815\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m     stride: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,\n\u001b[32m    832\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    833\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit/train the model on one or multiple series.\u001b[39;00m\n\u001b[32m    834\u001b[39m \n\u001b[32m    835\u001b[39m \u001b[33;03m    This method wraps around :func:`fit_from_dataset()`, constructing a default training\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    915\u001b[39m     (\n\u001b[32m    916\u001b[39m         (\n\u001b[32m    917\u001b[39m             series,\n\u001b[32m    918\u001b[39m             past_covariates,\n\u001b[32m    919\u001b[39m             future_covariates,\n\u001b[32m    920\u001b[39m         ),\n\u001b[32m    921\u001b[39m         params,\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_fit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m     \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m     \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m         series=seq2series(series),\n\u001b[32m    941\u001b[39m         past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m         future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:985\u001b[39m, in \u001b[36mTorchForecastingModel._setup_for_fit_from_dataset\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, sample_weight, stride, val_series, val_past_covariates, val_future_covariates, val_sample_weight, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs)\u001b[39m\n\u001b[32m    983\u001b[39m series = series2seq(series)\n\u001b[32m    984\u001b[39m past_covariates = series2seq(past_covariates)\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m future_covariates = \u001b[43mseries2seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m val_series = series2seq(val_series)\n\u001b[32m    987\u001b[39m val_past_covariates = series2seq(val_past_covariates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\ts_utils.py:103\u001b[39m, in \u001b[36mseries2seq\u001b[39m\u001b[34m(ts, seq_type_out, nested)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seq_type_out, SeriesType):\n\u001b[32m     96\u001b[39m     raise_log(\n\u001b[32m     97\u001b[39m         \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     98\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid parameter `seq_type_out=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_type_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. Must be one of `(0, 1, 2)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         ),\n\u001b[32m    100\u001b[39m         logger=logger,\n\u001b[32m    101\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m seq_type_in = \u001b[43mget_series_seq_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seq_type_out == seq_type_in:\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ts\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\ts_utils.py:220\u001b[39m, in \u001b[36mget_series_seq_type\u001b[39m\u001b[34m(ts)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ts, TimeSeries):\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesType.SINGLE\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mts\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, TimeSeries):\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesType.SEQ\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TiDEModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TiDE_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    # train_series=train_val,\n",
    "    val_series = val,\n",
    "    # val_series=None,\n",
    ")\n",
    "time_tide = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bd628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`predict()` was called with `n > output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "best_tide = TiDEModel.load_from_checkpoint(model_name='TiDE_model', best=True)\n",
    "preds_tide = best_tide.predict(\n",
    "    series            = train_val,\n",
    "    past_covariates   = past_cov,\n",
    "    future_covariates = future_cov,\n",
    "    n                 = test[0].n_timesteps \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04416e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 88.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "group_cols = [\"sku\",\"city\"]  # 예시\n",
    "groups_df = (\n",
    "    dataset\n",
    "    .loc[:, group_cols]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=group_cols)   # from_group_dataframe 도 내부적으로 정렬하므로\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(groups_df))):\n",
    "    group_id = groups_df.iloc[i]\n",
    "    \n",
    "    pred = preds_tide[i].to_dataframe()\n",
    "    pred = pred.reset_index()\n",
    "    pred = pred.rename(columns={\"index\": \"date\"})\n",
    "    # pred = pred.drop(columns=['discount_pct'])\n",
    "    \n",
    "    for j in range(len(pred)):\n",
    "        result.append({\n",
    "            \"sku\":  group_id[\"sku\"],\n",
    "            \"city\": group_id[\"city\"],\n",
    "            \"date\": pred['date'][j],\n",
    "            \"mean\": pred['demand'][j],\n",
    "            \"discount_pct\": pred['discount_pct'][j],\n",
    "            \"is_event\": pred['is_event'][j],\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df['mean'] = np.expm1(result_df['mean'])\n",
    "result_df['mean'] = result_df['mean'].round().astype(int)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "sub = pd.read_csv(\"extracted_contents/data/forecast_submission_template.csv\", parse_dates=[\"date\"])\n",
    "sub.drop(columns=['mean'], inplace=True)\n",
    "sub = sub.merge(result_df, on=['sku', 'city', 'date'], how='left')\n",
    "sub.to_csv(\"result.csv\", index=False)\n",
    "sub.drop(columns=['discount_pct', 'is_event'], inplace=True)\n",
    "sub.to_csv(\"forecast_submission_template.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8fc5b",
   "metadata": {},
   "source": [
    "# TSMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496e7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 100,\n",
    "    \"accelerator\": \"gpu\", \n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    # \"output_chunk_length\": 28,\n",
    "    # \"input_chunk_length\": 365,\n",
    "    \"output_chunk_length\": 7,\n",
    "    \"input_chunk_length\": 28,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.0001},\n",
    "#  'lr_scheduler_cls':ReduceLROnPlateau,   # <<<--- 사용할 스케줄러 클래스 직접 지정\n",
    "#  'lr_scheduler_kwargs':{                 # <<<--- 스케줄러에 전달할 파라미터 딕셔너리\n",
    "#      'monitor': 'val_loss',\n",
    "#      'patience': 5,\n",
    "#      'factor': 0.2, \n",
    "#      'threshold': 1e-4,\n",
    "#  },   \n",
    " 'loss_fn': EventWeightedMultiTaskLoss(),\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ef6ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX PRO 6000 Blackwell Workstation Edition') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                  | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0  | criterion             | EventWeightedMultiTaskLoss | 0      | train\n",
      "1  | train_criterion       | EventWeightedMultiTaskLoss | 0      | train\n",
      "2  | val_criterion         | EventWeightedMultiTaskLoss | 0      | train\n",
      "3  | train_metrics         | MetricCollection           | 0      | train\n",
      "4  | val_metrics           | MetricCollection           | 0      | train\n",
      "5  | rin                   | RINorm                     | 6      | train\n",
      "6  | fc_hist               | Linear                     | 203    | train\n",
      "7  | feature_mixing_hist   | _FeatureMixing             | 10.9 K | train\n",
      "8  | feature_mixing_future | _FeatureMixing             | 10.4 K | train\n",
      "9  | conditional_mixer     | ModuleList                 | 69.1 K | train\n",
      "10 | fc_out                | Linear                     | 195    | train\n",
      "------------------------------------------------------------------------------\n",
      "90.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.9 K    Total params\n",
      "0.364     Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 2788/2788 [02:07<00:00, 21.94it/s, train_loss=1.560, val_loss=4.730]\n"
     ]
    }
   ],
   "source": [
    "from darts.models import TSMixerModel\n",
    "\n",
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TSMixerModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TSMixer_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    # train_series=train_val,\n",
    "    val_series = val,\n",
    "    # val_series=None,\n",
    ")\n",
    "time_tide = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6de97",
   "metadata": {},
   "source": [
    "# N-HiTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f76ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 100,\n",
    "    \"accelerator\": \"gpu\", \n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"output_chunk_length\": 28,\n",
    "    \"input_chunk_length\": 365,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"random_state\": 42,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # N-HiTS 아키텍처 파라미터 \n",
    "    # --------------------------------------------------------------------------\n",
    "    \"num_stacks\": 3,              # 3개의 스택으로 잔차(residual)를 반복 학습 (표준값)\n",
    "    \"num_blocks\": 3,              # 각 스택 당 3개의 블록 사용 (표준값)\n",
    "    \"num_layers\": 4,              # 각 블록 내 4개의 MLP 레이어로 충분한 복잡성 부여\n",
    "    \"layer_widths\": 512,          # MLP 레이어의 너비. 모델의 표현력(capacity)을 결정\n",
    "    \"dropout\": 0.1,               # 과적합 방지를 위한 드롭아웃 (표준값)\n",
    "    \"activation\": \"ReLU\",         # 활성화 함수\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.0001},\n",
    " 'loss_fn': EventWeightedMultiTaskLoss(),\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }\n",
    "\n",
    "# --- 모델 생성 예시 ---\n",
    "# from darts.models import NHiTSModel\n",
    "#\n",
    "# model_nhits = NHiTSModel(\n",
    "#     output_dim=3, # 타겟 변수 개수 (demand, discount_pct, is_event)\n",
    "#     **n_hits_params\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ef97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Specified future encoders in `add_encoders` at model creation but model does not accept future covariates. future encoders will be ignored.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX PRO 6000 Blackwell Workstation Edition') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                       | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | criterion       | EventWeightedMultiTaskLoss | 0      | train\n",
      "1 | train_criterion | EventWeightedMultiTaskLoss | 0      | train\n",
      "2 | val_criterion   | EventWeightedMultiTaskLoss | 0      | train\n",
      "3 | train_metrics   | MetricCollection           | 0      | train\n",
      "4 | val_metrics     | MetricCollection           | 0      | train\n",
      "5 | rin             | RINorm                     | 6      | train\n",
      "6 | stacks          | ModuleList                 | 71.2 M | train\n",
      "-----------------------------------------------------------------------\n",
      "62.9 M    Trainable params\n",
      "8.2 M     Non-trainable params\n",
      "71.2 M    Total params\n",
      "284.728   Total estimated model params size (MB)\n",
      "142       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:   1%|          | 13/2088 [00:02<06:09,  5.62it/s, train_loss=5.680, val_loss=4.340]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:363\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_batch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m call._call_lightning_module_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_batch_end\u001b[39m\u001b[33m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:227\u001b[39m, in \u001b[36m_call_callback_hooks\u001b[39m\u001b[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback.state_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\tqdm_progress.py:279\u001b[39m, in \u001b[36mTQDMProgressBar.on_train_batch_end\u001b[39m\u001b[34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[39m\n\u001b[32m    278\u001b[39m _update_n(\u001b[38;5;28mself\u001b[39m.train_progress_bar, n)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28mself\u001b[39m.train_progress_bar.set_postfix(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\progress_bar.py:201\u001b[39m, in \u001b[36mProgressBar.get_metrics\u001b[39m\u001b[34m(self, trainer, pl_module)\u001b[39m\n\u001b[32m    200\u001b[39m standard_metrics = get_standard_metrics(trainer)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m pbar_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar_metrics\u001b[49m\n\u001b[32m    202\u001b[39m duplicates = \u001b[38;5;28mlist\u001b[39m(standard_metrics.keys() & pbar_metrics.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1667\u001b[39m, in \u001b[36mTrainer.progress_bar_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1661\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The metrics sent to the progress bar.\u001b[39;00m\n\u001b[32m   1662\u001b[39m \n\u001b[32m   1663\u001b[39m \u001b[33;03mThis includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\u001b[39;00m\n\u001b[32m   1664\u001b[39m \u001b[33;03m:paramref:`~pytorch_lightning.core.LightningModule.log.prog_bar` argument set.\u001b[39;00m\n\u001b[32m   1665\u001b[39m \n\u001b[32m   1666\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_logger_connector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar_metrics\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:255\u001b[39m, in \u001b[36m_LoggerConnector.progress_bar_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer._results:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mpbar\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m._progress_bar_metrics.update(metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:236\u001b[39m, in \u001b[36m_LoggerConnector.metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer._results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:493\u001b[39m, in \u001b[36m_ResultCollection.metrics\u001b[39m\u001b[34m(self, on_step)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result_metric.meta.prog_bar:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m         metrics[\u001b[33m\"\u001b[39m\u001b[33mpbar\u001b[39m\u001b[33m\"\u001b[39m][forked_name] = \u001b[43mconvert_tensors_to_scalars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:136\u001b[39m, in \u001b[36mconvert_tensors_to_scalars\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value.item()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_item\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py:66\u001b[39m, in \u001b[36mapply_to_collection\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:134\u001b[39m, in \u001b[36mconvert_tensors_to_scalars.<locals>.to_item\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    132\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe metric `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` does not contain a single element, thus it cannot be converted to a scalar.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m## COMMENT TO LOAD PRE-TRAINED MODEL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mfit_mixed_covariates_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mNHiTSModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecific_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNHiTS_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train_series=train_val,\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# val_series=None,\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m nhits_tide = time.time() - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfit_mixed_covariates_model\u001b[39m\u001b[34m(model_cls, common_model_args, specific_model_args, model_name, past_cov, future_cov, train_series, val_series, max_samples_per_ts, save, path)\u001b[39m\n\u001b[32m    114\u001b[39m model = model_cls(model_name=model_name,\n\u001b[32m    115\u001b[39m                 **common_model_args,\n\u001b[32m    116\u001b[39m                 **specific_model_args)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TRAIN ARGS ===================================\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# VAL ARGS ======================================\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save: model.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:944\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m     series=seq2series(series),\n\u001b[32m    941\u001b[39m     past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m     future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1127\u001b[39m, in \u001b[36mTorchForecastingModel.fit_from_dataset\u001b[39m\u001b[34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_from_dataset\u001b[39m(\n\u001b[32m   1076\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1082\u001b[39m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1083\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[33;03m    Train the model with a specific :class:`darts.utils.data.TorchTrainingDataset` instance.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1316\u001b[39m, in \u001b[36mTorchForecastingModel._train\u001b[39m\u001b[34m(self, trainer, model, train_loader, val_loader)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28mself\u001b[39m.load_ckpt_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requires_training:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m   1323\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = NHiTSModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'NHiTS_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = None,\n",
    "    train_series = train,\n",
    "    # train_series=train_val,\n",
    "    val_series = val,\n",
    "    # val_series=None,\n",
    ")\n",
    "nhits_tide = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45ac8e",
   "metadata": {},
   "source": [
    "# TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21edd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer arguments\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 50,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 200,\n",
    "    \"accelerator\": \"gpu\", # uncomment for gpu use\n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"output_chunk_length\": 7,\n",
    "    \"input_chunk_length\": 84,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ac34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.0001},\n",
    " 'loss_fn': MultiTaskLossModule(),\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5bfb6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX PRO 6000 Blackwell Workstation Edition') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | criterion                         | MultiTaskLossModule              | 0      | train\n",
      "1  | train_criterion                   | MultiTaskLossModule              | 0      | train\n",
      "2  | val_criterion                     | MultiTaskLossModule              | 0      | train\n",
      "3  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "4  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "5  | rin                               | RINorm                           | 6      | train\n",
      "6  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "7  | static_covariates_vsn             | _VariableSelectionNetwork        | 5.7 K  | train\n",
      "8  | encoder_vsn                       | _VariableSelectionNetwork        | 27.6 K | train\n",
      "9  | decoder_vsn                       | _VariableSelectionNetwork        | 24.9 K | train\n",
      "10 | static_context_grn                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K  | train\n",
      "12 | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K  | train\n",
      "13 | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K  | train\n",
      "14 | lstm_encoder                      | LSTM                             | 2.2 K  | train\n",
      "15 | lstm_decoder                      | LSTM                             | 2.2 K  | train\n",
      "16 | post_lstm_gan                     | _GateAddNorm                     | 576    | train\n",
      "17 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K  | train\n",
      "18 | multihead_attn                    | _InterpretableMultiHeadAttention | 676    | train\n",
      "19 | post_attn_gan                     | _GateAddNorm                     | 576    | train\n",
      "20 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "21 | pre_output_gan                    | _GateAddNorm                     | 576    | train\n",
      "22 | output_layer                      | Linear                           | 51     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "71.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "71.5 K    Total params\n",
      "0.286     Total estimated model params size (MB)\n",
      "1371      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  29%|██▉       | 3101/10711 [33:24<1:21:57,  1.55it/s, train_loss=0.248, val_loss=0.261] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\adam.py:225\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:241\u001b[39m, in \u001b[36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[39m\u001b[34m(loss)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_fn\u001b[39m(loss: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:213\u001b[39m, in \u001b[36mStrategy.backward\u001b[39m\u001b[34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.pre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.post_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:73\u001b[39m, in \u001b[36mPrecision.backward\u001b[39m\u001b[34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1097\u001b[39m, in \u001b[36mLightningModule.backward\u001b[39m\u001b[34m(self, loss, *args, **kwargs)\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m## COMMENT TO LOAD PRE-TRAINED MODEL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mfit_mixed_covariates_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTFTModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecific_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTFT_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m time_tft = time.time() - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfit_mixed_covariates_model\u001b[39m\u001b[34m(model_cls, common_model_args, specific_model_args, model_name, past_cov, future_cov, train_series, val_series, max_samples_per_ts, save, path)\u001b[39m\n\u001b[32m    114\u001b[39m model = model_cls(model_name=model_name,\n\u001b[32m    115\u001b[39m                 **common_model_args,\n\u001b[32m    116\u001b[39m                 **specific_model_args)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TRAIN ARGS ===================================\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# VAL ARGS ======================================\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save: model.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:944\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m     series=seq2series(series),\n\u001b[32m    941\u001b[39m     past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m     future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1127\u001b[39m, in \u001b[36mTorchForecastingModel.fit_from_dataset\u001b[39m\u001b[34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_from_dataset\u001b[39m(\n\u001b[32m   1076\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1082\u001b[39m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1083\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[33;03m    Train the model with a specific :class:`darts.utils.data.TorchTrainingDataset` instance.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1316\u001b[39m, in \u001b[36mTorchForecastingModel._train\u001b[39m\u001b[34m(self, trainer, model, train_loader, val_loader)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28mself\u001b[39m.load_ckpt_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requires_training:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m   1323\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TFTModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TFT_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    val_series = val,\n",
    ")\n",
    "time_tft = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b721f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`predict()` was called with `n > output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 8/8 [07:37<00:00,  0.02it/s]\n"
     ]
    }
   ],
   "source": [
    "best_tft = TFTModel.load_from_checkpoint(model_name='TFT_model', best=True)\n",
    "preds_tft = best_tft.predict(\n",
    "                    series            = train_val,\n",
    "                    past_covariates   = past_cov,\n",
    "                    future_covariates = future_cov,\n",
    "                    n                 = test[0].n_timesteps\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178bcd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 121.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "group_cols = [\"sku\",\"city\"]  # 예시\n",
    "groups_df = (\n",
    "    dataset\n",
    "    .loc[:, group_cols]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=group_cols)   # from_group_dataframe 도 내부적으로 정렬하므로\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(groups_df))):\n",
    "    group_id = groups_df.iloc[i]\n",
    "    \n",
    "    pred = preds_tft[i].to_dataframe()\n",
    "    pred = pred.reset_index()\n",
    "    pred = pred.rename(columns={\"index\": \"date\"})\n",
    "    pred = pred.drop(columns=['discount_pct'])\n",
    "    # pred['demand'] = np.exp(pred['demand'])\n",
    "    \n",
    "    for j in range(len(pred)):\n",
    "        result.append({\n",
    "            \"sku\":  group_id[\"sku\"],\n",
    "            \"city\": group_id[\"city\"],\n",
    "            \"date\": pred['date'][j],\n",
    "            \"mean\": pred['demand'][j],\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df['mean'] = np.expm1(result_df['mean'])\n",
    "result_df['mean'] = result_df['mean'].round().astype(int)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "sub = pd.read_csv(\"extracted_contents/data/forecast_submission_template.csv\", parse_dates=[\"date\"])\n",
    "sub.drop(columns=['mean'], inplace=True)\n",
    "sub = sub.merge(result_df, on=['sku', 'city', 'date'], how='left')\n",
    "sub.to_csv(\"forecast_submission_template.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
