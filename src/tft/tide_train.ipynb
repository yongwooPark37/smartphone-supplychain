{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c39ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import time\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.models import TiDEModel, NaiveMovingAverage, TFTModel\n",
    "from darts.metrics import mae, mse, smape\n",
    "from darts.utils.losses import MAELoss, MapeLoss, SmapeLoss\n",
    "import darts\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "\n",
    "def set_global_seed(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # CuDNN 연산을 deterministic하게 만들어 주지만, 약간 느려질 수 있음\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 값 고정\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ec71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_darts_time_series_group(\n",
    "    dataset: pd.DataFrame,\n",
    "    target: Union[List[str],str],\n",
    "    time_col: str,\n",
    "    group_cols: Union[List[str],str],\n",
    "    static_cols: Union[List[str],str]=None,\n",
    "    past_cols: Union[List[str],str]=None,\n",
    "    future_cols: Union[List[str],str]=None,\n",
    "    freq: str=None,\n",
    "    encode_static_cov: bool=True,\n",
    ")-> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], List[TimeSeries]]:\n",
    "\n",
    "    series_raw = TimeSeries.from_group_dataframe(\n",
    "    dataset,\n",
    "    time_col    =   time_col,\n",
    "    group_cols  =   group_cols,  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "    static_cols =   static_cols,  # also extract these additional columns as static covariates (without grouping)\n",
    "    value_cols  =   target,  # optionally, specify the time varying columns\n",
    "    n_jobs      =   -1,\n",
    "    verbose     =   False,\n",
    "    freq        =   freq,\n",
    "    )\n",
    "\n",
    "    if encode_static_cov:\n",
    "        static_cov_transformer = StaticCovariatesTransformer()\n",
    "        series_encoded = static_cov_transformer.fit_transform(series_raw)\n",
    "    else: series_encoded = []\n",
    "\n",
    "    if past_cols:\n",
    "        past_cov = TimeSeries.from_group_dataframe(\n",
    "            dataset,\n",
    "            time_col    =   time_col,\n",
    "            group_cols  =   group_cols,\n",
    "            value_cols  =   past_cols,\n",
    "            n_jobs      =   -1,\n",
    "            verbose     =   False,\n",
    "            freq        =   freq,\n",
    "            )\n",
    "    else: past_cov = []\n",
    "\n",
    "    if future_cols:\n",
    "        future_cov = TimeSeries.from_group_dataframe(\n",
    "            dataset,\n",
    "            time_col    =   time_col,\n",
    "            group_cols  =   group_cols,\n",
    "            value_cols  =   future_cols,\n",
    "            n_jobs      =   -1,\n",
    "            verbose     =   False,\n",
    "            freq        =   freq,\n",
    "            )\n",
    "    else: future_cov = []\n",
    "\n",
    "    return series_raw, series_encoded, past_cov, future_cov\n",
    "\n",
    "def split_grouped_darts_time_series(\n",
    "    series: List[TimeSeries],\n",
    "    split_date: Union[str, pd.Timestamp],\n",
    "    min_date: Union[str, pd.Timestamp]=None,\n",
    "    max_date: Union[str, pd.Timestamp]=None,\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
    "\n",
    "    if min_date:\n",
    "       raw_series = series.copy()\n",
    "       series = []\n",
    "       for s in raw_series:\n",
    "        try: series.append(s.split_before(pd.Timestamp(min_date)-timedelta(1))[1])\n",
    "        except: series.append(s)\n",
    "\n",
    "    if max_date:\n",
    "       raw_series = series.copy()\n",
    "       series = []\n",
    "       for s in raw_series:\n",
    "        try: series.append(s.split_before(pd.Timestamp(max_date))[0])\n",
    "        except: series.append(s)\n",
    "\n",
    "    split_0 = [s.split_before(pd.Timestamp(split_date))[0] for s in series]\n",
    "    split_1 = [s.split_before(pd.Timestamp(split_date))[1] for s in series]\n",
    "    return split_0, split_1\n",
    "\n",
    "def eval_forecasts(\n",
    "    pred_series: Union[List[TimeSeries], TimeSeries],\n",
    "    test_series: Union[List[TimeSeries], TimeSeries],\n",
    "    error_metric: darts.metrics,\n",
    "    plot: bool=False\n",
    ") -> List[float]:\n",
    "\n",
    "    errors = error_metric(test_series, pred_series)\n",
    "    print(errors)\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.hist(errors, bins=50)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Error\")\n",
    "        plt.title(f\"Mean error: {np.mean(errors):.3f}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return errors\n",
    "\n",
    "def fit_mixed_covariates_model(\n",
    "    model_cls,\n",
    "    common_model_args: dict,\n",
    "    specific_model_args: dict,\n",
    "    model_name: str,\n",
    "    past_cov: Union[List[TimeSeries], TimeSeries],\n",
    "    future_cov: Union[List[TimeSeries], TimeSeries],\n",
    "    train_series: Union[List[TimeSeries], TimeSeries],\n",
    "    val_series: Union[List[TimeSeries], TimeSeries]=None,\n",
    "    max_samples_per_ts: int=None,\n",
    "    save:bool=False,\n",
    "    path:str=\"\",\n",
    "):\n",
    "\n",
    "    # Declarare model\n",
    "    model = model_cls(model_name=model_name,\n",
    "                    **common_model_args,\n",
    "                    **specific_model_args)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(\n",
    "                    # TRAIN ARGS ===================================\n",
    "                    series                = train_series,\n",
    "                    past_covariates       = past_cov,\n",
    "                    future_covariates     = future_cov,\n",
    "                    max_samples_per_ts    = max_samples_per_ts,\n",
    "                    # VAL ARGS ======================================\n",
    "                    val_series            = val_series,\n",
    "                    val_past_covariates   = past_cov,\n",
    "                    val_future_covariates = future_cov,\n",
    "                )\n",
    "\n",
    "    if save: model.save(path)\n",
    "\n",
    "def backtesting(model, series, past_cov, future_cov, start_date, horizon, stride):\n",
    "  historical_backtest = model.historical_forecasts(\n",
    "    series, past_cov, future_cov,\n",
    "    start=start_date,\n",
    "    forecast_horizon=horizon,\n",
    "    stride=stride,  # Predict every N months\n",
    "    retrain=False,  # Keep the model fixed (no retraining)\n",
    "    overlap_end=False,\n",
    "    last_points_only=False\n",
    "  )\n",
    "  maes = model.backtest(series, historical_forecasts=historical_backtest, metric=mae)\n",
    "\n",
    "  return np.mean(maes)\n",
    "\n",
    "def process_predictions(\n",
    "    preds: List[TimeSeries],\n",
    "    series_raw: List[TimeSeries],\n",
    "    group_cols: List[str]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    list_df = [serie.pd_dataframe() for serie in preds]\n",
    "    for i in range(len(list_df)):\n",
    "      list_df[i]['Date'] = preds[i].time_index\n",
    "      for j in range(len(group_cols)):\n",
    "        list_df[i][group_cols[j]] = series_raw[i].static_covariates[group_cols[j]].values[0]\n",
    "    processed_preds =  pd.concat(list_df, ignore_index=True)\n",
    "    return processed_preds\n",
    "\n",
    "def price_weighted_mae(predictions, targets, prices):\n",
    "    \"\"\"\n",
    "    Compute the price-weighted Mean Absolute Error (MAE).\n",
    "\n",
    "    :param predictions: A list or 1D NumPy array of predicted values.\n",
    "    :param targets: A list or 1D NumPy array of actual (ground truth) values.\n",
    "    :param prices: A list or 1D NumPy array of prices corresponding to the targets.\n",
    "    :return: The price-weighted MAE as a float.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    predictions = np.array(predictions, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "    prices = np.array(prices, dtype=np.float32)\n",
    "\n",
    "    # Compute absolute error\n",
    "    error = np.abs(targets - predictions)\n",
    "\n",
    "    # Compute price-weighted error\n",
    "    weighted_error = error * prices\n",
    "\n",
    "    # Compute and return the mean of the weighted error\n",
    "    return np.mean(weighted_error)\n",
    "\n",
    "def local_iqr_clip(series, window=30, q1=0.25, q3=0.75, m=2.5):\n",
    "    roll_q1 = series.rolling(window, center=True).quantile(q1)\n",
    "    roll_q3 = series.rolling(window, center=True).quantile(q3)\n",
    "    iqr = roll_q3 - roll_q1\n",
    "    upper = roll_q3 + m * iqr\n",
    "    return series.clip(0, upper)\n",
    "\n",
    "class MultiTaskLossModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskLossModule, self).__init__()\n",
    "        self.alpha = 1.0\n",
    "        self.beta = 1.0\n",
    "        self.gamma = 5.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # 내부에 바로 구현\n",
    "        reg_pred   = y_pred[..., :2]\n",
    "        reg_true   = y_true[..., :2]\n",
    "        cls_pred   = y_pred[..., 2]\n",
    "        cls_true   = y_true[..., 2]\n",
    "\n",
    "        loss_demand   = F.mse_loss(reg_pred[..., 0], reg_true[..., 0])\n",
    "        loss_discount = F.mse_loss(reg_pred[..., 1], reg_true[..., 1])\n",
    "        loss_cls      = F.binary_cross_entropy_with_logits(cls_pred, cls_true)\n",
    "\n",
    "        return self.alpha*loss_demand + self.beta*loss_discount + self.gamma*loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATE = pd.Timestamp('2023-01-01')\n",
    "VAL_DATE_OUT = pd.Timestamp('2022-01-01')\n",
    "VAL_DATE_IN = pd.Timestamp('2022-01-01')\n",
    "\n",
    "dataset = pd.read_csv(\"data/all_master.csv\", parse_dates=[\"date\"])\n",
    "dataset = dataset.sort_values(by=[\"city\", \"sku\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daff2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    \"JPN\": [(\"2019-01-01\", \"2019-02-28\")],\n",
    "    \"KOR\": [(\"2018-01-15\", \"2018-04-15\")],\n",
    "    \"USA\": [(\"2020-01-01\", \"2020-04-30\"),\n",
    "            (\"2021-03-01\", \"2021-05-31\")]\n",
    "}\n",
    "\n",
    "dataset['is_event'] = 0\n",
    "\n",
    "for country, periods in LABELS.items():\n",
    "    for start, end in periods:\n",
    "        start_dt = pd.to_datetime(start)\n",
    "        end_dt   = pd.to_datetime(end)\n",
    "        mask = (\n",
    "            (dataset[\"country\"] == country) &\n",
    "            (dataset[\"date\"] >= start_dt) &\n",
    "            (dataset[\"date\"] <= end_dt)   # end 포함\n",
    "        )\n",
    "        dataset.loc[mask, \"is_event\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0399ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"data/train_demand.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572f1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = 'none'\n",
    "# preprocess = 'iqr'\n",
    "dataset = pd.get_dummies(dataset, columns=['season'], prefix='season')\n",
    "dataset = dataset[dataset['days_since_launch'] > 0]\n",
    "\n",
    "dataset['month'] = dataset['date'].dt.month\n",
    "dataset['day']   = dataset['date'].dt.day \n",
    "dataset['year']  = dataset['date'].dt.year\n",
    "dataset['day_of_week'] = dataset['date'].dt.dayofweek\n",
    "\n",
    "target_col = ['demand']\n",
    "time_col = 'date'\n",
    "group_cols = ['sku','city']\n",
    "\n",
    "drop_cols = [\n",
    "    # static\n",
    "    # 'country', \n",
    "    'category', 'family', 'storage_gb', 'colour', \n",
    "    # numeric\n",
    "    # 'season_Fall', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday',\n",
    "    'avg_temp', 'humidity', 'precip_mm',\n",
    "    'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover',\n",
    "    'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode',\n",
    "    'shortwave_rad_MJ', 'vpd', 'cdd18', 'delta_temp',\n",
    "    'delta_humidity',\n",
    "]\n",
    "drop_cols = []\n",
    "# past_cols = ['EMA_30', 'MA_30']\n",
    "past_cols = ['discount_pct', 'is_event']\n",
    "future_cols = ['season_Fall', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday','avg_temp', 'min_temp', 'max_temp', 'dewpoint', 'humidity', 'precip_mm',\n",
    "       'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover',\n",
    "       'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode',\n",
    "       'shortwave_rad_MJ', 'vpd', 'hdd18', 'cdd18', 'delta_temp',\n",
    "       'delta_humidity', 'brent_usd', 'local_fx', 'spend_usd', 'days_since_launch',\n",
    "       'month', 'day', \"year\", \"day_of_week\"\n",
    "]\n",
    "\n",
    "static_cols = ['country', 'category', 'family', 'storage_gb', 'colour', 'unit_price', 'life_days']\n",
    "\n",
    "dataset = dataset.drop(columns=drop_cols)\n",
    "future_cols = [col for col in future_cols if col not in drop_cols]\n",
    "static_cols = [col for col in static_cols if col not in drop_cols]\n",
    "\n",
    "\n",
    "if preprocess == 'clip':\n",
    "    print('clip')\n",
    "    low, high = dataset['demand'].quantile([0.00,0.95])\n",
    "    dataset['demand'] = dataset['demand'].clip(low, high)\n",
    "elif preprocess == 'iqr':\n",
    "    print('iqr')\n",
    "    dataset['demand'] = local_iqr_clip(dataset['demand'])\n",
    "dataset['demand'] = np.log1p(dataset['demand'])\n",
    "dataset['discount_pct'] = dataset['discount_pct']/100\n",
    "    \n",
    "series_raw, series, past_cov, future_cov = to_darts_time_series_group(\n",
    "    dataset=dataset,\n",
    "    target=target_col,\n",
    "    time_col=time_col,\n",
    "    group_cols=group_cols,\n",
    "    past_cols=past_cols,\n",
    "    future_cols=future_cols,\n",
    "    static_cols=static_cols,\n",
    "    freq='D', # daily\n",
    "    encode_static_cov=True, # so that the models can use the categorical variables (Agency & Product)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc90eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = split_grouped_darts_time_series(\n",
    "    series=series,\n",
    "    split_date=TEST_DATE\n",
    ")\n",
    "\n",
    "train, val = split_grouped_darts_time_series(\n",
    "    series=train_val,\n",
    "    split_date=VAL_DATE_OUT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 100,\n",
    "    \"accelerator\": \"gpu\", \n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"output_chunk_length\": 7,\n",
    "    \"input_chunk_length\": 84,\n",
    "    # \"output_chunk_length\": 1,\n",
    "    # \"input_chunk_length\": 90,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.0001},\n",
    " 'loss_fn': MAELoss(),\n",
    " 'use_layer_norm': True,\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2080d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "number of `past_covariates` features is <= `temporal_width_past`, leading to feature expansion.number of covariates: 3, `temporal_width_past=4`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX PRO 6000 Blackwell Workstation Edition') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                  | Type             | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0  | criterion             | MAELoss          | 0      | train\n",
      "1  | train_criterion       | MAELoss          | 0      | train\n",
      "2  | val_criterion         | MAELoss          | 0      | train\n",
      "3  | train_metrics         | MetricCollection | 0      | train\n",
      "4  | val_metrics           | MetricCollection | 0      | train\n",
      "5  | rin                   | RINorm           | 2      | train\n",
      "6  | past_cov_projection   | _ResidualBlock   | 1.1 K  | train\n",
      "7  | future_cov_projection | _ResidualBlock   | 5.3 K  | train\n",
      "8  | encoders              | Sequential       | 227 K  | train\n",
      "9  | decoders              | Sequential       | 20.7 K | train\n",
      "10 | temporal_decoder      | _ResidualBlock   | 728    | train\n",
      "11 | lookback_skip         | Linear           | 91     | train\n",
      "--------------------------------------------------------------------\n",
      "255 K     Trainable params\n",
      "0         Non-trainable params\n",
      "255 K     Total params\n",
      "1.022     Total estimated model params size (MB)\n",
      "49        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  10%|▉         | 261/2678 [00:12<01:54, 21.15it/s, train_loss=0.397, val_loss=0.367] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:306\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    305\u001b[39m dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m batch, _, __ = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:134\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     batch = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:61\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001b[39m, in \u001b[36m_MaxSizeCycle.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     out[i] = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1869\u001b[39m, in \u001b[36mTorchForecastingModel._batch_collate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m   1867\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, np.ndarray):\n\u001b[32m   1868\u001b[39m     aggregated.append(\n\u001b[32m-> \u001b[39m\u001b[32m1869\u001b[39m         torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m   1870\u001b[39m     )\n\u001b[32m   1871\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m elem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\numpy\\_core\\shape_base.py:467\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    466\u001b[39m expanded_arrays = [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m## COMMENT TO LOAD PRE-TRAINED MODEL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mfit_mixed_covariates_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTiDEModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecific_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTiDE_demand_onty_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train_series=train_val,\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# val_series=None,\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m time_tide = time.time() - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfit_mixed_covariates_model\u001b[39m\u001b[34m(model_cls, common_model_args, specific_model_args, model_name, past_cov, future_cov, train_series, val_series, max_samples_per_ts, save, path)\u001b[39m\n\u001b[32m    114\u001b[39m model = model_cls(model_name=model_name,\n\u001b[32m    115\u001b[39m                 **common_model_args,\n\u001b[32m    116\u001b[39m                 **specific_model_args)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TRAIN ARGS ===================================\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# VAL ARGS ======================================\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save: model.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:944\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m     series=seq2series(series),\n\u001b[32m    941\u001b[39m     past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m     future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1127\u001b[39m, in \u001b[36mTorchForecastingModel.fit_from_dataset\u001b[39m\u001b[34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_from_dataset\u001b[39m(\n\u001b[32m   1076\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1082\u001b[39m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1083\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[33;03m    Train the model with a specific :class:`darts.utils.data.TorchTrainingDataset` instance.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1316\u001b[39m, in \u001b[36mTorchForecastingModel._train\u001b[39m\u001b[34m(self, trainer, model, train_loader, val_loader)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28mself\u001b[39m.load_ckpt_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requires_training:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m   1323\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TiDEModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TiDE_demand_onty_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    # train_series=train_val,\n",
    "    val_series = val,\n",
    "    # val_series=None,\n",
    ")\n",
    "time_tide = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tide = TiDEModel.load_from_checkpoint(model_name='TiDE_demand_onty_model', best=True)\n",
    "preds_tide = best_tide.predict(\n",
    "                    series            = train_val,\n",
    "                    past_covariates   = past_cov,\n",
    "                    future_covariates = future_cov,\n",
    "                    n                 = test[0].n_timesteps \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65437078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "group_cols = [\"sku\",\"city\"]  # 예시\n",
    "groups_df = (\n",
    "    dataset\n",
    "    .loc[:, group_cols]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=group_cols)   # from_group_dataframe 도 내부적으로 정렬하므로\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(groups_df))):\n",
    "    group_id = groups_df.iloc[i]\n",
    "    \n",
    "    pred = preds_tide[i].to_dataframe()\n",
    "    pred = pred.reset_index()\n",
    "    pred = pred.rename(columns={\"index\": \"date\"})\n",
    "    # pred = pred.drop(columns=['discount_pct'])\n",
    "    \n",
    "    for j in range(len(pred)):\n",
    "        result.append({\n",
    "            \"sku\":  group_id[\"sku\"],\n",
    "            \"city\": group_id[\"city\"],\n",
    "            \"date\": pred['date'][j],\n",
    "            \"mean\": pred['demand'][j],\n",
    "            \"discount_pct\": pred['discount_pct'][j],\n",
    "            \"is_event\": pred['is_event'][j],\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df['mean'] = np.expm1(result_df['mean'])\n",
    "result_df['mean'] = result_df['mean'].round().astype(int)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "sub = pd.read_csv(\"extracted_contents/data/forecast_submission_template.csv\", parse_dates=[\"date\"])\n",
    "sub.drop(columns=['mean'], inplace=True)\n",
    "sub = sub.merge(result_df, on=['sku', 'city', 'date'], how='left')\n",
    "sub.to_csv(\"result.csv\", index=False)\n",
    "sub.drop(columns=['discount_pct', 'is_event'], inplace=True)\n",
    "sub.to_csv(\"forecast_submission_template.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45ac8e",
   "metadata": {},
   "source": [
    "# TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21edd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer arguments\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 50,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 200,\n",
    "    \"accelerator\": \"gpu\", # uncomment for gpu use\n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"input_chunk_length\": 30,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.0001},\n",
    " 'loss_fn': MultiTaskLossModule(),\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TFTModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TFT_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    val_series = val,\n",
    ")\n",
    "time_tft = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tft = TFTModel.load_from_checkpoint(model_name='TFT_model', best=True)\n",
    "preds_tft = best_tft.predict(\n",
    "                    series            = train_val,\n",
    "                    past_covariates   = past_cov,\n",
    "                    future_covariates = future_cov,\n",
    "                    n                 = test[0].n_timesteps\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "group_cols = [\"sku\",\"city\"]  # 예시\n",
    "groups_df = (\n",
    "    dataset\n",
    "    .loc[:, group_cols]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=group_cols)   # from_group_dataframe 도 내부적으로 정렬하므로\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(groups_df))):\n",
    "    group_id = groups_df.iloc[i]\n",
    "    \n",
    "    pred = preds_tft[i].to_dataframe()\n",
    "    pred = pred.reset_index()\n",
    "    pred = pred.rename(columns={\"index\": \"date\"})\n",
    "    pred = pred.drop(columns=['discount_pct'])\n",
    "    # pred['demand'] = np.exp(pred['demand'])\n",
    "    \n",
    "    for j in range(len(pred)):\n",
    "        result.append({\n",
    "            \"sku\":  group_id[\"sku\"],\n",
    "            \"city\": group_id[\"city\"],\n",
    "            \"date\": pred['date'][j],\n",
    "            \"mean\": pred['demand'][j],\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df['mean'] = np.expm1(result_df['mean'])\n",
    "result_df['mean'] = result_df['mean'].round().astype(int)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "sub = pd.read_csv(\"extracted_contents/data/forecast_submission_template.csv\", parse_dates=[\"date\"])\n",
    "sub.drop(columns=['mean'], inplace=True)\n",
    "sub = sub.merge(result_df, on=['sku', 'city', 'date'], how='left')\n",
    "sub.to_csv(\"forecast_submission_template.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
