{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c39ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import time\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import StaticCovariatesTransformer\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.models import TiDEModel, NaiveMovingAverage, TFTModel\n",
    "from darts.metrics import mae, mse, smape\n",
    "from darts.utils.losses import MAELoss, MapeLoss, SmapeLoss\n",
    "import darts\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04aea7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # CuDNN 연산을 deterministic하게 만들어 주지만, 약간 느려질 수 있음\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 시드 값 고정\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ec71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_darts_time_series_group(\n",
    "    dataset: pd.DataFrame,\n",
    "    target: Union[List[str],str],\n",
    "    time_col: str,\n",
    "    group_cols: Union[List[str],str],\n",
    "    static_cols: Union[List[str],str]=None,\n",
    "    past_cols: Union[List[str],str]=None,\n",
    "    future_cols: Union[List[str],str]=None,\n",
    "    freq: str=None,\n",
    "    encode_static_cov: bool=True,\n",
    ")-> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], List[TimeSeries]]:\n",
    "\n",
    "    series_raw = TimeSeries.from_group_dataframe(\n",
    "    dataset,\n",
    "    time_col    =   time_col,\n",
    "    group_cols  =   group_cols,  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "    static_cols =   static_cols,  # also extract these additional columns as static covariates (without grouping)\n",
    "    value_cols  =   target,  # optionally, specify the time varying columns\n",
    "    n_jobs      =   -1,\n",
    "    verbose     =   False,\n",
    "    freq        =   freq,\n",
    "    )\n",
    "\n",
    "    if encode_static_cov:\n",
    "        static_cov_transformer = StaticCovariatesTransformer()\n",
    "        series_encoded = static_cov_transformer.fit_transform(series_raw)\n",
    "    else: series_encoded = []\n",
    "\n",
    "    if past_cols:\n",
    "        past_cov = TimeSeries.from_group_dataframe(\n",
    "            dataset,\n",
    "            time_col    =   time_col,\n",
    "            group_cols  =   group_cols,\n",
    "            value_cols  =   past_cols,\n",
    "            n_jobs      =   -1,\n",
    "            verbose     =   False,\n",
    "            freq        =   freq,\n",
    "            )\n",
    "    else: past_cov = []\n",
    "\n",
    "    if future_cols:\n",
    "        future_cov = TimeSeries.from_group_dataframe(\n",
    "            dataset,\n",
    "            time_col    =   time_col,\n",
    "            group_cols  =   group_cols,\n",
    "            value_cols  =   future_cols,\n",
    "            n_jobs      =   -1,\n",
    "            verbose     =   False,\n",
    "            freq        =   freq,\n",
    "            )\n",
    "    else: future_cov = []\n",
    "\n",
    "    return series_raw, series_encoded, past_cov, future_cov\n",
    "\n",
    "def split_grouped_darts_time_series(\n",
    "    series: List[TimeSeries],\n",
    "    split_date: Union[str, pd.Timestamp],\n",
    "    min_date: Union[str, pd.Timestamp]=None,\n",
    "    max_date: Union[str, pd.Timestamp]=None,\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
    "\n",
    "    if min_date:\n",
    "       raw_series = series.copy()\n",
    "       series = []\n",
    "       for s in raw_series:\n",
    "        try: series.append(s.split_before(pd.Timestamp(min_date)-timedelta(1))[1])\n",
    "        except: series.append(s)\n",
    "\n",
    "    if max_date:\n",
    "       raw_series = series.copy()\n",
    "       series = []\n",
    "       for s in raw_series:\n",
    "        try: series.append(s.split_before(pd.Timestamp(max_date))[0])\n",
    "        except: series.append(s)\n",
    "\n",
    "    split_0 = [s.split_before(pd.Timestamp(split_date))[0] for s in series]\n",
    "    split_1 = [s.split_before(pd.Timestamp(split_date))[1] for s in series]\n",
    "    return split_0, split_1\n",
    "\n",
    "def eval_forecasts(\n",
    "    pred_series: Union[List[TimeSeries], TimeSeries],\n",
    "    test_series: Union[List[TimeSeries], TimeSeries],\n",
    "    error_metric: darts.metrics,\n",
    "    plot: bool=False\n",
    ") -> List[float]:\n",
    "\n",
    "    errors = error_metric(test_series, pred_series)\n",
    "    print(errors)\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.hist(errors, bins=50)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Error\")\n",
    "        plt.title(f\"Mean error: {np.mean(errors):.3f}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return errors\n",
    "\n",
    "def fit_mixed_covariates_model(\n",
    "    model_cls,\n",
    "    common_model_args: dict,\n",
    "    specific_model_args: dict,\n",
    "    model_name: str,\n",
    "    past_cov: Union[List[TimeSeries], TimeSeries],\n",
    "    future_cov: Union[List[TimeSeries], TimeSeries],\n",
    "    train_series: Union[List[TimeSeries], TimeSeries],\n",
    "    val_series: Union[List[TimeSeries], TimeSeries]=None,\n",
    "    max_samples_per_ts: int=None,\n",
    "    save:bool=False,\n",
    "    path:str=\"\",\n",
    "):\n",
    "\n",
    "    # Declarare model\n",
    "    model = model_cls(model_name=model_name,\n",
    "                    **common_model_args,\n",
    "                    **specific_model_args)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(\n",
    "                    # TRAIN ARGS ===================================\n",
    "                    series                = train_series,\n",
    "                    past_covariates       = past_cov,\n",
    "                    future_covariates     = future_cov,\n",
    "                    max_samples_per_ts    = max_samples_per_ts,\n",
    "                    # VAL ARGS ======================================\n",
    "                    val_series            = val_series,\n",
    "                    val_past_covariates   = past_cov,\n",
    "                    val_future_covariates = future_cov,\n",
    "                )\n",
    "\n",
    "    if save: model.save(path)\n",
    "\n",
    "def backtesting(model, series, past_cov, future_cov, start_date, horizon, stride):\n",
    "  historical_backtest = model.historical_forecasts(\n",
    "    series, past_cov, future_cov,\n",
    "    start=start_date,\n",
    "    forecast_horizon=horizon,\n",
    "    stride=stride,  # Predict every N months\n",
    "    retrain=False,  # Keep the model fixed (no retraining)\n",
    "    overlap_end=False,\n",
    "    last_points_only=False\n",
    "  )\n",
    "  maes = model.backtest(series, historical_forecasts=historical_backtest, metric=mae)\n",
    "\n",
    "  return np.mean(maes)\n",
    "\n",
    "def process_predictions(\n",
    "    preds: List[TimeSeries],\n",
    "    series_raw: List[TimeSeries],\n",
    "    group_cols: List[str]\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    list_df = [serie.pd_dataframe() for serie in preds]\n",
    "    for i in range(len(list_df)):\n",
    "      list_df[i]['Date'] = preds[i].time_index\n",
    "      for j in range(len(group_cols)):\n",
    "        list_df[i][group_cols[j]] = series_raw[i].static_covariates[group_cols[j]].values[0]\n",
    "    processed_preds =  pd.concat(list_df, ignore_index=True)\n",
    "    return processed_preds\n",
    "\n",
    "def price_weighted_mae(predictions, targets, prices):\n",
    "    \"\"\"\n",
    "    Compute the price-weighted Mean Absolute Error (MAE).\n",
    "\n",
    "    :param predictions: A list or 1D NumPy array of predicted values.\n",
    "    :param targets: A list or 1D NumPy array of actual (ground truth) values.\n",
    "    :param prices: A list or 1D NumPy array of prices corresponding to the targets.\n",
    "    :return: The price-weighted MAE as a float.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    predictions = np.array(predictions, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "    prices = np.array(prices, dtype=np.float32)\n",
    "\n",
    "    # Compute absolute error\n",
    "    error = np.abs(targets - predictions)\n",
    "\n",
    "    # Compute price-weighted error\n",
    "    weighted_error = error * prices\n",
    "\n",
    "    # Compute and return the mean of the weighted error\n",
    "    return np.mean(weighted_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc6f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATE = pd.Timestamp('2023-01-01')\n",
    "VAL_DATE_OUT = pd.Timestamp('2022-01-01')\n",
    "VAL_DATE_IN = pd.Timestamp('2022-01-01')\n",
    "# MIN_TRAIN_DATE = pd.Timestamp('2015-06-01')\n",
    "\n",
    "dataset = pd.read_csv(\"data/all_master.csv\", parse_dates=[\"date\"])\n",
    "dataset = dataset.sort_values(by=[ \"city\", \"sku\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daff2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      country       date  is_event\n",
      "14004     KOR 2018-01-15         1\n",
      "15004     KOR 2018-01-16         1\n",
      "16004     KOR 2018-01-17         1\n",
      "17004     KOR 2018-01-18         1\n",
      "18004     KOR 2018-01-19         1\n"
     ]
    }
   ],
   "source": [
    "LABELS = {\n",
    "    \"JPN\": [(\"2019-01-01\", \"2019-02-28\")],\n",
    "    \"KOR\": [(\"2018-01-15\", \"2018-04-15\")],\n",
    "    \"USA\": [(\"2020-01-01\", \"2020-04-30\"),\n",
    "            (\"2021-03-01\", \"2021-05-31\")]\n",
    "}\n",
    "\n",
    "dataset['is_event'] = 0\n",
    "\n",
    "for country, periods in LABELS.items():\n",
    "    for start, end in periods:\n",
    "        start_dt = pd.to_datetime(start)\n",
    "        end_dt   = pd.to_datetime(end)\n",
    "        mask = (\n",
    "            (dataset[\"country\"] == country) &\n",
    "            (dataset[\"date\"] >= start_dt) &\n",
    "            (dataset[\"date\"] <= end_dt)   # end 포함\n",
    "        )\n",
    "        dataset.loc[mask, \"is_event\"] = 1\n",
    "\n",
    "print(dataset[[\"country\", \"date\", \"is_event\"]].query(\"is_event==1\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1733b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cols ['is_holiday', 'avg_temp', 'min_temp', 'max_temp', 'dewpoint', 'humidity', 'precip_mm', 'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover', 'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode', 'shortwave_rad_MJ', 'vpd', 'hdd18', 'cdd18', 'delta_temp', 'delta_humidity', 'brent_usd', 'spend_usd', 'storage_gb', 'life_days', 'unit_price', 'days_since_launch', 'local_fx', 'mean']\n",
      "cat_cols ['country', 'season', 'category', 'family', 'colour']\n"
     ]
    }
   ],
   "source": [
    "target_col = ['demand', 'discount_pct', 'is_event']\n",
    "time_col = 'date'\n",
    "group_cols = ['sku','city']\n",
    "\n",
    "numeric_cols = dataset.select_dtypes(include=['number']).columns\n",
    "num_cols  = [c for c in numeric_cols if c not in target_col]      # target 제외\n",
    "categorical_cols = dataset.select_dtypes(include=['object']).columns\n",
    "cat_cols  = [c for c in categorical_cols if c not in ['sku', 'city', 'date']]\n",
    "\n",
    "print(\"num_cols\", num_cols)\n",
    "print(\"cat_cols\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c32cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ② ColumnTransformer :  Cat → One‑Hot,  Num → StandardScaler\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "#         (\"num\", StandardScaler(), num_cols)\n",
    "#     ],\n",
    "#     remainder=\"drop\"            # 원본 컬럼은 삭제\n",
    "# )\n",
    "\n",
    "# # ③ train + test 합친 뒤(피처 칼럼만) fit_transform → 데이터에 다시 붙이기\n",
    "# cols_to_encode = cat_cols + num_cols\n",
    "# encoded_arr = preprocessor.fit_transform(dataset[cols_to_encode])\n",
    "\n",
    "# # ④ 인코딩된 컬럼 이름 얻기\n",
    "# enc_cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(cat_cols)\n",
    "# enc_num_names = num_cols\n",
    "# new_cols = list(enc_cat_names) + enc_num_names\n",
    "\n",
    "# # ⑤ 기존 컬럼 삭제 + 인코딩된 배열 붙이기\n",
    "# df_encoded = dataset.drop(columns=cols_to_encode)\n",
    "# df_encoded[new_cols] = encoded_arr\n",
    "\n",
    "# static_cols = [\n",
    "#  # 국가\n",
    "#  'country_AUS', 'country_BRA', 'country_CAN', 'country_DEU', 'country_FRA',\n",
    "#  'country_GBR', 'country_JPN', 'country_KOR', 'country_USA', 'country_ZAF',\n",
    "\n",
    "#  # 카테고리\n",
    "#  'category_ONLINE', 'category_OOH', 'category_SOCIAL', 'category_TV',\n",
    "\n",
    "#  # 상품군\n",
    "#  'family_FAM_SMART1', 'family_FAM_SMART10', 'family_FAM_SMART11',\n",
    "#  'family_FAM_SMART12', 'family_FAM_SMART13', 'family_FAM_SMART14',\n",
    "#  'family_FAM_SMART15', 'family_FAM_SMART2', 'family_FAM_SMART3',\n",
    "#  'family_FAM_SMART4', 'family_FAM_SMART5', 'family_FAM_SMART6',\n",
    "#  'family_FAM_SMART7', 'family_FAM_SMART8', 'family_FAM_SMART9',\n",
    "\n",
    "#  # 색상\n",
    "#  'colour_Black', 'colour_Blue', 'colour_Green', 'colour_Red', 'colour_Silver','colour_White',\n",
    " \n",
    "#  'storage_gb', # 저장 용량(예: 전자제품 등)\n",
    "#  'unit_price', # 기본 가격\n",
    "#  'life_days'   # 출시 후 경과 일수\n",
    "# ]\n",
    "\n",
    "# future_cols = [\n",
    "#      # 계절\n",
    "#      'season_Fall', 'season_Spring', 'season_Summer', 'season_Winter',\n",
    "\n",
    "#      # 휴일\n",
    "#      \"is_holiday\",\n",
    "\n",
    "#      # 날씨\n",
    "#      \"avg_temp\", \"min_temp\", \"max_temp\", \"dewpoint\", \"humidity\", \"precip_mm\", \"rain_mm\", \"snow_mm\", \"snow_depth_cm\", \"pressure_msl\", \"cloud_cover\", \n",
    "#      \"wind_speed_avg\", \"wind_speed_max\", \"wind_gust_max\", \"wind_dir_mode\", \"shortwave_rad_MJ\", \"vpd\", \"hdd18\", \"cdd18\", \"delta_temp\", \"delta_humidity\",\n",
    "\n",
    "#      # 환율, 유가, 소비자 신뢰지수, 마케팅 지출\n",
    "#      \"brent_usd\", \"EUR=X\", \"KRW=X\", \"JPY=X\", \"GBP=X\", \"CAD=X\", \"AUD=X\", \"BRL=X\", \"ZAR=X\", \"confidence_index\", \"spend_usd\", \n",
    "     \n",
    "#      # 출시 후 경과 일수\n",
    "#      \"days_since_launch\"\n",
    "# ]\n",
    "# past_cols = ['EMA_4', 'MA_4']\n",
    "\n",
    "# series_raw, series, past_cov, future_cov = to_darts_time_series_group(\n",
    "#     dataset=df_encoded,\n",
    "#     target=target_col,\n",
    "#     time_col=time_col,\n",
    "#     group_cols=group_cols,\n",
    "#     past_cols=past_cols,\n",
    "#     future_cols=future_cols,\n",
    "#     freq='D', # daily\n",
    "#     encode_static_cov=False, # so that the models can use the categorical variables (Agency & Product)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55332c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_iqr_clip(series, window=30, q1=0.25, q3=0.75, m=2.5):\n",
    "    roll_q1 = series.rolling(window, center=True).quantile(q1)\n",
    "    roll_q3 = series.rolling(window, center=True).quantile(q3)\n",
    "    iqr = roll_q3 - roll_q1\n",
    "    upper = roll_q3 + m * iqr\n",
    "    return series.clip(0, upper)\n",
    "\n",
    "# preprocess = 'iqr'\n",
    "preprocess = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['season'], prefix='season')\n",
    "dataset = dataset[dataset['days_since_launch'] > 0]\n",
    "\n",
    "dataset['month'] = dataset['date'].dt.month\n",
    "dataset['day']   = dataset['date'].dt.day \n",
    "# drop_cols = [\n",
    "#     # static\n",
    "#     'country', 'category', 'family', 'storage_gb', 'colour', \n",
    "#     # numeric\n",
    "#     # 'season_Fall', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday',\n",
    "#     'avg_temp', 'humidity', 'precip_mm',\n",
    "#     'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover',\n",
    "#     'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode',\n",
    "#     'shortwave_rad_MJ', 'vpd', 'cdd18', 'delta_temp',\n",
    "#     'delta_humidity',\n",
    "# ]\n",
    "drop_cols = []\n",
    "# past_cols = ['EMA_30', 'MA_30']\n",
    "past_cols = []\n",
    "future_cols = ['season_Fall', 'season_Spring', 'season_Summer', 'season_Winter', 'is_holiday','avg_temp', 'min_temp', 'max_temp', 'dewpoint', 'humidity', 'precip_mm',\n",
    "       'rain_mm', 'snow_mm', 'snow_depth_cm', 'pressure_msl', 'cloud_cover',\n",
    "       'wind_speed_avg', 'wind_speed_max', 'wind_gust_max', 'wind_dir_mode',\n",
    "       'shortwave_rad_MJ', 'vpd', 'hdd18', 'cdd18', 'delta_temp',\n",
    "       'delta_humidity', 'brent_usd', 'local_fx', 'spend_usd', 'days_since_launch',\n",
    "       'month', 'day']\n",
    "static_cols = ['country', 'category', 'family', 'storage_gb', 'colour', 'unit_price', 'life_days']\n",
    "\n",
    "dataset = dataset.drop(columns=drop_cols)\n",
    "future_cols = [col for col in future_cols if col not in drop_cols]\n",
    "static_cols = [col for col in static_cols if col not in drop_cols]\n",
    "\n",
    "\n",
    "if preprocess == 'clip':\n",
    "    print('clip')\n",
    "    low, high = dataset['demand'].quantile([0.00,0.95])\n",
    "    dataset['demand'] = dataset['demand'].clip(low, high)\n",
    "elif preprocess == 'iqr':\n",
    "    print('iqr')\n",
    "    dataset['demand'] = local_iqr_clip(dataset['demand'])\n",
    "dataset['demand'] = np.log1p(dataset['demand'])\n",
    "dataset['discount_pct'] = dataset['discount_pct']/100\n",
    "    \n",
    "series_raw, series, past_cov, future_cov = to_darts_time_series_group(\n",
    "    dataset=dataset,\n",
    "    target=target_col,\n",
    "    time_col=time_col,\n",
    "    group_cols=group_cols,\n",
    "    past_cols=past_cols,\n",
    "    future_cols=future_cols,\n",
    "    static_cols=static_cols,\n",
    "    freq='D', # daily\n",
    "    encode_static_cov=True, # so that the models can use the categorical variables (Agency & Product)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc90eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = split_grouped_darts_time_series(\n",
    "    series=series,\n",
    "    split_date=TEST_DATE\n",
    ")\n",
    "\n",
    "train, val = split_grouped_darts_time_series(\n",
    "    series=train_val,\n",
    "    split_date=VAL_DATE_OUT\n",
    ")\n",
    "\n",
    "# _, val = split_grouped_darts_time_series(\n",
    "#     series=train_val,\n",
    "#     split_date=VAL_DATE_IN\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986289ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461 365 731\n"
     ]
    }
   ],
   "source": [
    "print(train[0].n_timesteps, val[0].n_timesteps, test[0].n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1cf35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer arguments\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 100,\n",
    "    \"accelerator\": \"gpu\", \n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"input_chunk_length\": 90,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 512,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.0001},\n",
    "#  'loss_fn': MAELoss(),\n",
    " 'loss_fn': SmapeLoss(),\n",
    " 'use_layer_norm': True,\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2080d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "number of `past_covariates` features is <= `temporal_width_past`, leading to feature expansion.number of covariates: 1, `temporal_width_past=4`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX PRO 6000 Blackwell Workstation Edition') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                  | Type             | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0  | criterion             | SmapeLoss        | 0      | train\n",
      "1  | train_criterion       | SmapeLoss        | 0      | train\n",
      "2  | val_criterion         | SmapeLoss        | 0      | train\n",
      "3  | train_metrics         | MetricCollection | 0      | train\n",
      "4  | val_metrics           | MetricCollection | 0      | train\n",
      "5  | rin                   | RINorm           | 6      | train\n",
      "6  | past_cov_projection   | _ResidualBlock   | 788    | train\n",
      "7  | future_cov_projection | _ResidualBlock   | 5.0 K  | train\n",
      "8  | encoders              | Sequential       | 273 K  | train\n",
      "9  | decoders              | Sequential       | 20.7 K | train\n",
      "10 | temporal_decoder      | _ResidualBlock   | 840    | train\n",
      "11 | lookback_skip         | Linear           | 91     | train\n",
      "--------------------------------------------------------------------\n",
      "301 K     Trainable params\n",
      "0         Non-trainable params\n",
      "301 K     Total params\n",
      "1.205     Total estimated model params size (MB)\n",
      "49        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  76%|███████▌  | 2032/2678 [01:40<00:32, 20.14it/s, train_loss=0.681, val_loss=0.680]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\adam.py:225\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\pl_forecasting_module.py:243\u001b[39m, in \u001b[36mPLForecastingModule.training_step\u001b[39m\u001b[34m(self, train_batch, batch_idx)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"performs the training step\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_val_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\pl_forecasting_module.py:279\u001b[39m, in \u001b[36mPLForecastingModule._train_val_step\u001b[39m\u001b[34m(self, batch, name, criterion, metrics)\u001b[39m\n\u001b[32m    269\u001b[39m (\n\u001b[32m    270\u001b[39m     past_target,\n\u001b[32m    271\u001b[39m     past_covariates,\n\u001b[32m   (...)\u001b[39m\u001b[32m    276\u001b[39m     future_target,\n\u001b[32m    277\u001b[39m ) = batch\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_produce_train_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistoric_future_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatic_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._compute_loss(output, future_target, criterion, sample_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\pl_forecasting_module.py:529\u001b[39m, in \u001b[36mPLForecastingModule._produce_train_output\u001b[39m\u001b[34m(self, input_batch)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generates train output.\u001b[39;00m\n\u001b[32m    520\u001b[39m \n\u001b[32m    521\u001b[39m \u001b[33;03mFeeds `PLForecastingModule` with (past target + past cov + historic future cov (concatenated), future cov,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    527\u001b[39m \u001b[33;03m    ``(past target, past cov, historic future cov, future cov, static cov)``.\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_input_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\pl_forecasting_module.py:56\u001b[39m, in \u001b[36mio_processor.<locals>.forward_wrapper\u001b[39m\u001b[34m(self, x_in, *args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# apply reversible instance normalization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m past_features[:, :, : \u001b[38;5;28mself\u001b[39m.n_targets] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_targets\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# run the forward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m## COMMENT TO LOAD PRE-TRAINED MODEL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mfit_mixed_covariates_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTiDEModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecific_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTiDE_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train_series=train_val,\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# val_series=None,\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m time_tide = time.time() - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfit_mixed_covariates_model\u001b[39m\u001b[34m(model_cls, common_model_args, specific_model_args, model_name, past_cov, future_cov, train_series, val_series, max_samples_per_ts, save, path)\u001b[39m\n\u001b[32m    114\u001b[39m model = model_cls(model_name=model_name,\n\u001b[32m    115\u001b[39m                 **common_model_args,\n\u001b[32m    116\u001b[39m                 **specific_model_args)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TRAIN ARGS ===================================\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# VAL ARGS ======================================\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save: model.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:944\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m     series=seq2series(series),\n\u001b[32m    941\u001b[39m     past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m     future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1127\u001b[39m, in \u001b[36mTorchForecastingModel.fit_from_dataset\u001b[39m\u001b[34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_from_dataset\u001b[39m(\n\u001b[32m   1076\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1082\u001b[39m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1083\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[33;03m    Train the model with a specific :class:`darts.utils.data.TorchTrainingDataset` instance.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1316\u001b[39m, in \u001b[36mTorchForecastingModel._train\u001b[39m\u001b[34m(self, trainer, model, train_loader, val_loader)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28mself\u001b[39m.load_ckpt_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requires_training:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m   1323\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TiDEModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TiDE_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    # train_series=train_val,\n",
    "    val_series = val,\n",
    "    # val_series=None,\n",
    ")\n",
    "time_tide = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af58bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`predict()` was called with `n > output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:04<00:00,  0.48it/s]\n"
     ]
    }
   ],
   "source": [
    "best_tide = TiDEModel.load_from_checkpoint(model_name='TiDE_model', best=True)\n",
    "preds_tide = best_tide.predict(\n",
    "                    series            = train_val,\n",
    "                    past_covariates   = past_cov,\n",
    "                    future_covariates = future_cov,\n",
    "                    n                 = test[0].n_timesteps\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65437078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 122.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "group_cols = [\"sku\",\"city\"]  # 예시\n",
    "groups_df = (\n",
    "    dataset\n",
    "    .loc[:, group_cols]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=group_cols)   # from_group_dataframe 도 내부적으로 정렬하므로\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(groups_df))):\n",
    "    group_id = groups_df.iloc[i]\n",
    "    \n",
    "    pred = preds_tide[i].to_dataframe()\n",
    "    pred = pred.reset_index()\n",
    "    pred = pred.rename(columns={\"index\": \"date\"})\n",
    "    pred = pred.drop(columns=['discount_pct'])\n",
    "    # pred['demand'] = np.exp(pred['demand'])\n",
    "    \n",
    "    for j in range(len(pred)):\n",
    "        result.append({\n",
    "            \"sku\":  group_id[\"sku\"],\n",
    "            \"city\": group_id[\"city\"],\n",
    "            \"date\": pred['date'][j],\n",
    "            \"mean\": pred['demand'][j],\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df['mean'] = np.expm1(result_df['mean'])\n",
    "result_df['mean'] = result_df['mean'].round().astype(int)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "sub = pd.read_csv(\"extracted_contents/data/forecast_submission_template.csv\", parse_dates=[\"date\"])\n",
    "sub.drop(columns=['mean'], inplace=True)\n",
    "sub = sub.merge(result_df, on=['sku', 'city', 'date'], how='left')\n",
    "sub.to_csv(\"forecast_submission_template.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc71950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                date           mean\n",
      "count                         731000  731000.000000\n",
      "mean   2024-01-01 00:00:00.000000256     336.139944\n",
      "min              2023-01-01 00:00:00      48.000000\n",
      "25%              2023-07-02 00:00:00     156.000000\n",
      "50%              2024-01-01 00:00:00     290.000000\n",
      "75%              2024-07-02 00:00:00     434.000000\n",
      "max              2024-12-31 00:00:00    1393.000000\n",
      "std                              NaN     235.139178\n"
     ]
    }
   ],
   "source": [
    "print(sub.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c90c5f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`predict()` was called with `n > output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:03<00:00,  0.51it/s]\n",
      "[np.float64(1.4195569461322382), np.float64(1.4566750275791374), np.float64(1.0310019604818716), np.float64(1.5310409914885312), np.float64(1.3156873865726968), np.float64(1.4801144540917812), np.float64(1.2965308247767622), np.float64(1.343512112939254), np.float64(1.4712970896826536), np.float64(1.0279684596691487), np.float64(1.3255514922155207), np.float64(1.43492652089878), np.float64(0.998626676108548), np.float64(1.7079982397744007), np.float64(1.2506074537145409), np.float64(1.1830339152527503), np.float64(1.4827519572623729), np.float64(1.4948550089526536), np.float64(1.474645549743995), np.float64(1.3385685079951024), np.float64(1.3119245904537458), np.float64(1.4535403497895734), np.float64(1.1883006897913), np.float64(1.343840282604734), np.float64(1.2903594392486408), np.float64(1.2965384352797258), np.float64(1.098375396354964), np.float64(1.1897440905841292), np.float64(1.0719203767029497), np.float64(1.08664500480695), np.float64(1.009469681867984), np.float64(1.1835450737983257), np.float64(1.2497556655068462), np.float64(1.494338586447064), np.float64(1.4298698672293013), np.float64(1.0274364854384905), np.float64(1.5401144347817572), np.float64(1.538834711167499), np.float64(1.4510679018690888), np.float64(1.3229893487094382), np.float64(1.3999637547397643), np.float64(1.2070847907536733), np.float64(1.519975783681567), np.float64(1.398241598145977), np.float64(1.4470454062274525), np.float64(1.3368404634316846), np.float64(1.357308190542689), np.float64(1.4126952986108976), np.float64(1.1691974271894865), np.float64(1.3570663115239776), np.float64(1.7135160672796128), np.float64(1.4485162981937334), np.float64(1.5233489645755633), np.float64(1.4532254733676284), np.float64(1.2628016940743956), np.float64(1.2355551904628461), np.float64(1.2246416235685436), np.float64(1.6243739368539727), np.float64(1.2339240668131504), np.float64(1.197835771497784), np.float64(1.1052232847256092), np.float64(1.1472988211784385), np.float64(1.2135187604379596), np.float64(1.1791308323057998), np.float64(1.2899911168695464), np.float64(1.225196421384221), np.float64(1.4705063296903562), np.float64(1.406366395107095), np.float64(1.0520332934551466), np.float64(1.4387693470071685), np.float64(1.463638219246787), np.float64(1.025784123805497), np.float64(1.5651699096539484), np.float64(1.2482613173537458), np.float64(1.1560998990662048), np.float64(1.453356678727764), np.float64(1.225085061748181), np.float64(1.388449399986948), np.float64(1.373368262262193), np.float64(1.1956651025067804), np.float64(1.738735618351091), np.float64(1.1661203548639436), np.float64(1.3675804110530796), np.float64(1.1703927899130355), np.float64(1.280766841296187), np.float64(0.9923350533648637), np.float64(1.1439311925513076), np.float64(1.0398492736379157), np.float64(1.1336662100349242), np.float64(1.2864961426103116), np.float64(1.3117147707216794), np.float64(1.407888373788408), np.float64(1.2650376167163169), np.float64(1.4972537952872673), np.float64(1.4709835077404794), np.float64(1.7947159122914589), np.float64(1.4065628275657196), np.float64(1.3990272258690448), np.float64(1.301104990701168), np.float64(1.3446721739676515), np.float64(1.363524540181359), np.float64(1.2561961265139232), np.float64(1.0647528423175654), np.float64(1.3472217030226106), np.float64(1.4081920624636488), np.float64(1.4830679152329733), np.float64(1.3606908664514237), np.float64(1.323195749425064), np.float64(1.813755871425514), np.float64(1.3691855258161314), np.float64(1.5689587693271956), np.float64(1.254704576977017), np.float64(1.473370185794435), np.float64(1.612084182946064), np.float64(1.1000566705451327), np.float64(1.3335803248295202), np.float64(1.3483259751815033), np.float64(1.0931125422450352), np.float64(1.2676267225066595), np.float64(1.0800203342714672), np.float64(1.3844564319806554), np.float64(1.232197610931633), np.float64(1.1010996409847682), np.float64(0.994663182987253), np.float64(1.0198791996304197), np.float64(1.1566576536958826), np.float64(1.247682975280045), np.float64(1.1073965106202146), np.float64(1.3423697539997834), np.float64(1.7191864112639184), np.float64(1.183954243223927), np.float64(1.1265993482276713), np.float64(1.464036880035355), np.float64(1.1626969779264291), np.float64(1.06139689193123), np.float64(1.5116757084647257), np.float64(0.9233383765156616), np.float64(0.9610246527675823), np.float64(1.1209461461928498), np.float64(1.0520997859515377), np.float64(1.4791475932285776), np.float64(1.5964690721349413), np.float64(1.2833016513466335), np.float64(1.158171995876213), np.float64(1.4571456330227885), np.float64(1.3078020830751143), np.float64(1.5454011363595062), np.float64(1.2846043492370411), np.float64(1.3108396646650733), np.float64(1.2558306389168767), np.float64(1.2752676555353204), np.float64(1.1251415702788814), np.float64(1.2185434878003973), np.float64(1.4521103284408996), np.float64(1.1788572213892985), np.float64(1.3737050691412012), np.float64(1.47453088437681), np.float64(1.1874974838173793), np.float64(1.060681668921117), np.float64(1.2220760333486234), np.float64(1.181433359311476), np.float64(1.1745127699574571), np.float64(0.9994846702335691), np.float64(0.994971678794421), np.float64(1.683913218590976), np.float64(1.32203723536605), np.float64(1.34974911172966), np.float64(1.2832198246984066), np.float64(1.1650472781830306), np.float64(0.8740790985143025), np.float64(1.491103518401713), np.float64(1.3003797306755835), np.float64(1.4535107897270987), np.float64(1.4868395672827548), np.float64(1.4158187653842884), np.float64(1.476609894543032), np.float64(1.327227233700384), np.float64(1.5596024649348208), np.float64(1.3301675296035138), np.float64(1.3229312254584866), np.float64(1.2521737164225681), np.float64(1.5120875986087405), np.float64(1.488250105892908), np.float64(1.4886726109911077), np.float64(1.2078040281302416), np.float64(1.4739980719522028), np.float64(1.3156723773534877), np.float64(1.2229667448312225), np.float64(1.1549441076274125), np.float64(1.5771627041048868), np.float64(1.081849282410901), np.float64(1.154062044417876), np.float64(1.5151450714350638), np.float64(1.4051790733243346), np.float64(1.4268777881484473), np.float64(1.3772509768454937), np.float64(0.7971788664498369), np.float64(1.2653017155864), np.float64(1.2098256152183562), np.float64(1.2943273018291943), np.float64(1.3399002889879272), np.float64(1.4575388272792573), np.float64(1.7399915063449534), np.float64(1.3416207294728557), np.float64(1.278053136681444), np.float64(1.4647803964193051), np.float64(1.2775264359986147), np.float64(1.6034879673768858), np.float64(1.421125534265394), np.float64(1.6102034507094305), np.float64(1.389465195887801), np.float64(1.2813631588929772), np.float64(1.566476167974271), np.float64(1.2130017163795228), np.float64(1.290785918155344), np.float64(1.300697312997021), np.float64(1.1524079044756705), np.float64(1.4936604759650163), np.float64(1.0685230406238848), np.float64(1.122089216283031), np.float64(1.379050943712227), np.float64(1.247176953813592), np.float64(1.4152289862775256), np.float64(1.233351494679463), np.float64(1.0653189619071266), np.float64(1.251897237013326), np.float64(1.0947007888844187), np.float64(1.197845779636455), np.float64(1.191835860560854), np.float64(1.268480022014225), np.float64(1.3795557779483074), np.float64(1.396383727975128), np.float64(1.2572560556345929), np.float64(1.4379164352970233), np.float64(1.541443167801663), np.float64(0.9179497379829029), np.float64(1.138023484092816), np.float64(1.5696371462095176), np.float64(1.2778107133754086), np.float64(1.287242578288187), np.float64(1.2607960718384585), np.float64(1.246265810255637), np.float64(1.5395026310874398), np.float64(1.5037579357660766), np.float64(1.3356351126056305), np.float64(1.3013253999636896), np.float64(1.7144739705450713), np.float64(1.2856185207220125), np.float64(1.2766964479480276), np.float64(1.60615514861112), np.float64(0.8296348201695638), np.float64(1.32159600551878), np.float64(1.2469324840584055), np.float64(1.2833772601990274), np.float64(0.997486945377911), np.float64(1.3653922912970118), np.float64(1.5065319599064586), np.float64(1.5520603908467203), np.float64(1.5577991025169224), np.float64(1.3200358384026563), np.float64(1.4663535322729948), np.float64(1.4487569521375745), np.float64(1.4084162234585438), np.float64(1.0919131667749324), np.float64(1.5406845128032347), np.float64(1.200014188847784), np.float64(1.3512923395310144), np.float64(1.2663556380533418), np.float64(1.4434504314256946), np.float64(1.6222115209790573), np.float64(1.1617871441548924), np.float64(1.2038954704294056), np.float64(1.555694438042417), np.float64(1.2882791783307028), np.float64(1.6109172091728328), np.float64(1.3256152920383353), np.float64(1.5451719313131407), np.float64(1.0300039855712206), np.float64(1.1571393166029578), np.float64(1.0201300656599843), np.float64(1.5283651916207657), np.float64(1.4940324239570628), np.float64(1.319218366903234), np.float64(1.3996485266042225), np.float64(1.3803186786083177), np.float64(1.3399145870457798), np.float64(1.4603083184872347), np.float64(1.6111339259363073), np.float64(1.3280933015030256), np.float64(1.4029964907198398), np.float64(1.3244825331299288), np.float64(1.4395202218031435), np.float64(1.4411111874673597), np.float64(1.3236253114698577), np.float64(1.390191461907237), np.float64(1.0686440358021387), np.float64(1.1433311679459757), np.float64(1.2206362766403798), np.float64(1.3726932803393943), np.float64(1.2275059709165694), np.float64(1.2754063747849913), np.float64(1.5432694821432156), np.float64(0.9739343260306057), np.float64(1.3123748624161766), np.float64(1.1815248801338931), np.float64(1.2559133945815855), np.float64(1.2385324008037482), np.float64(0.9981678748373157), np.float64(1.1629014916222158), np.float64(1.243724138552735), np.float64(1.2399858562463795), np.float64(1.5312883906290986), np.float64(1.8396500503948867), np.float64(1.696584830016917), np.float64(1.3245441161159706), np.float64(1.276166936964355), np.float64(1.2794268272484752), np.float64(1.0588786627032936), np.float64(1.255227559899316), np.float64(1.538870052085357), np.float64(1.507846252416999), np.float64(1.4365535617462744), np.float64(1.6108553002752037), np.float64(1.3934371012266569), np.float64(1.4888269727504315), np.float64(0.9448899010551658), np.float64(1.6342692647716812), np.float64(1.3752817103131287), np.float64(1.388089063646649), np.float64(0.8935544611018826), np.float64(1.4284818692938064), np.float64(1.3694918148653787), np.float64(1.3684277565163452), np.float64(1.3727568358941133), np.float64(1.2291758868249791), np.float64(1.2464706370310943), np.float64(1.4129887238176813), np.float64(1.3825437167618295), np.float64(1.415125982425515), np.float64(1.4460083680408085), np.float64(1.2850900904000095), np.float64(1.2188007632122577), np.float64(1.362280053690543), np.float64(1.4186275670207316), np.float64(1.7369540315034648), np.float64(1.6226033139772011), np.float64(1.3950685029926082), np.float64(1.015289623696639), np.float64(1.3216301178647183), np.float64(1.6030991756341848), np.float64(1.7076799156907239), np.float64(1.1493453855981683), np.float64(1.514359279033543), np.float64(1.3053670740783814), np.float64(1.1069353080174675), np.float64(1.2991115649438298), np.float64(1.5706788828679552), np.float64(0.9985431608575565), np.float64(1.6037589856246224), np.float64(1.1936416421698528), np.float64(1.3138697310941672), np.float64(1.3297979539608606), np.float64(1.5100139609738876), np.float64(1.5255915486850982), np.float64(1.2698071788211758), np.float64(1.501439380820705), np.float64(1.2807510407470957), np.float64(1.4465093497916477), np.float64(0.9730091117273011), np.float64(1.219560427861658), np.float64(1.211317533079259), np.float64(1.2308400870997063), np.float64(1.245654960543219), np.float64(1.38760355462106), np.float64(1.8693278750685325), np.float64(1.7234123156903824), np.float64(1.33578452178113), np.float64(1.4352582007194228), np.float64(1.1867937087333011), np.float64(1.1784173283396333), np.float64(1.2175751606781453), np.float64(1.0526346647601648), np.float64(1.4954028418864538), np.float64(1.2903581148826895), np.float64(1.4357796600203103), np.float64(1.2658028235134777), np.float64(1.3469259123088249), np.float64(1.3535125434879094), np.float64(1.2327105541372867), np.float64(1.2463896699096901), np.float64(1.772291883205222), np.float64(1.3476390329897647), np.float64(1.2233250742662254), np.float64(1.1470223173440555), np.float64(1.2467973181592007), np.float64(1.3261324378600767), np.float64(1.2458892782034021), np.float64(1.287731375335069), np.float64(1.5977847962462361), np.float64(1.2003936087629552), np.float64(1.2728657131706707), np.float64(1.5021768814059104), np.float64(1.284515342996866), np.float64(1.1264436961539812), np.float64(1.1890506761131638), np.float64(1.6445071188202744), np.float64(1.1840153260667745), np.float64(1.5091339969796786), np.float64(1.20878356834171), np.float64(1.153095363206676), np.float64(1.2913130399214832), np.float64(0.9983633545439322), np.float64(1.1932396327010835), np.float64(1.384247693587997), np.float64(1.2797884214826416), np.float64(1.2646925777399953), np.float64(1.3929560954650875), np.float64(1.3243595987241537), np.float64(1.3873522114898245), np.float64(1.023831969356695), np.float64(1.6185824079971938), np.float64(1.2798884217837971), np.float64(1.4539746561453206), np.float64(1.3347282958016133), np.float64(1.3318210198218368), np.float64(1.504858505453178), np.float64(1.5868337796640497), np.float64(1.3225702565938917), np.float64(1.4172821510235052), np.float64(1.4042827502594233), np.float64(1.225171423848757), np.float64(1.3417648281229526), np.float64(1.2723418239742614), np.float64(1.7224614346208433), np.float64(1.2242147815770363), np.float64(1.1300552822095717), np.float64(1.560889401105045), np.float64(1.319273673571965), np.float64(1.4673123023376673), np.float64(1.4483309380357081), np.float64(1.3904391453052005), np.float64(1.2162610758646206), np.float64(1.5179969866814829), np.float64(1.268002314687859), np.float64(1.3878617482730669), np.float64(1.1433778077544186), np.float64(1.1752389843067483), np.float64(1.2915526391175507), np.float64(1.0191524560370901), np.float64(1.180810368290087), np.float64(1.4095869155439282), np.float64(1.1577379705168505), np.float64(1.4724262677631041), np.float64(1.5975184782048077), np.float64(1.2062364385985722), np.float64(1.3673357494967164), np.float64(1.6905615387903126), np.float64(1.3723918000446498), np.float64(1.1739607204800158), np.float64(1.2205777702917175), np.float64(1.6911045735977142), np.float64(1.441364799475265), np.float64(1.4107147337233754), np.float64(1.1457395697605657), np.float64(1.4150730863011194), np.float64(1.2164349972862607), np.float64(1.6756769824788644), np.float64(1.218933441937486), np.float64(1.4065229719520964), np.float64(1.23236368085623), np.float64(1.2838772324010754), np.float64(1.3595875115836724), np.float64(1.5569249945604), np.float64(1.0888408792461317), np.float64(1.599676907714565), np.float64(1.1793228727519356), np.float64(1.4376473146156243), np.float64(0.9871962413939182), np.float64(1.7285162930093723), np.float64(1.3533711230572274), np.float64(1.2802170993567699), np.float64(1.3564466102583617), np.float64(1.443728643482571), np.float64(1.3373844978711085), np.float64(1.2017500566951083), np.float64(0.9840191808684734), np.float64(1.2418302110973005), np.float64(1.3367321774458656), np.float64(1.2387971224841674), np.float64(1.0357816845636496), np.float64(1.3245976321219028), np.float64(1.4741075231261382), np.float64(1.177091071265247), np.float64(1.1364936719455958), np.float64(1.2816779825925957), np.float64(1.5814061296194484), np.float64(0.9909079032074944), np.float64(1.110402434479846), np.float64(1.3887338540316063), np.float64(1.4521059705597357), np.float64(1.0567776252660455), np.float64(1.4755571190959), np.float64(1.145323508113039), np.float64(1.331100020067054), np.float64(1.1275021157183513), np.float64(1.5137365729364989), np.float64(1.110431340312689), np.float64(1.3384791884056333), np.float64(1.278821797049943), np.float64(1.2366342962486767), np.float64(1.2613440923548562), np.float64(1.6138870106208583), np.float64(1.3575170546583621), np.float64(1.1174241161878853), np.float64(1.2702219899868548), np.float64(1.1973439253594989), np.float64(1.3733553742693463), np.float64(1.307424981399995), np.float64(1.2063043053436564), np.float64(1.3595104903978326), np.float64(1.3365611505529849), np.float64(1.259044703694354), np.float64(1.139851899387977), np.float64(1.15324201777594), np.float64(1.4955020759013324), np.float64(1.0675028262164494), np.float64(1.3801997368176189), np.float64(1.0816275399818116), np.float64(1.3919837957716101), np.float64(1.6521920625272395), np.float64(1.640651617100884), np.float64(1.173385988791626), np.float64(1.1804322959913758), np.float64(1.1709090911182025), np.float64(1.488075024093498), np.float64(1.69540368115052), np.float64(1.2689207975395964), np.float64(1.4661012224392183), np.float64(1.3527712175508397), np.float64(1.2708329024912524), np.float64(1.2106208436706587), np.float64(1.231158850526057), np.float64(1.2090541922138451), np.float64(1.2407365625543856), np.float64(1.1586886200461273), np.float64(1.366565205554621), np.float64(1.3035864490740852), np.float64(1.4865918272307754), np.float64(1.6930360456803963), np.float64(1.3446205683104657), np.float64(0.876458630660164), np.float64(0.9819125636482084), np.float64(1.424379596494916), np.float64(1.5468724965511027), np.float64(1.4820216256882877), np.float64(1.2342728897103221), np.float64(1.0526223938038761), np.float64(1.457638407600004), np.float64(1.3473851955087495), np.float64(1.371099529599273), np.float64(1.4394314471919731), np.float64(1.2651040679633303), np.float64(1.3303251594364676), np.float64(1.202783532511873), np.float64(1.1405944557284564), np.float64(1.5399162495496832), np.float64(1.2234171587436942), np.float64(1.4264255182151717), np.float64(1.3927144719391409), np.float64(1.2963766197223736), np.float64(1.308170678548605), np.float64(1.1544621632381056), np.float64(1.4869165752259583), np.float64(1.1445615629166281), np.float64(1.518568907334192), np.float64(1.4834272178439927), np.float64(1.044159489973287), np.float64(1.3034147575840906), np.float64(1.226803073219881), np.float64(1.0490016573343528), np.float64(1.324528926395576), np.float64(1.4475999297354407), np.float64(1.2813756757173191), np.float64(1.3237822412335931), np.float64(1.1685876167423987), np.float64(1.351667304464702), np.float64(1.551642231039609), np.float64(1.3297348625092373), np.float64(1.3632767335494602), np.float64(1.3729245007039217), np.float64(1.2378751495868943), np.float64(1.27570742742034), np.float64(1.3260604208723015), np.float64(1.6979080435626848), np.float64(1.2531557695688162), np.float64(1.369259150637874), np.float64(1.4079482803209384), np.float64(1.5771039241041518), np.float64(1.4678231405150834), np.float64(1.2713899447682597), np.float64(1.4642911302698298), np.float64(1.540970671969375), np.float64(1.2826225771020991), np.float64(1.5926201921608645), np.float64(1.2325888855810374), np.float64(1.1364930635408923), np.float64(1.3913907353332595), np.float64(1.6346531076490816), np.float64(1.3139178444774502), np.float64(1.406545866896019), np.float64(1.4470528180036564), np.float64(1.236244420288574), np.float64(1.3542809840584795), np.float64(1.347688672003856), np.float64(1.1484305607261012), np.float64(1.4076595676073416), np.float64(1.1676229355838028), np.float64(1.3962097434014202), np.float64(1.2114353292994886), np.float64(1.2006821980916007), np.float64(1.3928960730191868), np.float64(1.1679525752904671), np.float64(1.2092380736887023), np.float64(1.525621600428265), np.float64(1.362069564288329), np.float64(1.4726968766022268), np.float64(1.1982658561614645), np.float64(1.5072301572791402), np.float64(1.4811447580337196), np.float64(1.4250096106915733), np.float64(1.3232970363023866), np.float64(1.5126132684906806), np.float64(1.2353198801691891), np.float64(1.5426777393382514), np.float64(1.2962118379709673), np.float64(1.1742993267813713), np.float64(1.0825859457197364), np.float64(1.2487843482480816), np.float64(1.2781720980339375), np.float64(1.1710364720647077), np.float64(1.1051756831842943), np.float64(1.4283220299274468), np.float64(1.257642368000044), np.float64(1.493409620318794), np.float64(1.436655545572418), np.float64(1.145760504423897), np.float64(1.301373620521423), np.float64(1.4301272318782656), np.float64(1.444082063025637), np.float64(1.271130685380537), np.float64(1.095990066563914), np.float64(1.2923591223208952), np.float64(1.3367765288834745), np.float64(1.0474171166315995), np.float64(1.1275169122069808), np.float64(1.2981611172874419), np.float64(1.385635280673761), np.float64(1.6542926536646059), np.float64(1.0317236629329818), np.float64(1.3671369438331205), np.float64(1.2776179399143193), np.float64(1.2589250008792032), np.float64(1.388173843966061), np.float64(1.3780220962944076), np.float64(1.5343549494671223), np.float64(1.2172941165624578), np.float64(1.2944985775213347), np.float64(1.7260442494978805), np.float64(1.5728669194663791), np.float64(1.462500115761503), np.float64(1.3800827183778224), np.float64(1.289196529945539), np.float64(1.52233980924167), np.float64(1.4645343536279731), np.float64(1.1920008798769222), np.float64(1.3062890576026278), np.float64(1.2515343015418896), np.float64(1.5481894213858038), np.float64(1.589229882462562), np.float64(1.3632279584724998), np.float64(1.3825551114882235), np.float64(1.3431064311970935), np.float64(1.3732068894200375), np.float64(1.131398715734187), np.float64(1.4579308033196918), np.float64(1.3824977331883899), np.float64(1.3201579875569305), np.float64(1.5678109566225769), np.float64(1.167617728891532), np.float64(1.5438352426513802), np.float64(1.4432547286458643), np.float64(1.4261999220862958), np.float64(1.5059939137443348), np.float64(1.2492019195054667), np.float64(1.2657842553262626), np.float64(1.5397940621373951), np.float64(1.671843071007027), np.float64(1.0707656092447457), np.float64(1.602146276662279), np.float64(1.2912799155910706), np.float64(1.2329374644385993), np.float64(1.2522073465346455), np.float64(1.4050498388416663), np.float64(1.1448388241650818), np.float64(1.36268433785995), np.float64(1.4560040471796407), np.float64(1.4996685308683098), np.float64(1.3080780815158297), np.float64(1.0594868470856424), np.float64(1.2603673915412732), np.float64(1.5846751655375562), np.float64(1.348295183719124), np.float64(1.3446125103756645), np.float64(1.0442850496720937), np.float64(1.164458789872883), np.float64(1.6041577268720837), np.float64(1.0903691440298575), np.float64(1.0887057857864066), np.float64(1.296464055451736), np.float64(1.553356612248965), np.float64(1.369165973636817), np.float64(1.3399159356898578), np.float64(1.3851157237981004), np.float64(1.0510599074129514), np.float64(1.5451064454569798), np.float64(1.397962660623188), np.float64(1.5078300678536602), np.float64(0.9771588386494967), np.float64(0.9498408891315967), np.float64(1.1751586717116207), np.float64(1.5249016943294231), np.float64(1.261106976688954), np.float64(1.2199586271796847), np.float64(1.3462349527517663), np.float64(1.5905368816754015), np.float64(1.4382857033975351), np.float64(1.3401463773716324), np.float64(1.3626235438321987), np.float64(1.4300559358129528), np.float64(1.3323218256261828), np.float64(1.4818260829306678), np.float64(1.4345997748817259), np.float64(1.2639223431770414), np.float64(1.3006555382901428), np.float64(1.5549889204160021), np.float64(1.5306501348699584), np.float64(1.5696816571656944), np.float64(1.308776530457503), np.float64(1.396070024586719), np.float64(1.2551032966435038), np.float64(1.556603245275015), np.float64(1.2735247643030616), np.float64(1.3893511798328737), np.float64(1.3861626393681994), np.float64(1.2371452109123782), np.float64(1.4184561034658099), np.float64(1.2059563078187603), np.float64(1.2521090206697718), np.float64(1.3064190325062435), np.float64(1.1884090585515623), np.float64(1.4056568157492124), np.float64(1.5315407927998341), np.float64(1.2866617536449987), np.float64(1.1654955364394597), np.float64(1.4295423139461678), np.float64(1.1311151444637126), np.float64(1.3348125738429688), np.float64(1.4982172754309404), np.float64(0.9669844011822805), np.float64(1.2238812339029836), np.float64(1.3182681614272285), np.float64(1.331945896532455), np.float64(1.1974010449645904), np.float64(1.8128736469134672), np.float64(1.3771631336831887), np.float64(1.5036300864120649), np.float64(1.2015488868479556), np.float64(1.1728637397308581), np.float64(1.247948411452389), np.float64(1.411719312854514), np.float64(1.7248506300842141), np.float64(1.681316098038007), np.float64(1.2483547340756482), np.float64(1.0518058961065087), np.float64(1.4654219380056952), np.float64(1.4842694669406065), np.float64(1.1512239934081152), np.float64(1.4094631559660995), np.float64(1.2902454398183354), np.float64(1.3637279557010793), np.float64(1.0252553507056592), np.float64(1.2998843299969027), np.float64(1.2999942097405812), np.float64(1.4127075678096987), np.float64(1.3348028154968306), np.float64(1.5215587532650914), np.float64(1.3241831942058662), np.float64(1.1527535765228516), np.float64(1.1714489362134841), np.float64(1.6514832419782377), np.float64(1.5379519958236034), np.float64(1.468269436230222), np.float64(1.5660887641042505), np.float64(1.2094481636823269), np.float64(1.1035518276262983), np.float64(1.0077412769665033), np.float64(1.1572808718829868), np.float64(1.3145614012860476), np.float64(1.41726566381364), np.float64(1.41420437360809), np.float64(1.2614376880001872), np.float64(1.4009665226834283), np.float64(1.6271158485416481), np.float64(1.110917702664516), np.float64(1.4330887042297797), np.float64(1.2230665594319765), np.float64(1.726265007408885), np.float64(1.2195519107460306), np.float64(1.196393100216013), np.float64(1.180867576007352), np.float64(1.4931466939212312), np.float64(1.3270423142690349), np.float64(1.5728683440716449), np.float64(1.8362040029494309), np.float64(1.2236627286245503), np.float64(1.4481948722902085), np.float64(1.2341985621087024), np.float64(1.4324277872813023), np.float64(1.0649509675469577), np.float64(1.2646606297166751), np.float64(1.5498152492244734), np.float64(1.1818442273724474), np.float64(1.2656351370006536), np.float64(1.2716299251845526), np.float64(1.2765108995677097), np.float64(1.1139655721827482), np.float64(1.069695181995247), np.float64(1.344306367698819), np.float64(1.3422273523588928), np.float64(1.0162787406754084), np.float64(1.1904451620840195), np.float64(1.358647744899629), np.float64(1.552094989370847), np.float64(1.015733247320028), np.float64(1.3714558932163214), np.float64(1.1743422407835302), np.float64(1.0382827515459105), np.float64(1.1446047935929287), np.float64(0.9223142851577124), np.float64(1.195454284779339), np.float64(1.5337852800531002), np.float64(1.4789519546252838), np.float64(1.4044304744725304), np.float64(1.6215555681914933), np.float64(1.3173573313709641), np.float64(1.539870263808534), np.float64(1.432058535776066), np.float64(1.4887607167316363), np.float64(1.327685368235926), np.float64(1.4556058848265694), np.float64(1.2572565057115996), np.float64(1.507422141516817), np.float64(1.233553261868196), np.float64(1.12239838618855), np.float64(1.1221304079333252), np.float64(1.3174687487065473), np.float64(1.231108332439868), np.float64(1.4777269927452958), np.float64(1.3878946437536366), np.float64(1.5283059694843368), np.float64(1.5880166305746763), np.float64(1.0887898085896444), np.float64(1.808691105557034), np.float64(1.4446195477408361), np.float64(1.2531251117460496), np.float64(1.0817002017179487), np.float64(1.4709365735476325), np.float64(1.019457225631433), np.float64(1.2846902989566469), np.float64(1.2600282007580517), np.float64(1.557320129458937), np.float64(1.4934642159153242), np.float64(0.9642500175420265), np.float64(1.1886068245915573), np.float64(1.4125607469415655), np.float64(1.1594983388817657), np.float64(1.5867241368772562), np.float64(1.2491479719454646), np.float64(1.494597994386758), np.float64(1.4006837814963204), np.float64(1.147236923614759), np.float64(1.670603682176137), np.float64(1.1533331985536601), np.float64(1.2881883515150536), np.float64(1.103748283335467), np.float64(1.3330368317783061), np.float64(1.3338206675693876), np.float64(1.2992315406059538), np.float64(1.36464781643802), np.float64(1.1029098675117832), np.float64(1.3062507536658352), np.float64(1.3252102380257915), np.float64(1.4063817828490437), np.float64(1.1404654645492491), np.float64(1.0809777252454138), np.float64(1.5053141126015115), np.float64(1.3544446647042916), np.float64(1.4417635278238217), np.float64(1.2064732881731204), np.float64(1.4889280896344732), np.float64(1.0975929377316989), np.float64(1.2827034199169576), np.float64(1.4025697999250095), np.float64(1.1615943090748166), np.float64(1.494512428864562), np.float64(1.1395713471453321), np.float64(1.3134394524733788), np.float64(1.3552882080298692), np.float64(1.2967114296023416), np.float64(1.0152996579745888), np.float64(1.3477287733097787), np.float64(1.5319620932205433), np.float64(1.2122364774609755), np.float64(1.3157190770014773), np.float64(1.220899858730387), np.float64(1.0997352323563132), np.float64(1.565186144411742), np.float64(1.471583575728431), np.float64(1.3772418278041216), np.float64(1.298437689797086), np.float64(1.188730314944144), np.float64(1.3789180993998664), np.float64(1.151923907687869), np.float64(1.4046349478008886), np.float64(1.08873556966131), np.float64(1.2867362172415309), np.float64(1.3335052804288743), np.float64(1.352076459487261), np.float64(1.3884103785713129), np.float64(0.954422996757268), np.float64(1.1746898201372848), np.float64(1.52426802469667), np.float64(1.4997265718228994), np.float64(1.0941943493716144), np.float64(1.7522904224463762), np.float64(1.6095504330605261), np.float64(1.4334609638332971), np.float64(1.5177017603550969), np.float64(1.3662617677092936), np.float64(1.3076188846785726), np.float64(1.311903192797637), np.float64(0.9814379501591943), np.float64(1.2653563125328042), np.float64(1.4007585229199004), np.float64(1.5852821165308144), np.float64(1.1854833995035792), np.float64(1.2810777766900714), np.float64(1.50707295800538), np.float64(1.742059348972895), np.float64(1.4092804318905854), np.float64(1.2908925733008212), np.float64(1.432829406225007), np.float64(1.2576792504734144), np.float64(1.4753351217302597), np.float64(1.371818844221527), np.float64(1.1519577490347699), np.float64(1.4606664026470155), np.float64(1.5390142779737743), np.float64(0.9992818099244607), np.float64(1.1972388154622555), np.float64(0.9925130442604864), np.float64(0.9177381012120532), np.float64(1.137323476445401), np.float64(1.2544268935691731), np.float64(1.407886846331527), np.float64(1.208421070287889), np.float64(1.2452232525799107), np.float64(1.6119271351171167), np.float64(1.2012509318950242), np.float64(1.462210869049274), np.float64(1.7988115084523253), np.float64(1.1858472530824133), np.float64(1.2549910279237821), np.float64(1.1044678779486572), np.float64(1.0350568211897013), np.float64(1.3841028412420948), np.float64(1.2659659531034455), np.float64(1.3651219325501422), np.float64(1.1749714385857546), np.float64(1.4990225674633246), np.float64(1.2348273810042014), np.float64(1.3673776862377844), np.float64(1.3904133324419237), np.float64(1.2641697854336253), np.float64(1.122190038206449), np.float64(1.1628278844014508), np.float64(1.268811117028036), np.float64(1.3421697934591168), np.float64(1.1622510110162771), np.float64(1.4832260729631472)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_tide = TiDEModel.load_from_checkpoint(model_name='TiDE_model', best=True)\n",
    "preds_tide = best_tide.predict(\n",
    "                    series            = train,\n",
    "                    past_covariates   = past_cov,\n",
    "                    future_covariates = future_cov,\n",
    "                    n                 = val[0].n_timesteps\n",
    "                )\n",
    "\n",
    "maes_tide = eval_forecasts(preds_tide, val, mae, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mae\n",
    "\n",
    "# test_series, pred_series 는 각자 List[TimeSeries] (multivariate) 라고 가정\n",
    "val_demand = [ ts.univariate_component('demand') for ts in val ]\n",
    "pred_demand = [ ts.univariate_component('demand') for ts in preds_tide ]\n",
    "\n",
    "maes_tide = eval_forecasts(pred_demand, val_demand, mae, plot=True)\n",
    "# errors = mae(test_demand, pred_demand)\n",
    "# print(\"Demand MAE:\", errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45ac8e",
   "metadata": {},
   "source": [
    "# TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21edd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer arguments\n",
    "early_stopping_args = {\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"patience\": 50,\n",
    "    \"min_delta\": 1e-3,\n",
    "    \"mode\": \"min\",\n",
    "}\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "    \"max_epochs\": 200,\n",
    "    \"accelerator\": \"gpu\", # uncomment for gpu use\n",
    "    \"callbacks\": [EarlyStopping(**early_stopping_args)],\n",
    "    \"enable_progress_bar\":True\n",
    "}\n",
    "\n",
    "common_model_args = {\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"input_chunk_length\": 150,\n",
    "    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n",
    "    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n",
    "    \"force_reset\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ac34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    \"position\": {\"past\": [\"relative\"], \"future\": [\"relative\"]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "best_hp = {\n",
    " 'optimizer_kwargs': {'lr':0.00005},\n",
    " 'loss_fn': MAELoss(),\n",
    " 'use_reversible_instance_norm': True,\n",
    " 'add_encoders':encoders,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5bfb6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX PRO 6000 Blackwell Workstation Edition') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | criterion                         | MAELoss                          | 0      | train\n",
      "1  | train_criterion                   | MAELoss                          | 0      | train\n",
      "2  | val_criterion                     | MAELoss                          | 0      | train\n",
      "3  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "4  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "5  | rin                               | RINorm                           | 4      | train\n",
      "6  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "7  | static_covariates_vsn             | _VariableSelectionNetwork        | 2.3 K  | train\n",
      "8  | encoder_vsn                       | _VariableSelectionNetwork        | 8.2 K  | train\n",
      "9  | decoder_vsn                       | _VariableSelectionNetwork        | 5.9 K  | train\n",
      "10 | static_context_grn                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K  | train\n",
      "12 | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K  | train\n",
      "13 | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K  | train\n",
      "14 | lstm_encoder                      | LSTM                             | 2.2 K  | train\n",
      "15 | lstm_decoder                      | LSTM                             | 2.2 K  | train\n",
      "16 | post_lstm_gan                     | _GateAddNorm                     | 576    | train\n",
      "17 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K  | train\n",
      "18 | multihead_attn                    | _InterpretableMultiHeadAttention | 676    | train\n",
      "19 | post_attn_gan                     | _GateAddNorm                     | 576    | train\n",
      "20 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "21 | pre_output_gan                    | _GateAddNorm                     | 576    | train\n",
      "22 | output_layer                      | Linear                           | 34     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "30.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.0 K    Total params\n",
      "0.120     Total estimated model params size (MB)\n",
      "527       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  21%|██        | 2134/10243 [15:00<56:59,  2.37it/s, train_loss=1.420, val_loss=0.977]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\optim\\adam.py:225\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:241\u001b[39m, in \u001b[36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[39m\u001b[34m(loss)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_fn\u001b[39m(loss: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:213\u001b[39m, in \u001b[36mStrategy.backward\u001b[39m\u001b[34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.pre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.post_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:73\u001b[39m, in \u001b[36mPrecision.backward\u001b[39m\u001b[34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1097\u001b[39m, in \u001b[36mLightningModule.backward\u001b[39m\u001b[34m(self, loss, *args, **kwargs)\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m## COMMENT TO LOAD PRE-TRAINED MODEL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mfit_mixed_covariates_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTFTModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_model_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecific_model_args\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_hp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTFT_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m time_tft = time.time() - start\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfit_mixed_covariates_model\u001b[39m\u001b[34m(model_cls, common_model_args, specific_model_args, model_name, past_cov, future_cov, train_series, val_series, max_samples_per_ts, save, path)\u001b[39m\n\u001b[32m    114\u001b[39m model = model_cls(model_name=model_name,\n\u001b[32m    115\u001b[39m                 **common_model_args,\n\u001b[32m    116\u001b[39m                 **specific_model_args)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TRAIN ARGS ===================================\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m                \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples_per_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# VAL ARGS ======================================\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_past_covariates\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_future_covariates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save: model.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:944\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m     series=seq2series(series),\n\u001b[32m    941\u001b[39m     past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m     future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\utils\\torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1127\u001b[39m, in \u001b[36mTorchForecastingModel.fit_from_dataset\u001b[39m\u001b[34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_from_dataset\u001b[39m(\n\u001b[32m   1076\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1082\u001b[39m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1083\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[33;03m    Train the model with a specific :class:`darts.utils.data.TorchTrainingDataset` instance.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1316\u001b[39m, in \u001b[36mTorchForecastingModel._train\u001b[39m\u001b[34m(self, trainer, model, train_loader, val_loader)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28mself\u001b[39m.load_ckpt_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requires_training:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m   1323\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tjqtj\\miniconda3\\envs\\ml\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "past_cov = None if not past_cov else past_cov\n",
    "\n",
    "start = time.time()\n",
    "## COMMENT TO LOAD PRE-TRAINED MODEL\n",
    "fit_mixed_covariates_model(\n",
    "    model_cls = TFTModel,\n",
    "    common_model_args = common_model_args,\n",
    "    specific_model_args = best_hp,\n",
    "    model_name = 'TFT_model',\n",
    "    past_cov = past_cov,\n",
    "    future_cov = future_cov,\n",
    "    train_series = train,\n",
    "    val_series = val,\n",
    ")\n",
    "time_tft = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b721f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`predict()` was called with `n > output_chunk_length`: using auto-regression to forecast the values after `output_chunk_length` points. The model will access `(n - output_chunk_length)` future values of your `past_covariates` (relative to the first predicted time step). To hide this warning, set `show_warnings=False`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 8/8 [07:37<00:00,  0.02it/s]\n"
     ]
    }
   ],
   "source": [
    "best_tft = TFTModel.load_from_checkpoint(model_name='TFT_model', best=True)\n",
    "preds_tft = best_tft.predict(\n",
    "                    series            = train_val,\n",
    "                    past_covariates   = past_cov,\n",
    "                    future_covariates = future_cov,\n",
    "                    n                 = test[0].n_timesteps\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178bcd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 121.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "group_cols = [\"sku\",\"city\"]  # 예시\n",
    "groups_df = (\n",
    "    dataset\n",
    "    .loc[:, group_cols]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=group_cols)   # from_group_dataframe 도 내부적으로 정렬하므로\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(groups_df))):\n",
    "    group_id = groups_df.iloc[i]\n",
    "    \n",
    "    pred = preds_tft[i].to_dataframe()\n",
    "    pred = pred.reset_index()\n",
    "    pred = pred.rename(columns={\"index\": \"date\"})\n",
    "    pred = pred.drop(columns=['discount_pct'])\n",
    "    # pred['demand'] = np.exp(pred['demand'])\n",
    "    \n",
    "    for j in range(len(pred)):\n",
    "        result.append({\n",
    "            \"sku\":  group_id[\"sku\"],\n",
    "            \"city\": group_id[\"city\"],\n",
    "            \"date\": pred['date'][j],\n",
    "            \"mean\": pred['demand'][j],\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df['mean'] = np.expm1(result_df['mean'])\n",
    "result_df['mean'] = result_df['mean'].round().astype(int)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "sub = pd.read_csv(\"extracted_contents/data/forecast_submission_template.csv\", parse_dates=[\"date\"])\n",
    "sub.drop(columns=['mean'], inplace=True)\n",
    "sub = sub.merge(result_df, on=['sku', 'city', 'date'], how='left')\n",
    "sub.to_csv(\"forecast_submission_template.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
