# src/models/xgboost_subset_berlin_sku0001.py
# XGBoost ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸(2022 ê²€ì¦ ë™ì¼) + 2023-01-01 ~ 2023-06-30 êµ¬ê°„ì— ëŒ€í•´
# Berlin, SKU0001 ì¡°í•©ë§Œ ë°°ì¹˜ ì˜ˆì¸¡ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì„œë¸Œì…‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

import pandas as pd
import numpy as np
from datetime import datetime
import time
from tqdm import tqdm
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import sys
from pathlib import Path

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—†ì´ ìŠ¤í¬ë¦½íŠ¸ ë‹¨ë… ì‹¤í–‰ì„ ì§€ì›í•˜ê¸° ìœ„í•œ ê²½ë¡œ ì£¼ì…
_SCRIPT = Path(__file__).resolve()
_PROJECT_ROOT = _SCRIPT.parents[2]
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

# ê¸°ì¡´ xgboost_modelì˜ ìœ í‹¸/ì „ì²˜ë¦¬/í•™ìŠµ/ì‹œê°í™” ì¬ì‚¬ìš©
from src.models.xgboost_model import (
    print_progress,
    load_enhanced_training_data,
    prepare_lightgbm_features,
    analyze_multicollinearity,
    train_xgboost_model,
    create_xgboost_validation_visualization,
    create_xgboost_full_timeline_visualization,
    get_hardcoded_event_periods,
    DATA_DIR,
)

def create_subset_visualization(demand_data: pd.DataFrame, result_df: pd.DataFrame, start_time):
    """Berlin - SKU0001 ì „ìš© íƒ€ì„ë¼ì¸ ì‹œê°í™” (2022-2024, ì˜ˆì¸¡ì€ 2023-01~06ë§Œ í‘œì‹œ)"""
    city, sku = 'Berlin', 'SKU0001'
    print_progress(f"ğŸ“Š Plotting {city}-{sku}...", start_time)

    # ì‹¤ì œ ë°ì´í„° (2022-2022)
    actual = demand_data[(demand_data['city'] == city) & (demand_data['sku'] == sku)][['date','demand']].copy()
    actual = actual[actual['date'] >= pd.Timestamp('2022-01-01')]
    # ì˜ˆì¸¡ ë°ì´í„° (ì„œë¸Œì…‹)
    pred = result_df[(result_df['city'] == city) & (result_df['sku'] == sku)][['date','mean']].copy()
    pred = pred.rename(columns={'mean':'demand'})

    plt.figure(figsize=(14,5))
    if len(actual) > 0:
        actual = actual.sort_values('date')
        plt.plot(actual['date'], actual['demand'], label='Actual (2018-2022)', color='blue', linewidth=2)
    if len(pred) > 0:
        pred = pred.sort_values('date')
        plt.plot(pred['date'], pred['demand'], label='Predicted (2023-01~06)', color='red', linewidth=2)

    plt.axvline(pd.Timestamp('2023-01-01'), color='green', linestyle=':', alpha=0.7, label='Forecast start')
    plt.title('XGBoost Subset: Berlin - SKU0001 (2022-2024, Pred 2023-01~06)')
    plt.xlabel('Date')
    plt.ylabel('Demand')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    # xì¶• ë²”ìœ„ ê³ ì • (ì˜¤ë¥¸ìª½ ë: 2023-06-30)
    plt.xlim([pd.Timestamp('2022-06-01'), pd.Timestamp('2023-06-30')])

    out_path = DATA_DIR / 'xgb_subset_berlin_sku0001_timeline.png'
    plt.savefig(out_path, dpi=300, bbox_inches='tight')
    plt.close()
    print_progress(f"ğŸ“ Saved subset plot: {out_path}", start_time)
    
def predict_future_xgboost_subset(model, demand_data, feature_cols, label_encoders, start_time):
    """Berlin, SKU0001ë§Œ 2023-01-01 ~ 2023-06-30 ê¸°ê°„ ì˜ˆì¸¡ (ë‚ ì§œ ë°°ì¹˜ ì˜ˆì¸¡)"""
    print_progress("ğŸ”® XGBoost ì„œë¸Œì…‹ ë¯¸ë˜ ì˜ˆì¸¡ (Berlin-SKU0001, 2023-01~06) ì‹œì‘", start_time)

    # 2023-06-30ê¹Œì§€ë§Œ
    future_dates = pd.date_range(start='2023-01-01', end='2023-06-30', freq='D')

    # ì™¸ìƒ/ë³´ì¡° í”¼ì²˜ ë¡œë“œ (xgboost_modelê³¼ ë™ì¼ ì²˜ë¦¬)
    oil = pd.read_csv(DATA_DIR / "oil_price_processed.csv", parse_dates=["date"])
    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    currency = pd.read_csv(DATA_DIR / "currency_processed.csv", parse_dates=["date"])
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence_processed.csv", parse_dates=["date"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])

    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']

    marketing_agg = marketing.groupby(['date', 'country'])['spend_usd'].sum().reset_index()
    country_cov = consumer_conf[['date', 'country', 'confidence_index']]
    country_cov = country_cov.merge(marketing_agg, on=['date', 'country'], how='left')
    country_cov = country_cov.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how='left')
    country_cov = country_cov.merge(calendar[['date', 'country', 'season']], on=['date', 'country'], how='left')

    cov_date = oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']]
    cov_date = cov_date.merge(currency[['date'] + fx_cols], on='date', how='left')

    cov_future = country_cov.merge(cov_date, on='date', how='left').sort_values(['country','date'])
    # ê°„ë‹¨ ë³´ê°„/ë³´ì • (ì„œë¸Œì…‹ì—ì„œë„ ë™ì¼)
    try:
        cov_future[['confidence_index','spend_usd','avg_temp','humidity']] = (
            cov_future.groupby('country')[['confidence_index','spend_usd','avg_temp','humidity']].ffill()
        )
        for c in ['confidence_index','spend_usd','avg_temp','humidity']:
            med = cov_future[c].median() if cov_future[c].notna().any() else 0.0
            cov_future[c] = cov_future[c].fillna(med)
        if 'season' in cov_future.columns:
            cov_future['season'] = cov_future.groupby('country')['season'].ffill().fillna('')
        cov_date = cov_date.sort_values('date')
        for c in ['brent_usd','pct_change','volatility_7d'] + fx_cols:
            if c in cov_date.columns:
                cov_date[c] = cov_date[c].ffill()
    except Exception as e:
        print(f"[WARN] cov ffill failed: {e}")

    # ì¡°í•© ê³ ì •: Berlin, SKU0001
    city_target = 'Berlin'
    sku_target = 'SKU0001'
    combos = pd.DataFrame([{'city': city_target, 'sku': sku_target}])

    # ìµœê·¼ ë°ì´í„°/ë²„í¼ ì¤€ë¹„
    result = []
    cov_future_idx = cov_future.set_index(['date','country']).sort_index()
    cov_date_idx = cov_date.set_index('date').sort_index()
    sku_launch_map = {row['sku']: row['launch_date'] for _, row in sku_meta.iterrows()}

    # í•´ë‹¹ ì¡°í•©ì˜ ìµœê·¼ 60ê°œ
    recent_data = demand_data[(demand_data['city'] == city_target) & (demand_data['sku'] == sku_target)].tail(60)
    if len(recent_data) == 0:
        print("âš ï¸ Berlin-SKU0001 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['sku','city','date','mean'])

    buf = recent_data['demand'].dropna().tolist() or [0.0]
    country = recent_data['country'].iloc[0]
    family = recent_data['family'].iloc[0]
    season_fixed = recent_data['season'].iloc[0]

    # ë°”ë‹¥ê°’(ìµœê·¼ 14ì¼ í‰ê· ì˜ 10%)
    floor_val = max(0.0, float(recent_data['demand'].tail(14).mean()) * 0.10)

    print_progress(f"ğŸ“Š ì˜ˆì¸¡ ëŒ€ìƒ(ì„œë¸Œì…‹): {city_target}-{sku_target} Ã— {len(future_dates)}ì¼", start_time)

    for date in tqdm(future_dates, desc="ì˜ˆì¸¡ ì§„í–‰(ì„œë¸Œì…‹)"):
        row = {
            'date': date, 'city': city_target, 'sku': sku_target,
            'month': date.month, 'dayofyear': date.dayofyear, 'weekday': date.weekday(),
            'country': country, 'family': family, 'season': season_fixed,
        }

        # êµ­ê°€ ê³µë³€ëŸ‰
        try:
            cr = cov_future_idx.loc[(date, country)]
            row['confidence_index'] = cr.get('confidence_index', 0)
            row['spend_usd'] = cr.get('spend_usd', 0)
            row['avg_temp'] = cr.get('avg_temp', 0)
            row['humidity'] = cr.get('humidity', 0)
            row['season'] = cr.get('season', row.get('season',''))
        except Exception:
            for c in ['confidence_index','spend_usd','avg_temp','humidity']:
                row[c] = 0

        # ë‚ ì§œ ê³µë³€ëŸ‰
        try:
            dr = cov_date_idx.loc[date]
            row['brent_usd'] = dr.get('brent_usd', 0)
            row['pct_change'] = dr.get('pct_change', 0)
            row['volatility_7d'] = dr.get('volatility_7d', 0)
            for fx in fx_cols:
                row[fx] = dr.get(fx, 0)
        except Exception:
            for c in ['brent_usd','pct_change','volatility_7d'] + fx_cols:
                row[c] = 0

        # days_since_launch
        ld = sku_launch_map.get(sku_target, None)
        if ld is not None and pd.notna(ld):
            row['days_since_launch'] = max(0, (date - ld).days)
        else:
            row['days_since_launch'] = 0

        # ì´ë²¤íŠ¸ í”Œë˜ê·¸
        is_event = 0
        for (cty, year), (s, e) in get_hardcoded_event_periods().items():
            if cty == country and date >= pd.to_datetime(s) and date <= pd.to_datetime(e):
                is_event = 1
                break
        row['is_event'] = is_event

        # ìˆ˜ìš” ì‹œê³„ì—´ í”¼ì²˜
        for lag in [1,3,7,14]:
            row[f'demand_lag_{lag}'] = buf[-lag] if len(buf) >= lag else (buf[0] if len(buf) > 0 else 0)
        for window in [7,14]:
            if len(buf) > 0:
                series = buf[-window:] if len(buf) >= window else buf
                row[f'demand_rolling_mean_{window}'] = float(np.mean(series))
                row[f'demand_rolling_std_{window}'] = float(np.std(series, ddof=1)) if len(series) > 1 else 0.0
            else:
                row[f'demand_rolling_mean_{window}'] = 0.0
                row[f'demand_rolling_std_{window}'] = 0.0

        # ê°€ê²©/í• ì¸ ìµœê·¼ê°’ ë° ì‹œê³„ì—´
        if 'unit_price' in recent_data.columns and len(recent_data) > 0:
            row['unit_price'] = float(recent_data['unit_price'].ffill().iloc[-1])
        if 'discount_pct' in recent_data.columns and len(recent_data) > 0:
            row['discount_pct'] = float(recent_data['discount_pct'].ffill().iloc[-1])
            if row['discount_pct'] > 1.0:
                row['discount_pct'] = row['discount_pct'] / 100.0
        for col in ['discount_pct','unit_price']:
            if col in recent_data.columns:
                for lag in [1,3,7,14]:
                    row[f'{col}_lag_{lag}'] = recent_data[col].iloc[-lag] if len(recent_data) >= lag else (recent_data[col].iloc[0] if len(recent_data) > 0 else 0)
                for window in [7,14]:
                    vals = recent_data[col].tail(window)
                    row[f'{col}_rolling_mean_{window}'] = vals.mean() if len(vals) > 0 else (recent_data[col].mean() if len(recent_data) > 0 else 0)

        # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
        for col in ['city','sku','country','family','season']:
            enc_col = f"{col}_encoded"
            if enc_col in feature_cols:
                val = str(row.get(col,''))
                if col in label_encoders and len(label_encoders[col].classes_) > 0:
                    if val not in label_encoders[col].classes_:
                        val = str(label_encoders[col].classes_[0])
                    row[enc_col] = label_encoders[col].transform([val])[0]
                else:
                    row[enc_col] = 0

        # ë‹¨ì¼ í–‰ ì˜ˆì¸¡
        row_df = pd.DataFrame([row])
        for c in feature_cols:
            if c not in row_df.columns:
                row_df[c] = 0
        pred = float(model.predict(row_df[feature_cols])[0])
        # ë°”ë‹¥ê°’ ì ìš©
        pred = max(pred, floor_val)
        pred = int(max(0, round(pred)))

        # ê²°ê³¼ + ë²„í¼ ê°±ì‹ 
        result.append({'sku': sku_target, 'city': city_target, 'date': date, 'mean': pred})
        buf.append(pred)
        if len(buf) > 60:
            buf = buf[-60:]

    return pd.DataFrame(result)[['sku','city','date','mean']]


def generate_xgboost_forecast_subset():
    print_progress("=== XGBoost ì„œë¸Œì…‹(ë² ë¥¼ë¦°-SKU0001, 2023-01~06) ì˜ˆì¸¡ ìƒì„± ===")
    total_start_time = time.time()

    # 1) ë°ì´í„° ë¡œë“œ
    demand_data, events_df, label_encoders = load_enhanced_training_data()

    # 2) í”¼ì²˜ ì¤€ë¹„
    demand_data, feature_cols = prepare_lightgbm_features(demand_data)

    # 3) ë¶„í• 
    print_progress("ğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...", total_start_time)
    train_data = demand_data[demand_data['date'] < '2022-01-01'].copy()
    val_data = demand_data[(demand_data['date'] >= '2022-01-01') & (demand_data['date'] < '2023-01-01')].copy()

    # 4) ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„(ìƒ˜í”Œ)
    print_progress("ğŸ” ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì¤‘...", total_start_time)
    X_train_sample = train_data[feature_cols].sample(n=min(10000, len(train_data)), random_state=42)
    _ = analyze_multicollinearity(X_train_sample, feature_cols, total_start_time)

    # 5) í•™ìŠµ
    model = train_xgboost_model(train_data, val_data, feature_cols, total_start_time)

    # 6) ê²€ì¦
    print_progress("ğŸ“ˆ ê²€ì¦ ì„±ëŠ¥ í‰ê°€ ì¤‘...", total_start_time)
    X_val = val_data[feature_cols]
    y_val = val_data['demand']
    val_pred = model.predict(X_val)
    create_xgboost_validation_visualization(val_data, val_pred, total_start_time)

    # 7) ì„œë¸Œì…‹ ë¯¸ë˜ ì˜ˆì¸¡
    print_progress("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡(ì„œë¸Œì…‹) ì¤‘...", total_start_time)
    result_df = predict_future_xgboost_subset(model, demand_data, feature_cols, label_encoders, total_start_time)

    # 8) ì €ì¥
    output_path = DATA_DIR / "xgboost_forecast_submission_subset_berlin_sku0001.csv"
    result_df.to_csv(output_path, index=False)
    print_progress(f"âœ… ì„œë¸Œì…‹ ì˜ˆì¸¡ ì™„ë£Œ. ì €ì¥: {output_path}", total_start_time)

    # 9) Berlin-SKU0001 ì „ìš© ì‹œê°í™”
    print_progress("ğŸ“Š Berlin-SKU0001 ì „ìš© ì‹œê°í™” ìƒì„± ì¤‘...", total_start_time)
    create_subset_visualization(demand_data, result_df, total_start_time)

    return result_df


def main():
    print_progress("=== XGBoost ì„œë¸Œì…‹ ì‹¤í–‰ ì‹œì‘ ===")
    _ = generate_xgboost_forecast_subset()
    print_progress("âœ… XGBoost ì„œë¸Œì…‹ ì‹¤í–‰ ì™„ë£Œ")


if __name__ == "__main__":
    main()




