# src/models/xgboost_model.py
# XGBoost ê¸°ë°˜ ê³ ê¸‰ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ - final_forecast_modelê³¼ ë™ì¼ ë¡œì§, ëª¨ë¸ë§Œ XGBoostë¡œ êµì²´

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import time
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
import xgboost as xgb

# ê²½ë¡œ ì„¤ì • (final_forecast_modelì™€ ë™ì¼)
DATA_DIR = Path("C:/projects/smartphone-supplychain/data")

def print_progress(message, start_time=None):
    """ì§„í–‰ìƒí™©ê³¼ ì‹œê°„ì„ ì¶œë ¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    current_time = datetime.now().strftime("%H:%M:%S")
    if start_time:
        elapsed = time.time() - start_time
        print(f"[{current_time}] â±ï¸ {elapsed:.1f}ì´ˆ - {message}")
    else:
        print(f"[{current_time}] {message}")

def get_country_mapping():
    """ë„ì‹œ-êµ­ê°€ ë§¤í•‘"""
    return {
        'Washington_DC': 'USA', 'New_York': 'USA', 'Chicago': 'USA', 'Dallas': 'USA',
        'Berlin': 'DEU', 'Munich': 'DEU', 'Frankfurt': 'DEU', 'Hamburg': 'DEU',
        'Paris': 'FRA', 'Lyon': 'FRA', 'Marseille': 'FRA', 'Toulouse': 'FRA',
        'Seoul': 'KOR', 'Busan': 'KOR', 'Incheon': 'KOR', 'Gwangju': 'KOR',
        'Tokyo': 'JPN', 'Osaka': 'JPN', 'Nagoya': 'JPN', 'Fukuoka': 'JPN',
        'Manchester': 'GBR', 'London': 'GBR', 'Birmingham': 'GBR', 'Glasgow': 'GBR',
        'Ottawa': 'CAN', 'Toronto': 'CAN', 'Vancouver': 'CAN', 'Montreal': 'CAN',
        'Canberra': 'AUS', 'Sydney': 'AUS', 'Melbourne': 'AUS', 'Brisbane': 'AUS',
        'Brasilia': 'BRA', 'Sao_Paulo': 'BRA', 'Rio_de_Janeiro': 'BRA', 'Salvador': 'BRA',
        'Pretoria': 'ZAF', 'Johannesburg': 'ZAF', 'Cape_Town': 'ZAF', 'Durban': 'ZAF'
    }

def get_hardcoded_event_periods():
    """
    Define hardcoded event periods based on analysis
    Returns: Dictionary with country-year as key and (start_date, end_date) as value
    """
    event_periods = {
        ('KOR', 2018): ('2018-02-15', '2018-03-19'),
        ('JPN', 2019): ('2019-01-19', '2019-03-03'),
        ('USA', 2020): ('2020-01-26', '2020-04-04'),
        ('USA', 2021): ('2021-03-13', '2021-06-06'),
        ('KOR', 2022): ('2022-02-05', '2022-04-12'),
        ('KOR', 2023): ('2023-06-01', '2023-08-31'),
        ('JPN', 2024): ('2024-02-01', '2024-04-30')
    }
    return event_periods

def load_enhanced_training_data():
    """LightGBM ëª¨ë¸ìš© ê³ ê¸‰ í•™ìŠµ ë°ì´í„° ë¡œë“œ (ë™ì¼ ë¡œì§ ì¬í˜„)"""
    print_progress("=== LightGBM ëª¨ë¸ìš© ê³ ê¸‰ ë°ì´í„° ë¡œë“œ ===")
    start_time = time.time()
    # ë°ì´í„° ë¡œë“œ
    print_progress("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...", start_time)
    conn = sqlite3.connect(DATA_DIR / "demand_train.db")
    demand = pd.read_sql_query("SELECT * FROM demand_train", conn)
    conn.close()
    demand['date'] = pd.to_datetime(demand['date'])

    oil = pd.read_csv(DATA_DIR / "oil_price_processed.csv", parse_dates=["date"])
    currency = pd.read_csv(DATA_DIR / "currency_processed.csv", parse_dates=["date"])
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence_processed.csv", parse_dates=["date"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])
    ppt = pd.read_csv(DATA_DIR / "price_promo_train.csv", parse_dates=["date"])

    print_progress("ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...", start_time)
    # ê¸°ë³¸ í”¼ì²˜
    country_map = get_country_mapping()
    demand["country"] = demand["city"].map(country_map)
    demand["month"] = demand["date"].dt.month
    demand["dayofyear"] = demand["date"].dt.dayofyear
    demand["weekday"] = demand["date"].dt.weekday

    # ì™¸ë¶€ ë°ì´í„° ë³‘í•©
    demand = demand.merge(calendar[["date", "country", "season"]], on=["date", "country"], how="left")
    demand = demand.merge(sku_meta[["sku", "family", "storage_gb", "launch_date"]], on="sku", how="left")
    demand["days_since_launch"] = (demand["date"] - demand["launch_date"]).dt.days.clip(lower=0)

    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    demand = demand.merge(oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']], on='date', how='left')

    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']
    demand = demand.merge(currency[['date'] + fx_cols], on='date', how='left')

    demand = demand.merge(consumer_conf[['date', 'country', 'confidence_index']], on=['date', 'country'], how='left')

    marketing_agg = marketing.groupby(['date', 'country'])['spend_usd'].sum().reset_index()
    demand = demand.merge(marketing_agg, on=['date', 'country'], how='left')

    demand = demand.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how="left")
    demand = demand.merge(ppt[['date', 'sku', 'city', 'unit_price', 'discount_pct']], on=['date', 'sku', 'city'], how="left")

    # ì´ë²¤íŠ¸ ê¸°ê°„ ì„¤ì • (í•˜ë“œì½”ë”©)
    print_progress("ğŸ“… í•˜ë“œì½”ë”©ëœ ì´ë²¤íŠ¸ ê¸°ê°„ ì„¤ì • ì¤‘...")
    event_periods = get_hardcoded_event_periods()
    events_list = []
    for (country, year), (start_date, end_date) in event_periods.items():
        events_list.append({
            'country': country,
            'year': year,
            'start_date': pd.to_datetime(start_date),
            'end_date': pd.to_datetime(end_date)
        })
    events_df = pd.DataFrame(events_list)

    print_progress(f"âœ… ì„¤ì •ëœ ì´ë²¤íŠ¸: {len(events_df)}ê°œ")
    for _, event in events_df.iterrows():
        print(f"  - {event['country']} ({event['year']}): {event['start_date'].strftime('%Y-%m-%d')} ~ {event['end_date'].strftime('%Y-%m-%d')}")

    # is_event ì»¬ëŸ¼ ìƒì„±
    demand['is_event'] = 0
    for _, event in events_df.iterrows():
        mask = (
            (demand['country'] == event['country']) &
            (demand['date'] >= event['start_date']) &
            (demand['date'] <= event['end_date'])
        )
        demand.loc[mask, 'is_event'] = 1

    event_count = demand['is_event'].sum()
    print_progress(f"âœ… ì´ë²¤íŠ¸ ê¸°ê°„ ë°ì´í„° í¬ì¸íŠ¸: {event_count:,}ê°œ")

    # ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±
    print_progress("ğŸ“ˆ ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì¤‘...", start_time)
    for col in ['demand']:
        if col in demand.columns:
            for lag in [1, 3, 7, 14]:
                demand[f'{col}_lag_{lag}'] = demand.groupby(['city', 'sku'])[col].shift(lag)
            for window in [7, 14]:
                demand[f'{col}_rolling_mean_{window}'] = demand.groupby(['city', 'sku'])[col].transform(lambda x: x.rolling(window, min_periods=1).mean())
            for window in [7, 14]:
                demand[f'{col}_rolling_std_{window}'] = demand.groupby(['city', 'sku'])[col].transform(lambda x: x.rolling(window, min_periods=1).std())

    # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
    print_progress("ğŸ”¤ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘...", start_time)
    categorical_cols = ['city', 'sku', 'country', 'family', 'season']
    label_encoders = {}
    for col in categorical_cols:
        if col in demand.columns:
            le = LabelEncoder()
            demand[f'{col}_encoded'] = le.fit_transform(demand[col].astype(str))
            label_encoders[col] = le

    # í• ì¸ìœ¨ ì •ê·œí™” ë° NaN ë””ë²„ê¹…
    demand['discount_pct'] = demand['discount_pct'] / 100
    print("=== Before fillna ===")
    for col in ["demand","unit_price","discount_pct","spend_usd","brent_usd","confidence_index"]:
        if col in demand.columns:
            print(col, "nan:", demand[col].isna().sum(), "min:", demand[col].min() if demand[col].notna().any() else None)
    print(demand[demand["unit_price"].isna()].head(10)[["date","city","sku","unit_price"]])

    demand = demand.fillna(0)

    # ë””ë²„ê¹…: ì£¼ìš” í”¼ì²˜ í†µê³„ëŸ‰ ì¶œë ¥
    print_progress("ğŸ” ë””ë²„ê¹…: ë°ì´í„° ë¡œë“œ í›„ ì£¼ìš” í”¼ì²˜ í†µê³„ëŸ‰ í™•ì¸ ì¤‘...", start_time)
    debug_cols = ['demand', 'demand_ratio', 'unit_price', 'discount_pct', 'spend_usd', 'brent_usd', 'confidence_index']
    for col in debug_cols:
        if col in demand.columns:
            print(f"  - {col}: Mean={demand[col].mean():.2f}, Std={demand[col].std():.2f}, Min={demand[col].min():.2f}, Max={demand[col].max():.2f}, NaN={demand[col].isnull().sum()}")
    print("--------------------------------------------------")
    print(demand.head(20))
    print_progress(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {demand.shape}", start_time)
    print_progress(f"ğŸ“ˆ ì´ë²¤íŠ¸ ê°ì§€: {len(events_df)}ê°œ", start_time)

    return demand, events_df, label_encoders

def prepare_lightgbm_features(demand_data):
    """LightGBM ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„ (ë™ì¼ ë¡œì§ ì¬í˜„)"""
    print_progress("ğŸ”§ LightGBM ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
    start_time = time.time()
    feature_cols = [
        'month', 'dayofyear', 'weekday',
        'storage_gb', 'days_since_launch',
        'city_encoded', 'sku_encoded', 'country_encoded', 'family_encoded', 'season_encoded',
        'unit_price', 'discount_pct',
        'avg_temp', 'humidity',
        'brent_usd', 'pct_change', 'volatility_7d',
        'confidence_index', 'spend_usd',
        'EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X',
        'is_event'
    ]
    ts_features = [col for col in demand_data.columns if any(x in col for x in ['lag_', 'rolling_mean_', 'rolling_std_'])]
    feature_cols.extend(ts_features)
    feature_cols = [col for col in feature_cols if col in demand_data.columns]
    remove_features = ['dayofyear']
    before_cnt = len(feature_cols)
    feature_cols = [col for col in feature_cols if col not in remove_features]
    removed_cnt = before_cnt - len(feature_cols)
    if removed_cnt > 0:
        print_progress(f"ğŸ§¹ ì œì™¸í•œ í”¼ì²˜ ìˆ˜: {removed_cnt}ê°œ ({[f for f in remove_features if f in demand_data.columns]})", start_time)
    print_progress(f"âœ… í”¼ì²˜ ì¤€ë¹„ ì™„ë£Œ:", start_time)
    print_progress(f"  - ì´ í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ", start_time)
    print_progress(f"  - ì‹œê³„ì—´ í”¼ì²˜: {len(ts_features)}ê°œ", start_time)
    return demand_data, feature_cols

def calculate_vif(X, feature_names):
    """VIF(Variance Inflation Factor) ê³„ì‚°"""
    print_progress("ğŸ” VIF ë¶„ì„ ì¤‘...")
    vif_data = []
    for i, feature in enumerate(feature_names):
        X_temp = X.drop(columns=[feature])
        y_temp = X[feature]
        model = LinearRegression()
        model.fit(X_temp, y_temp)
        r_squared = model.score(X_temp, y_temp)
        if r_squared < 0.999:
            vif = 1 / (1 - r_squared)
        else:
            vif = float('inf')
        vif_data.append({'feature': feature, 'vif': vif, 'r_squared': r_squared})
    vif_df = pd.DataFrame(vif_data)
    vif_df = vif_df.sort_values('vif', ascending=False)
    return vif_df

def analyze_multicollinearity(X, feature_names, start_time):
    """ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ (VIF + ìƒê´€ê´€ê³„)"""
    print_progress("ğŸ” ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì‹œì‘...", start_time)
    vif_df = calculate_vif(X, feature_names)
    print_progress("ğŸ“Š VIF ë¶„ì„ ê²°ê³¼:", start_time)
    print("  - VIF > 10: ì‹¬ê°í•œ ë‹¤ì¤‘ê³µì„ ì„±")
    print("  - VIF > 5: ì£¼ì˜ê°€ í•„ìš”í•œ ë‹¤ì¤‘ê³µì„ ì„±")
    print("  - VIF > 2: ì•½ê°„ì˜ ë‹¤ì¤‘ê³µì„ ì„±")
    print()
    high_vif_features = vif_df[vif_df['vif'] > 10]
    moderate_vif_features = vif_df[(vif_df['vif'] > 5) & (vif_df['vif'] <= 10)]
    if not high_vif_features.empty:
        print("ğŸš¨ ì‹¬ê°í•œ ë‹¤ì¤‘ê³µì„ ì„± (VIF > 10):")
        for _, row in high_vif_features.iterrows():
            print(f"  - {row['feature']}: VIF={row['vif']:.2f}, RÂ²={row['r_squared']:.4f}")
        print()
    if not moderate_vif_features.empty:
        print("âš ï¸ ì£¼ì˜ê°€ í•„ìš”í•œ ë‹¤ì¤‘ê³µì„ ì„± (VIF > 5):")
        for _, row in moderate_vif_features.iterrows():
            print(f"  - {row['feature']}: VIF={row['vif']:.2f}, RÂ²={row['r_squared']:.4f}")
        print()
    vif_csv_path = DATA_DIR / 'vif_analysis.csv'
    vif_df.to_csv(vif_csv_path, index=False)
    print_progress(f"ğŸ“ VIF ë¶„ì„ ê²°ê³¼ ì €ì¥: {vif_csv_path}", start_time)
    print_progress("ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...", start_time)
    corr_matrix = X.corr()
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) > 0.8:
                high_corr_pairs.append({'feature1': corr_matrix.columns[i], 'feature2': corr_matrix.columns[j], 'correlation': corr_val})
    if high_corr_pairs:
        print("ğŸ”— ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.8):")
        high_corr_pairs.sort(key=lambda x: abs(x['correlation']), reverse=True)
        for pair in high_corr_pairs[:20]:
            print(f"  - {pair['feature1']} â†” {pair['feature2']}: r={pair['correlation']:.4f}")
        print()
    plt.figure(figsize=(20, 16))
    mask = np.abs(corr_matrix) < 0.5
    sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r', center=0, square=True, linewidths=0.5, cbar_kws={"shrink": .8})
    plt.title('Feature Correlation Matrix (|r| >= 0.5)', fontsize=16)
    plt.tight_layout()
    corr_png_path = DATA_DIR / 'feature_correlation_heatmap.png'
    plt.savefig(corr_png_path, dpi=300, bbox_inches='tight')
    plt.close()
    print_progress(f"ğŸ“ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì €ì¥: {corr_png_path}", start_time)
    print_progress("ğŸ“Š í”¼ì²˜ ê·¸ë£¹ë³„ ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„...", start_time)
    lag_features = [f for f in feature_names if 'lag_' in f]
    rolling_mean_features = [f for f in feature_names if 'rolling_mean_' in f]
    rolling_std_features = [f for f in feature_names if 'rolling_std_' in f]
    print(f"  - Lag í”¼ì²˜ ìˆ˜: {len(lag_features)}")
    print(f"  - Rolling Mean í”¼ì²˜ ìˆ˜: {len(rolling_mean_features)}")
    print(f"  - Rolling Std í”¼ì²˜ ìˆ˜: {len(rolling_std_features)}")
    for group_name, group_features in [('Lag', lag_features), ('Rolling Mean', rolling_mean_features), ('Rolling Std', rolling_std_features)]:
        if group_features:
            group_vif = vif_df[vif_df['feature'].isin(group_features)]
            high_vif_in_group = group_vif[group_vif['vif'] > 5]
            if not high_vif_in_group.empty:
                print(f"  - {group_name} ê·¸ë£¹ ë‚´ ë†’ì€ VIF í”¼ì²˜:")
                for _, row in high_vif_in_group.iterrows():
                    print(f"    * {row['feature']}: VIF={row['vif']:.2f}")
    return vif_df, high_corr_pairs

def _compute_event_weights(train_data: pd.DataFrame, event_weight: float = 100) -> np.ndarray:
    """ì´ë²¤íŠ¸ êµ¬ê°„ ê°€ì¤‘ì¹˜ ë²¡í„° ìƒì„±"""
    return np.where(train_data['is_event'] == 1, event_weight, 1.0)


def _plot_and_save_feature_importance(model: xgb.XGBRegressor, feature_cols, start_time):
    """XGBoost í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° ë° ì €ì¥/ì‹œê°í™”"""
    try:
        print_progress("ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° ì¤‘...", start_time)

        # 1) gain ê¸°ë°˜ ì¤‘ìš”ë„ ì‚°ì¶œ
        booster = model.get_booster()
        # ìµœì‹  ë²„ì „ì€ booster.feature_namesê°€ pandas ì»¬ëŸ¼ëª…ì„ ë³´ì¡´í•¨
        feature_names = booster.feature_names if getattr(booster, 'feature_names', None) else list(feature_cols)
        gain_map = booster.get_score(importance_type='gain')  # dict: {feature_name: gain}

        # dictë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬ (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ëŠ” 0 ì²˜ë¦¬)
        gain_values = []
        for f in feature_names:
            gain_values.append(gain_map.get(f, 0.0))

        fi_df = pd.DataFrame({
            'feature': feature_names,
            'gain': gain_values,
        })
        fi_df['gain_pct'] = fi_df['gain'] / (fi_df['gain'].sum() + 1e-9)
        fi_df = fi_df.sort_values('gain', ascending=False).reset_index(drop=True)

        # ì €ì¥
        fi_csv_path = DATA_DIR / 'xgboost_feature_importance.csv'
        fi_df.to_csv(fi_csv_path, index=False)
        print_progress(f"ğŸ“ í”¼ì²˜ ì¤‘ìš”ë„ CSV ì €ì¥: {fi_csv_path}", start_time)

        # ìƒìœ„ 30ê°œ ì‹œê°í™”
        top_n = min(30, len(fi_df))
        top_df = fi_df.head(top_n).iloc[::-1]
        plt.figure(figsize=(10, max(6, top_n * 0.3)))
        plt.barh(top_df['feature'], top_df['gain'])
        plt.title('XGBoost Feature Importance (gain ê¸°ì¤€)')
        plt.xlabel('Gain')
        plt.tight_layout()
        fi_png_path = DATA_DIR / 'xgboost_feature_importance_top30.png'
        plt.savefig(fi_png_path, dpi=300, bbox_inches='tight')
        plt.close()
        print_progress(f"ğŸ“ í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„ ì €ì¥: {fi_png_path}", start_time)

        # ì½˜ì†” ìƒìœ„ 20ê°œ ì¶œë ¥
        print("\nìƒìœ„ ì¤‘ìš” í”¼ì²˜ (gain ê¸°ì¤€) Top 20:")
        for i, row in fi_df.head(20).iterrows():
            print(f"  {i+1:2d}. {row['feature']}: gain={row['gain']:.1f}, gain_pct={row['gain_pct']*100:.2f}%")
        print()
    except Exception as e:
        print(f"âš ï¸ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°/ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def train_xgboost_model(train_data: pd.DataFrame, val_data: pd.DataFrame, feature_cols, start_time):
    """XGBoost ëª¨ë¸ í•™ìŠµ (GPU ìš°ì„ , ì‹¤íŒ¨ ì‹œ CPU í´ë°±)"""
    print_progress("ğŸš€ XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘...", start_time)

    # ë°ì´í„° ì¤€ë¹„
    X_train = train_data[feature_cols]
    y_train = train_data['demand']
    X_val = val_data[feature_cols]
    y_val = val_data['demand']

    print_progress(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {X_train.shape}, ê²€ì¦ ë°ì´í„°: {X_val.shape}", start_time)

    # ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜
    print_progress("ğŸ” ë””ë²„ê¹…: ì´ë²¤íŠ¸ ë°ì´í„° ë¶„ì„ ì¤‘...", start_time)
    train_event_count = train_data['is_event'].sum()
    train_total_count = len(train_data)
    val_event_count = val_data['is_event'].sum()
    val_total_count = len(val_data)
    print(f"  - í›ˆë ¨ ë°ì´í„°: ì´ {train_total_count:,}ê°œ ì¤‘ ì´ë²¤íŠ¸ {train_event_count:,}ê°œ ({train_event_count/train_total_count*100:.2f}%)")
    print(f"  - ê²€ì¦ ë°ì´í„°: ì´ {val_total_count:,}ê°œ ì¤‘ ì´ë²¤íŠ¸ {val_event_count:,}ê°œ ({val_event_count/val_total_count*100:.2f}%)")

    event_weight = 100
    final_weights = _compute_event_weights(train_data, event_weight)
    print(f"  - ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜: {event_weight}")
    print(f"  - ì´ë²¤íŠ¸ ê¸°ê°„ í‰ê·  ê°€ì¤‘ì¹˜: {final_weights[train_data['is_event'] == 1].mean():.1f}")
    print(f"  - ë¹„ì´ë²¤íŠ¸ ê¸°ê°„ í‰ê·  ê°€ì¤‘ì¹˜: {final_weights[train_data['is_event'] == 0].mean():.1f}")
    print(f"  - ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {final_weights[train_data['is_event'] == 1].mean() / final_weights[train_data['is_event'] == 0].mean():.1f}ë°°")

    # ê³µí†µ í•˜ì´í¼íŒŒë¼ë¯¸í„° (LightGBM ì„¤ì •ì„ XGBoostì— ìœ ì‚¬ ë§¤í•‘)
    # ë²„ì „ì— ë”°ë¼ GPU íŒŒë¼ë¯¸í„°ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì„¤ì •
    _ver = getattr(xgb, '__version__', '1.6.0')
    try:
        _ver_parts = [int(p) for p in _ver.split('.')[:2]]
    except Exception:
        _ver_parts = [1, 6]
    _major, _minor = (_ver_parts + [0, 0])[:2]

    gpu_params = {}
    if _major >= 2:
        # XGBoost 2.x: device íŒŒë¼ë¯¸í„° ì‚¬ìš© + GPU predictor ê°•ì œ
        gpu_params = dict(device='cuda', tree_method='hist', predictor='gpu_predictor')
    else:
        # XGBoost 1.x: gpu_hist / gpu_predictor ì‚¬ìš©
        gpu_params = dict(tree_method='gpu_hist', predictor='gpu_predictor')

    common_params = dict(
        n_estimators=1200,
        learning_rate=0.005,
        max_depth=7,
        subsample=0.7,
        colsample_bytree=0.8,
        min_child_weight=1.0,
        gamma=0.0,
        reg_alpha=0.0,
        reg_lambda=1.0,
        random_state=42,
        objective='reg:squarederror',
        eval_metric='rmse',
        n_jobs=-1,
        verbosity=1,
        **gpu_params,
    )

    # í•™ìŠµ (GPU â†’ ì‹¤íŒ¨ ì‹œ CPUë¡œ ì¬ì‹œë„)
    def _fit_with_params(params):
        model = xgb.XGBRegressor(**params)
        # í˜¸í™˜ì„±: ì¼ë¶€ ë²„ì „ì€ early_stopping_rounds/verbose/callbacks ë¯¸ì§€ì›
        try:
            try:
                # ìš°ì„  callbacks ë°©ì‹ ì‹œë„
                es_cb = getattr(xgb.callback, 'EarlyStopping', None)
                callbacks = [es_cb(rounds=50)] if es_cb is not None else None
                model.fit(
                    X_train,
                    y_train,
                    sample_weight=final_weights,
                    eval_set=[(X_train, y_train), (X_val, y_val)],
                    callbacks=callbacks,
                )
            except TypeError:
                # callbacks ì¸ì ë¯¸ì§€ì› â†’ ë‹¨ìˆœ í•™ìŠµ (ES ë¹„í™œì„±í™”)
                print_progress("âš ï¸ í˜„ì¬ XGBoost ë²„ì „ì—ì„œ Early Stopping ë¯¸ì§€ì› â†’ ES ë¹„í™œì„±í™”í•˜ê³  í•™ìŠµ ì§„í–‰", start_time)
                model.fit(
                    X_train,
                    y_train,
                    sample_weight=final_weights,
                    eval_set=[(X_train, y_train), (X_val, y_val)],
                )
        except TypeError as e:
            # ìµœí›„ ìˆ˜ë‹¨: eval_set ë„ ì œê±°
            print_progress(f"âš ï¸ fit ì¸ì í˜¸í™˜ì„± ë¬¸ì œë¡œ eval_set ì œê±° í›„ ì¬ì‹œë„: {e}", start_time)
            model.fit(
                X_train,
                y_train,
                sample_weight=final_weights,
            )
        return model

    print_progress("ğŸ“š ëª¨ë¸ í•™ìŠµ ì¤‘... (GPU ì‹œë„)", start_time)
    # ë¹Œë“œ ì„¤ì •(ê°€ëŠ¥ ì‹œ) ì¶œë ¥: CUDA ì§€ì› ì—¬ë¶€ í™•ì¸ì— ë„ì›€
    try:
        if hasattr(xgb, 'print_config'):
            print_progress("ğŸ”§ XGBoost Build Config:", start_time)
            xgb.print_config()
    except Exception as e:
        print(f"[WARN] print_config ì‹¤íŒ¨: {e}")
    try:
        model = _fit_with_params(common_params)
    except Exception as gpu_err:
        print(f"âš ï¸ GPU í•™ìŠµ ì‹¤íŒ¨, CPUë¡œ í´ë°±í•©ë‹ˆë‹¤: {gpu_err}")
        cpu_params = {**common_params, 'tree_method': 'hist', 'predictor': 'auto'}
        model = _fit_with_params(cpu_params)

    print_progress(f"âœ… XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {time.time() - start_time:.1f}ì´ˆ", start_time)

    # í”¼ì²˜ ì¤‘ìš”ë„ ì‚°ì¶œ ë° ì €ì¥
    _plot_and_save_feature_importance(model, feature_cols, start_time)

    # ì‹¤ì œ ì‚¬ìš©ëœ ì¥ì¹˜/ì•Œê³ ë¦¬ì¦˜ í™•ì¸ (Booster ì„¤ì • + ëª¨ë¸ íŒŒë¼ë¯¸í„°)
    try:
        import json as _json
        cfg_json = model.get_booster().save_config()
        cfg = _json.loads(cfg_json)
        generic = cfg.get('learner', {}).get('generic_param', {})
        used_tree_method = str(generic.get('tree_method', 'unknown'))
        used_predictor = str(generic.get('predictor', 'unknown'))
        device = str(generic.get('device', 'n/a'))

        # ëª¨ë¸ íŒŒë¼ë¯¸í„°ë„ í•¨ê»˜ í™•ì¸
        params_used = model.get_xgb_params()
        p_tree_method = str(params_used.get('tree_method', ''))
        p_predictor = str(params_used.get('predictor', ''))
        p_device = str(params_used.get('device', ''))

        print_progress(
            f"ğŸ§­ Booster Config â†’ tree_method={used_tree_method}, predictor={used_predictor}, device={device}",
            start_time
        )
        print_progress(
            f"ğŸ§­ Model Params  â†’ tree_method={p_tree_method}, predictor={p_predictor}, device={p_device}",
            start_time
        )

        used_gpu = (
            'cuda' in device.lower() or 'cuda' in p_device.lower() or
            'gpu' in used_predictor.lower() or 'gpu' in p_predictor.lower() or
            used_tree_method == 'gpu_hist' or p_tree_method == 'gpu_hist'
        )
        if used_gpu:
            print_progress("âœ… GPU ì‚¬ìš©ìœ¼ë¡œ íŒë‹¨ë¨", start_time)
        else:
            print_progress("âš ï¸ GPU ì‚¬ìš© ì§•í›„ê°€ ì—†ì–´ CPUë¡œ íŒë‹¨ë¨", start_time)
    except Exception as e:
        print(f"[WARN] Booster ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}")

    return model


def create_xgboost_validation_visualization(val_data: pd.DataFrame, val_pred: np.ndarray, start_time):
    """XGBoost ëª¨ë¸ 2022ë…„ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”"""
    print_progress("ğŸ“Š XGBoost ëª¨ë¸ 2022ë…„ ê²€ì¦ ì‹œê°í™” ìƒì„± ì¤‘...", start_time)

    val_data_with_pred = val_data.copy()
    val_data_with_pred['predicted_demand'] = val_pred

    # ìƒ˜í”Œë§ (í•œêµ­ 4ê°œ ë„ì‹œ ê³ ì •, ì—†ìœ¼ë©´ ì„ì˜ ìƒìœ„ 5 ì¡°í•©)
    korean_cities = ['Seoul', 'Busan', 'Incheon', 'Gwangju']
    sample_combinations = []
    for city in korean_cities:
        city_data = val_data_with_pred[val_data_with_pred['city'] == city]
        if not city_data.empty:
            first_sku = city_data['sku'].iloc[0]
            sample_combinations.append({'city': city, 'sku': first_sku})
    if len(sample_combinations) == 0:
        sample_combinations = val_data_with_pred[['city', 'sku']].drop_duplicates().head(5).to_dict('records')

    fig, axes = plt.subplots(len(sample_combinations), 1, figsize=(15, 4 * len(sample_combinations)))
    if len(sample_combinations) == 1:
        axes = [axes]

    for i, combo in enumerate(sample_combinations):
        city, sku = combo['city'], combo['sku']
        mask = (val_data_with_pred['city'] == city) & (val_data_with_pred['sku'] == sku)
        combo_data = val_data_with_pred[mask].sort_values('date')
        if len(combo_data) > 0:
            actual_vals = combo_data['demand'].values
            pred_vals = combo_data['predicted_demand'].values
            axes[i].plot(actual_vals, label='Actual', linewidth=2)
            axes[i].plot(pred_vals, label='Predicted', linewidth=2, linestyle='--')
            axes[i].set_title(f'2022 Validation - {city} - {sku}')
            axes[i].set_xlabel('Index')
            axes[i].set_ylabel('Demand')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        else:
            axes[i].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[i].transAxes)
            axes[i].set_title(f'2022 Validation - {city} - {sku}')

    plt.tight_layout()
    output_path = DATA_DIR / "xgboost_validation_2022.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print_progress(f"ğŸ“ Saved XGBoost 2022 validation plot: {output_path}", start_time)


def create_xgboost_full_timeline_visualization(demand_data: pd.DataFrame, result_df: pd.DataFrame, start_time):
    """XGBoost ëª¨ë¸ 2018-2024 ì „ì²´ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œê°í™”"""
    print_progress("ğŸ“Š Creating XGBoost 2018-2024 full timeline plot...", start_time)

    print_progress("ğŸ” Debug: Checking stats for full timeline visualization...", start_time)
    print(f"  - 2023-2024 ì˜ˆì¸¡ê°’ (result_df): Mean={result_df['mean'].mean():.2f}, Max={result_df['mean'].max():.2f}, Non-zero ratio={(result_df['mean'] > 0).mean()*100:.2f}%")

    actual_data = demand_data[['date', 'city', 'sku', 'demand']].copy()
    actual_data['type'] = 'actual'
    pred_vis_data = result_df[['date', 'city', 'sku', 'mean']].copy().rename(columns={'mean': 'demand'})
    pred_vis_data['type'] = 'predicted'
    combined_data = pd.concat([
        actual_data[['date', 'city', 'sku', 'demand', 'type']],
        pred_vis_data
    ], ignore_index=True)
    combined_data['date'] = pd.to_datetime(combined_data['date'])

    available_cities = combined_data['city'].unique()
    available_skus = combined_data['sku'].unique()
    sample_cities = available_cities[:5] if len(available_cities) >= 5 else available_cities
    sample_skus = available_skus[:3] if len(available_skus) >= 3 else available_skus

    print(f"ğŸ“Š Sample cities for plotting: {sample_cities}")
    print(f"ğŸ“Š Sample SKUs for plotting: {sample_skus}")

    fig, axes = plt.subplots(len(sample_cities), len(sample_skus), figsize=(24, 18))
    fig.suptitle('XGBoost: 2018-2024 Full Timeline - Actual vs Predicted', fontsize=16, fontweight='bold')

    for i, city in enumerate(sample_cities):
        for j, sku in enumerate(sample_skus):
            city_sku_data = combined_data[(combined_data['city'] == city) & (combined_data['sku'] == sku)]
            if len(city_sku_data) > 0:
                city_sku_data = city_sku_data.sort_values('date')
                actual_mask = city_sku_data['type'] == 'actual'
                pred_mask = city_sku_data['type'] == 'predicted'
                actual_plot = city_sku_data[actual_mask]
                pred_plot = city_sku_data[pred_mask]

                if len(actual_plot) > 0:
                    axes[i, j].plot(actual_plot['date'], actual_plot['demand'], label='Actual (2018-2022)', color='blue', linewidth=2)
                if len(pred_plot) > 0:
                    axes[i, j].plot(pred_plot['date'], pred_plot['demand'], label='Predicted (2023-2024)', color='red', linewidth=2, linestyle='--')

                axes[i, j].axvline(x=pd.Timestamp('2023-01-01'), color='green', linestyle=':', alpha=0.7, label='Forecast start')
                axes[i, j].set_title(f'{city} - {sku}', fontsize=12, fontweight='bold')
                axes[i, j].set_xlabel('Date')
                axes[i, j].set_ylabel('Demand')
                axes[i, j].legend()
                axes[i, j].grid(True, alpha=0.3)
                axes[i, j].tick_params(axis='x', rotation=45)
                if len(actual_plot) > 0 and len(pred_plot) > 0:
                    y_min = min(actual_plot['demand'].min(), pred_plot['demand'].min())
                    y_max = max(actual_plot['demand'].max(), pred_plot['demand'].max())
                    axes[i, j].set_ylim([y_min * 0.8, y_max * 1.2])
            else:
                axes[i, j].text(0.5, 0.5, 'ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=axes[i, j].transAxes)
                axes[i, j].set_title(f'{city} - {sku}', fontsize=12, fontweight='bold')

    plt.tight_layout()
    output_path = DATA_DIR / "xgboost_full_timeline_2018_2024.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print_progress(f"ğŸ“ Saved XGBoost full timeline plot: {output_path}", start_time)


## ì˜ˆì¸¡ í—¬í¼ëŠ” ì›ë˜ ë°©ì‹ìœ¼ë¡œ ë³µêµ¬í–ˆìœ¼ë¯€ë¡œ ì œê±°

def predict_future_xgboost(model, demand_data, feature_cols, label_encoders, start_time, events_df=None):
    """XGBoost ëª¨ë¸ë¡œ ë¯¸ë˜ ì˜ˆì¸¡ (final_forecast_modelì˜ ë¡œì§ì„ ë™ì¼í•˜ê²Œ ìœ ì§€)"""
    print_progress("ğŸ”® XGBoost ëª¨ë¸ë¡œ ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘...", start_time)

    # 2023-2024ë…„ ë‚ ì§œ ìƒì„± ë° ê³µë³€ëŸ‰/ì´ë²¤íŠ¸/ì˜¤í† ë ˆê·¸ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´
    # final_forecast_modelì˜ êµ¬í˜„ì„ ê°€ì ¸ì˜¤ì§€ ì•Šê³  ë™ì¼ êµ¬í˜„ì„ ì´ê³³ì— ë³µì œ
    future_dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')

    oil = pd.read_csv(DATA_DIR / "oil_price_processed.csv", parse_dates=["date"])
    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    currency = pd.read_csv(DATA_DIR / "currency_processed.csv", parse_dates=["date"])
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence_processed.csv", parse_dates=["date"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])

    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']

    marketing_agg = marketing.groupby(['date', 'country'])['spend_usd'].sum().reset_index()
    country_cov = consumer_conf[['date', 'country', 'confidence_index']]
    country_cov = country_cov.merge(marketing_agg, on=['date', 'country'], how='left')
    country_cov = country_cov.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how='left')
    country_cov = country_cov.merge(calendar[['date', 'country', 'season']], on=['date', 'country'], how='left')

    cov_date = oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']]
    cov_date = cov_date.merge(currency[['date'] + fx_cols], on='date', how='left')

    cov_future = country_cov.merge(cov_date, on='date', how='left')

    # ê²°ì¸¡/ë¯¸ì¡´ì¬ ê°’ ë³´ì •: ë¯¸ë˜(2023-2024) ê³µë³€ëŸ‰ì´ ë¹„ë©´ 0ìœ¼ë¡œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ â†’ ê·¸ë£¹ë³„/ì‹œê°„ë³„ ë³´ê°„
    try:
        cov_future = cov_future.sort_values(['country', 'date'])
        cov_future[['confidence_index','spend_usd','avg_temp','humidity']] = (
            cov_future.groupby('country')[['confidence_index','spend_usd','avg_temp','humidity']].ffill()
        )
        # ë‚¨ì€ ê²°ì¸¡ì€ 0ì´ ì•„ë‹Œ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ì›€
        for c in ['confidence_index','spend_usd','avg_temp','humidity']:
            if c in cov_future.columns:
                med = cov_future[c].median() if cov_future[c].notna().any() else 0.0
                cov_future[c] = cov_future[c].fillna(med)

        # season ë¬¸ìì—´ ê²°ì¸¡ì€ ì§ì „ê°’ìœ¼ë¡œ ìœ ì§€
        if 'season' in cov_future.columns:
            cov_future['season'] = cov_future.groupby('country')['season'].ffill().fillna('')

        # ë‚ ì§œ ê³µë³€ëŸ‰ë„ ì‹œê°„ ì¶•ìœ¼ë¡œ ë³´ê°„
        cov_date = cov_date.sort_values('date')
        fill_cols = ['brent_usd','pct_change','volatility_7d'] + fx_cols
        for c in fill_cols:
            if c in cov_date.columns:
                cov_date[c] = cov_date[c].ffill()
    except Exception as e:
        print(f"[WARN] cov fill forward failed: {e}")

    # ë””ë²„ê¹…: cov í…Œì´ë¸” êµ¬ì¡° ë¯¸ë¦¬ë³´ê¸°
    try:
        print_progress("ğŸ” cov_future preview:", start_time)
        print(f"  - shape: {cov_future.shape}")
        print(f"  - columns: {list(cov_future.columns)}")
        # 2023~2024 ê²°ì¸¡ë¥  ìš”ì•½
        mask_2324 = (cov_future['date'] >= pd.Timestamp('2023-01-01')) & (cov_future['date'] <= pd.Timestamp('2024-12-31'))
        sub = cov_future.loc[mask_2324]
        if len(sub) > 0:
            check_cols = ['confidence_index','spend_usd','avg_temp','humidity','brent_usd','pct_change','volatility_7d']
            nz = {c: float((sub[c]==0).mean()) if c in sub.columns else None for c in check_cols}
            print(f"  - 2023-2024 zero ratio: {nz}")
        print(cov_future.head(5))
    except Exception as e:
        print(f"[WARN] cov_future preview failed: {e}")

    event_periods = get_hardcoded_event_periods()
    print_progress("ğŸ“¢ 2023-2024 í™•ì • ì´ë²¤íŠ¸ êµ¬ê°„:")
    for (country, year), (start_date, end_date) in event_periods.items():
        if year in [2023, 2024]:
            print(f"  - {country} | {start_date} ~ {end_date}")

    city_sku_combinations = demand_data[['city', 'sku']].drop_duplicates()
    result = []
    print_progress(f"ğŸ“Š ì˜ˆì¸¡ ëŒ€ìƒ: {len(city_sku_combinations)}ê°œ ì¡°í•© Ã— {len(future_dates)}ì¼ = {len(city_sku_combinations) * len(future_dates):,}ê°œ", start_time)
    # ì˜ˆì¸¡ ì „ GPU ê²½ë¡œ ì¬í™•ì¸ (predictor/device)
    try:
        import json as _json
        cfg_json = model.get_booster().save_config()
        cfg = _json.loads(cfg_json)
        generic = cfg.get('learner', {}).get('generic_param', {})
        used_predictor = str(generic.get('predictor', 'unknown'))
        device = str(generic.get('device', 'n/a'))
        print_progress(f"ğŸ” Predict Config â†’ predictor={used_predictor}, device={device}", start_time)
    except Exception as e:
        print(f"[WARN] ì˜ˆì¸¡ ì „ GPU ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}")

    for idx, (_, combo) in enumerate(tqdm(city_sku_combinations.iterrows(), total=len(city_sku_combinations), desc="ì˜ˆì¸¡ ì§„í–‰")):
        city, sku = combo['city'], combo['sku']
        recent_data = demand_data[(demand_data['city'] == city) & (demand_data['sku'] == sku)].tail(60)
        if len(recent_data) == 0:
            continue

        demand_buffer = recent_data['demand'].dropna().tolist()
        if len(demand_buffer) == 0:
            demand_buffer = [0.0]
        if idx < 3:
            print(f"[DBG] {city}-{sku} initial demand_buffer (last 5): {demand_buffer[-5:]}  size={len(demand_buffer)}")

        for date in future_dates:
            future_row = {
                'date': date,
                'city': city,
                'sku': sku,
                'month': date.month,
                'dayofyear': date.dayofyear,
                'weekday': date.weekday(),
            }

            if len(recent_data) > 0:
                future_row['country'] = recent_data['country'].iloc[0]
                future_row['family'] = recent_data['family'].iloc[0]
                future_row['season'] = recent_data['season'].iloc[0]
            else:
                first_combo = demand_data[(demand_data['city'] == city) & (demand_data['sku'] == sku)]
                if len(first_combo) > 0:
                    future_row['country'] = first_combo['country'].iloc[0]
                    future_row['family'] = first_combo['family'].iloc[0]
                    future_row['season'] = first_combo['season'].iloc[0]
                else:
                    future_row['country'] = demand_data['country'].iloc[0]
                    future_row['family'] = demand_data['family'].iloc[0]
                    future_row['season'] = demand_data['season'].iloc[0]

            lookup_country = future_row['country'] if 'country' in future_row else None
            cov_row = cov_future[(cov_future['date'] == date) & (cov_future['country'] == lookup_country)]
            if len(cov_row) > 0:
                cov_row = cov_row.iloc[0]
                future_row['confidence_index'] = cov_row.get('confidence_index', 0)
                future_row['spend_usd'] = cov_row.get('spend_usd', 0)
                future_row['avg_temp'] = cov_row.get('avg_temp', 0)
                future_row['humidity'] = cov_row.get('humidity', 0)
                future_row['season'] = cov_row.get('season', future_row.get('season', ''))
            else:
                for col in ['confidence_index', 'spend_usd', 'avg_temp', 'humidity']:
                    future_row[col] = 0

            cov_date_row = cov_date[cov_date['date'] == date]
            if len(cov_date_row) > 0:
                cov_date_row = cov_date_row.iloc[0]
                future_row['brent_usd'] = cov_date_row.get('brent_usd', 0)
                future_row['pct_change'] = cov_date_row.get('pct_change', 0)
                future_row['volatility_7d'] = cov_date_row.get('volatility_7d', 0)
                for fx in fx_cols:
                    future_row[fx] = cov_date_row.get(fx, 0)
            else:
                for col in ['brent_usd', 'pct_change', 'volatility_7d'] + fx_cols:
                    future_row[col] = 0

            if sku in sku_meta['sku'].values:
                sku_launch_date = sku_meta[sku_meta['sku'] == sku]['launch_date'].iloc[0]
                if pd.notna(sku_launch_date):
                    future_row['days_since_launch'] = max(0, (date - sku_launch_date).days)
                else:
                    future_row['days_since_launch'] = 0
            else:
                future_row['days_since_launch'] = 0

            for col in ['city', 'sku', 'country', 'family', 'season']:
                # ì¸ì½”ë”ëŠ” ìƒìœ„ ë ˆë²¨ì—ì„œ ì£¼ì…ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì—´ë§Œ ì¤€ë¹„
                pass

            # ì´ë²¤íŠ¸ í”Œë˜ê·¸
            is_event = 0
            for (country, year), (start_date, end_date) in event_periods.items():
                if (
                    country == future_row['country']
                    and future_row['date'] >= pd.to_datetime(start_date)
                    and future_row['date'] <= pd.to_datetime(end_date)
                ):
                    is_event = 1
                    break
            future_row['is_event'] = is_event

            for lag in [1, 3, 7, 14]:
                if len(demand_buffer) >= lag:
                    future_row[f'demand_lag_{lag}'] = demand_buffer[-lag]
                else:
                    future_row[f'demand_lag_{lag}'] = demand_buffer[0] if len(demand_buffer) > 0 else 0
            for window in [7, 14]:
                if len(demand_buffer) > 0:
                    series = demand_buffer[-window:] if len(demand_buffer) >= window else demand_buffer
                    future_row[f'demand_rolling_mean_{window}'] = float(np.mean(series))
                    future_row[f'demand_rolling_std_{window}'] = float(np.std(series, ddof=1)) if len(series) > 1 else 0.0
                else:
                    future_row[f'demand_rolling_mean_{window}'] = 0.0
                    future_row[f'demand_rolling_std_{window}'] = 0.0

            if 'unit_price' in recent_data.columns and len(recent_data) > 0:
                future_row['unit_price'] = float(recent_data['unit_price'].ffill().iloc[-1])
            if 'discount_pct' in recent_data.columns and len(recent_data) > 0:
                future_row['discount_pct'] = float(recent_data['discount_pct'].ffill().iloc[-1])
                # í• ì¸ìœ¨ ìŠ¤ì¼€ì¼ ë³´ì •(í•™ìŠµê³¼ ë™ì¼ 0~1)
                if future_row['discount_pct'] > 1.0:
                    future_row['discount_pct'] = future_row['discount_pct'] / 100.0

            for col in ['discount_pct', 'unit_price']:
                if col in recent_data.columns:
                    for lag in [1, 3, 7, 14]:
                        if len(recent_data) >= lag:
                            future_row[f'{col}_lag_{lag}'] = recent_data[col].iloc[-lag]
                        else:
                            future_row[f'{col}_lag_{lag}'] = recent_data[col].iloc[0] if len(recent_data) > 0 else 0
                    for window in [7, 14]:
                        vals = recent_data[col].tail(window)
                        future_row[f'{col}_rolling_mean_{window}'] = vals.mean() if len(vals) > 0 else (recent_data[col].mean() if len(recent_data) > 0 else 0)

            if lookup_country is not None:
                for lag in [1, 3, 7, 14]:
                    lag_date = date - pd.Timedelta(days=lag)
                    v = cov_future.loc[(cov_future['date'] == lag_date) & (cov_future['country'] == lookup_country), 'spend_usd']
                    future_row[f'spend_usd_lag_{lag}'] = float(v.iloc[0]) if len(v) > 0 else 0.0
                for window in [7, 14]:
                    start_d = date - pd.Timedelta(days=window-1)
                    mask = (cov_future['country'] == lookup_country) & (cov_future['date'] >= start_d) & (cov_future['date'] <= date)
                    vals = cov_future.loc[mask, 'spend_usd']
                    future_row[f'spend_usd_rolling_mean_{window}'] = float(vals.mean()) if len(vals) > 0 else 0.0

            row_df = pd.DataFrame([future_row])
            # ëˆ„ë½ëœ í”¼ì²˜ ë³´ì •
            for col in feature_cols:
                if col not in row_df.columns:
                    row_df[col] = 0

            # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”© (í•™ìŠµ ì‹œ LabelEncoder ì‚¬ìš©ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
            for col in ['city', 'sku', 'country', 'family', 'season']:
                encoded_col = f"{col}_encoded"
                if encoded_col in feature_cols:
                    value_to_encode = str(future_row.get(col, ''))
                    if value_to_encode is None:
                        value_to_encode = ''
                    if col in label_encoders:
                        le = label_encoders[col]
                        # ë¯¸ì§€ì˜ í´ë˜ìŠ¤ ì²˜ë¦¬: ì²« í´ë˜ìŠ¤ ëŒ€ì²´
                        if len(le.classes_) > 0 and value_to_encode not in le.classes_:
                            value_to_encode = str(le.classes_[0])
                        row_df[encoded_col] = le.transform([value_to_encode])[0]
                    else:
                        row_df[encoded_col] = 0

            X_row = row_df[feature_cols]
            pred_demand = float(model.predict(X_row)[0])
            pred_demand = int(max(0, round(pred_demand)))

            future_row['mean'] = pred_demand
            result.append(future_row)

            demand_buffer.append(pred_demand)
            if len(demand_buffer) > 60:
                demand_buffer = demand_buffer[-60:]

    result_df = pd.DataFrame(result)[['sku', 'city', 'date', 'mean']]

    # ì—°ì†ì„± ë³´ì •: 2022-12 ìµœê·¼ ìˆ˜ìš” ëŒ€ë¹„ 2023-01 ì´ˆ ì˜ˆì¸¡ì´ ê³¼ë„í•˜ê²Œ ë‚®ì•„ì§€ëŠ” í˜„ìƒ ì™„í™”
    try:
        last_ref = demand_data[(demand_data['date'] >= '2022-12-01') & (demand_data['date'] <= '2022-12-31')]
        ref_mean = last_ref.groupby(['city','sku'])['demand'].mean().rename('ref_mean')

        first_pred = result_df[result_df['date'] <= pd.Timestamp('2023-01-07')]
        pred_mean = first_pred.groupby(['city','sku'])['mean'].mean().rename('pred_mean')

        scale_df = ref_mean.reset_index().merge(pred_mean.reset_index(), on=['city','sku'], how='left')
        scale_df['pred_mean'] = scale_df['pred_mean'].fillna(scale_df['pred_mean'].median() if scale_df['pred_mean'].notna().any() else 1.0)
        scale_df['scale'] = scale_df['ref_mean'] / scale_df['pred_mean'].clip(lower=1e-6)
        # ê³¼ë„í•œ ìŠ¤ì¼€ì¼ì€ í´ë¦¬í•‘
        scale_df['scale'] = scale_df['scale'].clip(lower=0.8, upper=1.5)

        result_df = result_df.merge(scale_df[['city','sku','scale']], on=['city','sku'], how='left')
        result_df['scale'] = result_df['scale'].fillna(1.0)
        result_df['mean'] = (result_df['mean'] * result_df['scale']).round().astype(int)
        result_df = result_df.drop(columns=['scale'])
        print_progress("ğŸ”§ ì ìš©: 2023-01 ì—°ì†ì„± ë³´ì •(ìŠ¤ì¼€ì¼ë§ 0.5~2.0)", start_time)
    except Exception as e:
        print(f"[WARN] ì—°ì†ì„± ë³´ì • ë‹¨ê³„ ì‹¤íŒ¨: {e}")

    print_progress(f"ğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„ - í‰ê· : {result_df['mean'].mean():.2f}, ì¤‘ì•™ê°’: {result_df['mean'].median():.2f}, ìµœëŒ€: {result_df['mean'].max():.2f}, ìµœì†Œ: {result_df['mean'].min():.2f}", start_time)
    print_progress(f"ğŸ“Š 0ì´ ì•„ë‹Œ ì˜ˆì¸¡ê°’ ë¹„ìœ¨: {(result_df['mean'] > 0).mean():.3f}", start_time)
    return result_df


def predict_future_xgboost_batched(model, demand_data, feature_cols, label_encoders, start_time, events_df=None):
    """XGBoost ëª¨ë¸ë¡œ ë¯¸ë˜ ì˜ˆì¸¡ (ë‚ ì§œ ë‹¨ìœ„ ë°°ì¹˜ ì˜ˆì¸¡ìœ¼ë¡œ ì†ë„ ìµœì í™”)"""
    print_progress("ğŸ”® XGBoost ëª¨ë¸ë¡œ ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘... (ë°°ì¹˜ ì˜ˆì¸¡)", start_time)

    # 2023-2024 ë‚ ì§œ ìƒì„±
    future_dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')

    # ì™¸ìƒ/ë³´ì¡° í”¼ì²˜ ë¡œë“œ (trainê³¼ ë™ì¼ ì „ì²˜ë¦¬: pct_change/volatility_7d, í• ì¸ìœ¨ ìŠ¤ì¼€ì¼ ë“±)
    oil = pd.read_csv(DATA_DIR / "oil_price_processed.csv", parse_dates=["date"])
    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    currency = pd.read_csv(DATA_DIR / "currency_processed.csv", parse_dates=["date"])
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence_processed.csv", parse_dates=["date"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])

    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']

    marketing_agg = marketing.groupby(['date', 'country'])['spend_usd'].sum().reset_index()
    country_cov = consumer_conf[['date', 'country', 'confidence_index']]
    country_cov = country_cov.merge(marketing_agg, on=['date', 'country'], how='left')
    country_cov = country_cov.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how='left')
    country_cov = country_cov.merge(calendar[['date', 'country', 'season']], on=['date', 'country'], how='left')

    cov_date = oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']]
    cov_date = cov_date.merge(currency[['date'] + fx_cols], on='date', how='left')

    cov_future = country_cov.merge(cov_date, on='date', how='left')

    try:
        print_progress("ğŸ” cov_future preview:", start_time)
        print(f"  - shape: {cov_future.shape}")
        print(f"  - columns: {list(cov_future.columns)}")
        print(cov_future.head(5))
    except Exception as e:
        print(f"[WARN] cov_future preview failed: {e}")

    event_periods = get_hardcoded_event_periods()
    print_progress("ğŸ“¢ 2023-2024 í™•ì • ì´ë²¤íŠ¸ êµ¬ê°„:", start_time)
    for (country, year), (start_date, end_date) in event_periods.items():
        if year in [2023, 2024]:
            print(f"  - {country} | {start_date} ~ {end_date}")

    # ëª¨ë“  ì¡°í•©
    city_sku_combinations = demand_data[['city', 'sku']].drop_duplicates().reset_index(drop=True)
    result = []
    print_progress(f"ğŸ“Š ì˜ˆì¸¡ ëŒ€ìƒ: {len(city_sku_combinations)}ê°œ ì¡°í•© Ã— {len(future_dates)}ì¼ = {len(city_sku_combinations) * len(future_dates):,}ê°œ", start_time)

    # ì¸ë±ì‹± ìµœì í™”
    cov_future_idx = cov_future.set_index(['date', 'country']).sort_index()
    cov_date_idx = cov_date.set_index('date').sort_index()
    sku_launch_map = {row['sku']: row['launch_date'] for _, row in sku_meta.iterrows()}

    # ì¡°í•©ë³„ ë²„í¼/ìµœê·¼ê°’/ë©”íƒ€ ì¤€ë¹„
    buffers = {}
    recent_map = {}
    meta_map = {}
    for idx, (_, combo) in enumerate(city_sku_combinations.iterrows()):
        city, sku = combo['city'], combo['sku']
        recent_data = demand_data[(demand_data['city'] == city) & (demand_data['sku'] == sku)].tail(60)
        if len(recent_data) == 0:
            continue
        buf = recent_data['demand'].dropna().tolist() or [0.0]
        buffers[(city, sku)] = buf
        recent_map[(city, sku)] = recent_data
        meta_map[(city, sku)] = {
            'country': recent_data['country'].iloc[0] if 'country' in recent_data.columns else None,
            'family': recent_data['family'].iloc[0] if 'family' in recent_data.columns else None,
            'season': recent_data['season'].iloc[0] if 'season' in recent_data.columns else None,
        }
        if idx < 3:
            print(f"[DBG] {city}-{sku} initial demand_buffer (last 5): {buf[-5:]}  size={len(buf)}")

    # ì˜ˆì¸¡ ì „ GPU ê²½ë¡œ ì¬í™•ì¸
    try:
        import json as _json
        cfg_json = model.get_booster().save_config()
        cfg = _json.loads(cfg_json)
        generic = cfg.get('learner', {}).get('generic_param', {})
        used_predictor = str(generic.get('predictor', 'unknown'))
        device = str(generic.get('device', 'n/a'))
        print_progress(f"ğŸ” Predict Config â†’ predictor={used_predictor}, device={device}", start_time)
    except Exception as e:
        print(f"[WARN] ì˜ˆì¸¡ ì „ GPU ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}")

    # ë‚ ì§œ ë‹¨ìœ„ ë°°ì¹˜ ì˜ˆì¸¡
    for date in tqdm(future_dates, desc="ì˜ˆì¸¡ ì§„í–‰(ë‚ ì§œ ë°°ì¹˜)"):
        batch_rows = []
        batch_keys = []

        for _, combo in city_sku_combinations.iterrows():
            city, sku = combo['city'], combo['sku']
            key = (city, sku)
            if key not in buffers:
                continue
            recent_data = recent_map[key]
            buf = buffers[key]
            meta = meta_map[key]
            country = meta['country']
            family = meta['family']
            season_fixed = meta['season']

            future_row = {
                'date': date,
                'city': city,
                'sku': sku,
                'month': date.month,
                'dayofyear': date.dayofyear,
                'weekday': date.weekday(),
                'country': country,
                'family': family,
                'season': season_fixed,
            }

            # êµ­ê°€/ë‚ ì§œ ê³µë³€ëŸ‰
            try:
                cov_row = cov_future_idx.loc[(date, country)]
                future_row['confidence_index'] = cov_row.get('confidence_index', 0)
                future_row['spend_usd'] = cov_row.get('spend_usd', 0)
                future_row['avg_temp'] = cov_row.get('avg_temp', 0)
                future_row['humidity'] = cov_row.get('humidity', 0)
                future_row['season'] = cov_row.get('season', future_row.get('season', ''))
            except Exception:
                for col in ['confidence_index', 'spend_usd', 'avg_temp', 'humidity']:
                    future_row[col] = 0

            try:
                cov_date_row = cov_date_idx.loc[date]
                future_row['brent_usd'] = cov_date_row.get('brent_usd', 0)
                future_row['pct_change'] = cov_date_row.get('pct_change', 0)
                future_row['volatility_7d'] = cov_date_row.get('volatility_7d', 0)
                for fx in fx_cols:
                    future_row[fx] = cov_date_row.get(fx, 0)
            except Exception:
                for col in ['brent_usd', 'pct_change', 'volatility_7d'] + fx_cols:
                    future_row[col] = 0

            # ì¶œì‹œì¼ ê²½ê³¼ì¼
            launch_date = sku_launch_map.get(sku, None)
            if launch_date is not None and pd.notna(launch_date):
                future_row['days_since_launch'] = max(0, (date - launch_date).days)
            else:
                future_row['days_since_launch'] = 0

            # ì´ë²¤íŠ¸ í”Œë˜ê·¸
            is_event = 0
            for (cty, year), (start_date, end_date) in event_periods.items():
                if cty == country and date >= pd.to_datetime(start_date) and date <= pd.to_datetime(end_date):
                    is_event = 1
                    break
            future_row['is_event'] = is_event

            # ìˆ˜ìš” ì‹œê³„ì—´ í”¼ì²˜ (ë²„í¼ ê¸°ë°˜) - ì™„ì „ 0 ë²„í¼ ë°©ì§€ ê°€ë“œ
            if not buf:
                buf = [max(1.0, recent_data['demand'].mean() if len(recent_data) > 0 else 1.0)]
                buffers[key] = buf
            for lag in [1, 3, 7, 14]:
                future_row[f'demand_lag_{lag}'] = buf[-lag] if len(buf) >= lag else (buf[0] if len(buf) > 0 else 0)
            for window in [7, 14]:
                if len(buf) > 0:
                    series = buf[-window:] if len(buf) >= window else buf
                    future_row[f'demand_rolling_mean_{window}'] = float(np.mean(series))
                    future_row[f'demand_rolling_std_{window}'] = float(np.std(series, ddof=1)) if len(series) > 1 else 0.0
                else:
                    future_row[f'demand_rolling_mean_{window}'] = 0.0
                    future_row[f'demand_rolling_std_{window}'] = 0.0

            # ê°€ê²©/í• ì¸ ìµœê·¼ê°’ ë° ì‹œê³„ì—´
            if 'unit_price' in recent_data.columns and len(recent_data) > 0:
                future_row['unit_price'] = float(recent_data['unit_price'].ffill().iloc[-1])
            if 'discount_pct' in recent_data.columns and len(recent_data) > 0:
                future_row['discount_pct'] = float(recent_data['discount_pct'].ffill().iloc[-1])
                if future_row['discount_pct'] > 1.0:
                    future_row['discount_pct'] = future_row['discount_pct'] / 100.0
            for col in ['discount_pct', 'unit_price']:
                if col in recent_data.columns:
                    for lag in [1, 3, 7, 14]:
                        future_row[f'{col}_lag_{lag}'] = (
                            recent_data[col].iloc[-lag] if len(recent_data) >= lag else (recent_data[col].iloc[0] if len(recent_data) > 0 else 0)
                        )
                    for window in [7, 14]:
                        vals = recent_data[col].tail(window)
                        future_row[f'{col}_rolling_mean_{window}'] = vals.mean() if len(vals) > 0 else (recent_data[col].mean() if len(recent_data) > 0 else 0)

            # spend_usd lag/rolling
            for lag in [1, 3, 7, 14]:
                lag_date = date - pd.Timedelta(days=lag)
                try:
                    v = cov_future_idx.loc[(lag_date, country), 'spend_usd']
                    future_row[f'spend_usd_lag_{lag}'] = float(v) if pd.notna(v) else 0.0
                except Exception:
                    future_row[f'spend_usd_lag_{lag}'] = 0.0
            for window in [7, 14]:
                start_d = date - pd.Timedelta(days=window-1)
                try:
                    vals = cov_future_idx.loc[(slice(start_d, date), country), 'spend_usd']
                    future_row[f'spend_usd_rolling_mean_{window}'] = float(vals.mean()) if len(vals) > 0 else 0.0
                except Exception:
                    future_row[f'spend_usd_rolling_mean_{window}'] = 0.0

            # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
            for col in ['city', 'sku', 'country', 'family', 'season']:
                encoded_col = f"{col}_encoded"
                if encoded_col in feature_cols:
                    val = str(future_row.get(col, ''))
                    if col in label_encoders and len(label_encoders[col].classes_) > 0:
                        if val not in label_encoders[col].classes_:
                            val = str(label_encoders[col].classes_[0])
                        future_row[encoded_col] = label_encoders[col].transform([val])[0]
                    else:
                        future_row[encoded_col] = 0

            batch_rows.append(future_row)
            batch_keys.append(key)

        if not batch_rows:
            continue

        batch_df = pd.DataFrame(batch_rows)
        # ë””ë²„ê·¸: ì´ˆê¸° 1ì£¼ êµ¬ê°„, ìƒìœ„ 3ê°œ ì¡°í•© ìƒ˜í”Œì˜ í•µì‹¬ í”¼ì²˜ ìƒíƒœ ì¶œë ¥
        if date <= pd.Timestamp('2023-01-07'):
            sample_df = batch_df.head(3).copy()
            check_cols = [c for c in feature_cols if (
                c.startswith('demand_lag_') or c.startswith('demand_rolling_mean_') or
                c in ['unit_price','discount_pct','spend_usd','confidence_index','brent_usd']
            )]
            common = [c for c in check_cols if c in sample_df.columns]
            if len(sample_df) and len(common):
                nz_ratio = (sample_df[common].replace(0, np.nan).notna().mean(axis=1)).round(2).tolist()
                print_progress(f"[DBG] {str(date.date())} first3 nz_ratio={nz_ratio}", start_time)
                try:
                    for r in range(min(3, len(sample_df))):
                        cid, sk = sample_df.iloc[r]['city'], sample_df.iloc[r]['sku']
                        dl1 = sample_df.iloc[r].get('demand_lag_1', np.nan)
                        su = sample_df.iloc[r].get('spend_usd', np.nan)
                        up = sample_df.iloc[r].get('unit_price', np.nan)
                        dc = sample_df.iloc[r].get('discount_pct', np.nan)
                        print(f"  - {cid}-{sk} dlag1={dl1} spend={su} unit={up} disc={dc}")
                except Exception:
                    pass
        for col in feature_cols:
            if col not in batch_df.columns:
                batch_df[col] = 0
        X_batch = batch_df[feature_cols]
        preds = model.predict(X_batch)
        # ì™„ì „ 0 ë“œë¦¬í”„íŠ¸ ë°©ì§€: ë§¤ìš° ì‘ì€ ì˜ˆì¸¡ì€ ìµœê·¼ í‰ê· ì˜ ì‘ì€ ë¹„ìœ¨ë¡œ ë°”ë‹¥ ì„¤ì •
        # ìµœê·¼ê°’ ê¸°ì¤€ ë°”ë‹¥ê°’ ë§µ êµ¬ì„±
        floor_map = {}
        for key, recent_df in recent_map.items():
            m = float(recent_df['demand'].tail(14).mean()) if len(recent_df) > 0 else 0.0
            floor_map[key] = max(0.0, m * 0.10)  # ìµœê·¼ 10%ë¥¼ ë°”ë‹¥ìœ¼ë¡œ (ê°•í™”)

        for i, pred in enumerate(preds):
            city, sku = batch_keys[i]
            base_floor = floor_map.get((city, sku), 0.0)
            pred_adj = max(float(pred), base_floor)
            pred_demand = int(max(0, round(pred_adj)))
            result.append({
                'sku': sku,
                'city': city,
                'date': batch_rows[i]['date'],
                'mean': pred_demand,
            })
            buffers[(city, sku)].append(pred_demand)
            if len(buffers[(city, sku)]) > 60:
                buffers[(city, sku)] = buffers[(city, sku)][-60:]

    result_df = pd.DataFrame(result)[['sku', 'city', 'date', 'mean']]
    print_progress(f"ğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„ - í‰ê· : {result_df['mean'].mean():.2f}, ì¤‘ì•™ê°’: {result_df['mean'].median():.2f}, ìµœëŒ€: {result_df['mean'].max():.2f}, ìµœì†Œ: {result_df['mean'].min():.2f}", start_time)
    print_progress(f"ğŸ“Š 0ì´ ì•„ë‹Œ ì˜ˆì¸¡ê°’ ë¹„ìœ¨: {(result_df['mean'] > 0).mean():.3f}", start_time)
    return result_df


def generate_xgboost_forecast():
    """XGBoost ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡ ìƒì„± (LightGBM ë¡œì§ê³¼ ë™ì¼ íë¦„)"""
    print_progress("=== XGBoost ëª¨ë¸ ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡ ìƒì„± ===")
    total_start_time = time.time()

    # 1. ë°ì´í„° ë¡œë“œ
    demand_data, events_df, label_encoders = load_enhanced_training_data()

    if events_df is not None and len(events_df) > 0:
        print("\nğŸ“¢ í™•ì •ëœ ì´ë²¤íŠ¸ êµ¬ê°„ ëª©ë¡ (2018~2024):")
        events_df_sorted = events_df.sort_values(['year','country','start_date']) if 'year' in events_df.columns else events_df
        for _, ev in events_df_sorted.iterrows():
            yr = ev['start_date'].year
            if 2018 <= yr <= 2024:
                print(f"  - {ev['country']} | {ev['start_date'].date()} ~ {ev['end_date'].date()} (year={yr})")

    # 2. í”¼ì²˜ ì¤€ë¹„ (ë™ì¼ í”¼ì²˜ì…‹ ì¬ì‚¬ìš©)
    demand_data, feature_cols = prepare_lightgbm_features(demand_data)

    # 3. ë°ì´í„° ë¶„í• 
    print_progress("ğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...", total_start_time)
    train_data = demand_data[demand_data['date'] < '2022-01-01'].copy()
    val_data = demand_data[(demand_data['date'] >= '2022-01-01') & (demand_data['date'] < '2023-01-01')].copy()
    print_progress(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ - Train: {len(train_data):,}ê°œ, Val: {len(val_data):,}ê°œ", total_start_time)

    # 4. ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ (ë™ì¼ ìœ ì§€)
    print_progress("ğŸ” ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì¤‘...", total_start_time)
    X_train_sample = train_data[feature_cols].sample(n=min(10000, len(train_data)), random_state=42)
    _ = analyze_multicollinearity(X_train_sample, feature_cols, total_start_time)

    # 5. ëª¨ë¸ í•™ìŠµ
    model = train_xgboost_model(train_data, val_data, feature_cols, total_start_time)

    # 6. ê²€ì¦ ì„±ëŠ¥ í‰ê°€
    print_progress("ğŸ“ˆ ê²€ì¦ ì„±ëŠ¥ í‰ê°€ ì¤‘...", total_start_time)
    X_val = val_data[feature_cols]
    y_val = val_data['demand']
    val_pred = model.predict(X_val)
    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    val_r2 = r2_score(y_val, val_pred)
    print_progress(f"ğŸ“Š ê²€ì¦ ì„±ëŠ¥ - RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}", total_start_time)

    # 7. 2022ë…„ ê²€ì¦ ì‹œê°í™”
    create_xgboost_validation_visualization(val_data, val_pred, total_start_time)

    # 8. ë¯¸ë˜ ì˜ˆì¸¡ (2023-2024)
    print_progress("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘...", total_start_time)
    # ë°°ì¹˜ ì˜ˆì¸¡ ë²„ì „ìœ¼ë¡œ êµì²´
    result_df = predict_future_xgboost_batched(model, demand_data, feature_cols, label_encoders, total_start_time, events_df=events_df)

    # 9. ê²°ê³¼ ì €ì¥
    output_path = DATA_DIR / "xgboost_forecast_submission.csv"
    result_df.to_csv(output_path, index=False)
    print_progress(f"âœ… XGBoost ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ: {time.time() - total_start_time:.1f}ì´ˆ", total_start_time)
    print_progress(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_path}", total_start_time)
    print_progress(f"ğŸ“Š ì´ ì˜ˆì¸¡ ìˆ˜: {len(result_df):,}", total_start_time)
    print_progress(f"ğŸ“ˆ í‰ê·  ìˆ˜ìš”: {result_df['mean'].mean():.1f}", total_start_time)
    print_progress(f"ğŸ“Š ê²€ì¦ RMSE: {val_rmse:.4f}", total_start_time)

    # 10. ì „ì²´ íƒ€ì„ë¼ì¸ ì‹œê°í™”
    print_progress("ğŸ“Š 2018-2024 ì „ì²´ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œê°í™” ìƒì„± ì¤‘...", total_start_time)
    create_xgboost_full_timeline_visualization(demand_data, result_df, total_start_time)

    return result_df


def main():
    print_progress("=== XGBoost ëª¨ë¸ ê¸°ë°˜ ê³ ê¸‰ ì‹œê³„ì—´ ì˜ˆì¸¡ ===")
    _ = generate_xgboost_forecast()
    print_progress("âœ… XGBoost ëª¨ë¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

