# src/models/final_forecast_model.py
# LightGBM ê¸°ë°˜ ê³ ê¸‰ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ - ì¶œì œì ì ‘ê·¼ë²• ë°˜ì˜

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import platform
import warnings
warnings.filterwarnings('ignore')
import time
from datetime import datetime
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
import lightgbm as lgb

# ê²½ë¡œ ì„¤ì •
SCRIPT = Path(__file__).resolve()
PROJECT_ROOT = SCRIPT.parents[2]
DATA_DIR = Path("C:/projects/smartphone-supplychain/data")

def print_progress(message, start_time=None):
    """ì§„í–‰ìƒí™©ê³¼ ì‹œê°„ì„ ì¶œë ¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    current_time = datetime.now().strftime("%H:%M:%S")
    if start_time:
        elapsed = time.time() - start_time
        print(f"[{current_time}] â±ï¸ {elapsed:.1f}ì´ˆ - {message}")
    else:
        print(f"[{current_time}] {message}")

def get_country_mapping():
    """ë„ì‹œ-êµ­ê°€ ë§¤í•‘"""
    return {
        'Washington_DC': 'USA', 'New_York': 'USA', 'Chicago': 'USA', 'Dallas': 'USA',
        'Berlin': 'DEU', 'Munich': 'DEU', 'Frankfurt': 'DEU', 'Hamburg': 'DEU',
        'Paris': 'FRA', 'Lyon': 'FRA', 'Marseille': 'FRA', 'Toulouse': 'FRA',
        'Seoul': 'KOR', 'Busan': 'KOR', 'Incheon': 'KOR', 'Gwangju': 'KOR',
        'Tokyo': 'JPN', 'Osaka': 'JPN', 'Nagoya': 'JPN', 'Fukuoka': 'JPN',
        'Manchester': 'GBR', 'London': 'GBR', 'Birmingham': 'GBR', 'Glasgow': 'GBR',
        'Ottawa': 'CAN', 'Toronto': 'CAN', 'Vancouver': 'CAN', 'Montreal': 'CAN',
        'Canberra': 'AUS', 'Sydney': 'AUS', 'Melbourne': 'AUS', 'Brisbane': 'AUS',
        'Brasilia': 'BRA', 'Sao_Paulo': 'BRA', 'Rio_de_Janeiro': 'BRA', 'Salvador': 'BRA',
        'Pretoria': 'ZAF', 'Johannesburg': 'ZAF', 'Cape_Town': 'ZAF', 'Durban': 'ZAF'
    }

def get_hardcoded_event_periods():
    """
    Define hardcoded event periods based on analysis
    Returns: Dictionary with country-year as key and (start_date, end_date) as value
    """
    event_periods = {
        ('KOR', 2018): ('2018-02-15', '2018-03-19'),
        ('JPN', 2019): ('2019-01-19', '2019-03-03'),
        ('USA', 2020): ('2020-01-26', '2020-04-04'),
        ('USA', 2021): ('2021-03-13', '2021-06-06'),
        ('KOR', 2022): ('2022-02-05', '2022-04-12'),
        ('KOR', 2023): ('2023-06-01', '2023-08-31'),
        ('JPN', 2024): ('2024-02-01', '2024-04-30')
    }
    return event_periods

def load_enhanced_training_data():
    """LightGBM ëª¨ë¸ìš© ê³ ê¸‰ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    print_progress("=== LightGBM ëª¨ë¸ìš© ê³ ê¸‰ ë°ì´í„° ë¡œë“œ ===")
    start_time = time.time()
    
    # ë°ì´í„° ë¡œë“œ
    print_progress("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...", start_time)
    conn = sqlite3.connect(DATA_DIR / "demand_train.db")
    demand = pd.read_sql_query("SELECT * FROM demand_train", conn)
    conn.close()
    demand['date'] = pd.to_datetime(demand['date'])

    oil = pd.read_csv(DATA_DIR / "oil_price_processed.csv", parse_dates=["date"])
    currency = pd.read_csv(DATA_DIR / "currency_processed.csv", parse_dates=["date"])
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence_processed.csv", parse_dates=["date"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])
    ppt = pd.read_csv(DATA_DIR / "price_promo_train.csv", parse_dates=["date"])
    
    print_progress("ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...", start_time)
    
    # ê¸°ë³¸ í”¼ì²˜
    country_map = get_country_mapping()
    demand["country"] = demand["city"].map(country_map)
    demand["month"] = demand["date"].dt.month
    demand["dayofyear"] = demand["date"].dt.dayofyear
    demand["weekday"] = demand["date"].dt.weekday
    
    # ì™¸ë¶€ ë°ì´í„° ë³‘í•©
    # calendar ë°ì´í„°ëŠ” dateì™€ countryë¡œ ë³‘í•©
    demand = demand.merge(calendar[["date", "country", "season"]], on=["date", "country"], how="left")
    demand = demand.merge(sku_meta[["sku", "family", "storage_gb", "launch_date"]], on="sku", how="left")
    demand["days_since_launch"] = (demand["date"] - demand["launch_date"]).dt.days.clip(lower=0)

    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    demand = demand.merge(oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']], on='date', how='left')
    
    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']
    demand = demand.merge(currency[['date'] + fx_cols], on='date', how='left')
    
    demand = demand.merge(consumer_conf[['date', 'country', 'confidence_index']], on=['date', 'country'], how='left')
    
    marketing_agg = marketing.groupby(['date', 'country'])['spend_usd'].sum().reset_index()
    demand = demand.merge(marketing_agg, on=['date', 'country'], how='left')
    
    demand = demand.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how="left")
    demand = demand.merge(ppt[['date', 'sku', 'city', 'unit_price', 'discount_pct']], on=['date', 'sku', 'city'], how="left")
    
    # ì´ë²¤íŠ¸ ê¸°ê°„ ì„¤ì • (í•˜ë“œì½”ë”©)
    print_progress("ğŸ“… í•˜ë“œì½”ë”©ëœ ì´ë²¤íŠ¸ ê¸°ê°„ ì„¤ì • ì¤‘...")
    event_periods = get_hardcoded_event_periods()
    
    # ì´ë²¤íŠ¸ ê¸°ê°„ì„ DataFrameìœ¼ë¡œ ë³€í™˜
    events_list = []
    for (country, year), (start_date, end_date) in event_periods.items():
        events_list.append({
            'country': country,
            'year': year,
            'start_date': pd.to_datetime(start_date),
            'end_date': pd.to_datetime(end_date)
        })
    events_df = pd.DataFrame(events_list)
    
    print_progress(f"âœ… ì„¤ì •ëœ ì´ë²¤íŠ¸: {len(events_df)}ê°œ")
    for _, event in events_df.iterrows():
        print(f"  - {event['country']} ({event['year']}): {event['start_date'].strftime('%Y-%m-%d')} ~ {event['end_date'].strftime('%Y-%m-%d')}")
    
    # is_event ì»¬ëŸ¼ ìƒì„±
    demand['is_event'] = 0
    
    for _, event in events_df.iterrows():
        mask = (
            (demand['country'] == event['country']) & 
            (demand['date'] >= event['start_date']) & 
            (demand['date'] <= event['end_date'])
        )
        demand.loc[mask, 'is_event'] = 1
    
    event_count = demand['is_event'].sum()
    print_progress(f"âœ… ì´ë²¤íŠ¸ ê¸°ê°„ ë°ì´í„° í¬ì¸íŠ¸: {event_count:,}ê°œ")
    
    # ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±
    print_progress("ğŸ“ˆ ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì¤‘...", start_time)
        
    # ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±
    for col in ['demand']:
        if col in demand.columns:
            # Lag í”¼ì²˜
            for lag in [1, 3, 7, 14]:
                demand[f'{col}_lag_{lag}'] = demand.groupby(['city', 'sku'])[col].shift(lag)
            
            # Rolling í‰ê·  (transform ì‚¬ìš©ìœ¼ë¡œ ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²°)
            for window in [7, 14]:
                demand[f'{col}_rolling_mean_{window}'] = demand.groupby(['city', 'sku'])[col].transform(lambda x: x.rolling(window, min_periods=1).mean())
            
            # Rolling í‘œì¤€í¸ì°¨ (transform ì‚¬ìš©ìœ¼ë¡œ ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²°)
            for window in [7, 14]:
                demand[f'{col}_rolling_std_{window}'] = demand.groupby(['city', 'sku'])[col].transform(lambda x: x.rolling(window, min_periods=1).std())
    
    # ê³„ì ˆì„± ë° ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”©
    print_progress("ğŸ”¤ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”© ì¤‘...", start_time)
    
    # Label Encoding
    categorical_cols = ['city', 'sku', 'country', 'family', 'season']
    label_encoders = {}
    
    for col in categorical_cols:
        if col in demand.columns:
            le = LabelEncoder()
            demand[f'{col}_encoded'] = le.fit_transform(demand[col].astype(str))
            label_encoders[col] = le
    
    # í• ì¸ìœ¨ ì •ê·œí™”
    demand['discount_pct'] = demand['discount_pct'] / 100
    
    print("=== Before fillna ===")
    for col in ["demand","unit_price","discount_pct","spend_usd","brent_usd","confidence_index"]:
        if col in demand.columns:
            print(col, "nan:", demand[col].isna().sum(), "min:", demand[col].min() if demand[col].notna().any() else None)

    # ì–´ë–¤ ì¡°í•©ì´ ë¹„ì—ˆëŠ”ì§€ ìƒ˜í”Œ
    print(demand[demand["unit_price"].isna()].head(10)[["date","city","sku","unit_price"]])

    # NaN ì²˜ë¦¬
    demand = demand.fillna(0)
    
    # ë””ë²„ê¹…: ì£¼ìš” í”¼ì²˜ í†µê³„ëŸ‰ ì¶œë ¥
    print_progress("ğŸ” ë””ë²„ê¹…: ë°ì´í„° ë¡œë“œ í›„ ì£¼ìš” í”¼ì²˜ í†µê³„ëŸ‰ í™•ì¸ ì¤‘...", start_time)
    debug_cols = ['demand', 'demand_ratio', 'unit_price', 'discount_pct', 'spend_usd', 'brent_usd', 'confidence_index']
    for col in debug_cols:
        if col in demand.columns:
            print(f"  - {col}: Mean={demand[col].mean():.2f}, Std={demand[col].std():.2f}, Min={demand[col].min():.2f}, Max={demand[col].max():.2f}, NaN={demand[col].isnull().sum()}")
    print("--------------------------------------------------")
    print(demand.head(20))
    print_progress(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {demand.shape}", start_time)
    print_progress(f"ğŸ“ˆ ì´ë²¤íŠ¸ ê°ì§€: {len(events_df)}ê°œ", start_time)
    
    return demand, events_df, label_encoders

def augment_event_data(demand_data, events_df, augmentation_factor=3):
    """
    ì´ë²¤íŠ¸ ë°ì´í„° ì¦ê°•: ìœ ì‚¬í•œ íŒ¨í„´ì„ ê°€ì§„ ê°€ìƒ ë°ì´í„° ìƒì„±
    
    Args:
        demand_data: ì›ë³¸ ìˆ˜ìš” ë°ì´í„°
        events_df: ì´ë²¤íŠ¸ ì •ë³´ DataFrame
        augmentation_factor: ì¦ê°• ë°°ìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
        augmented_data: ì¦ê°•ëœ ë°ì´í„°
    """
    print_progress(f"ğŸ”„ ì´ë²¤íŠ¸ ë°ì´í„° ì¦ê°• ì‹œì‘ (ë°°ìˆ˜: {augmentation_factor})")
    
    # ì´ë²¤íŠ¸ ê¸°ê°„ ë°ì´í„° ì¶”ì¶œ
    event_data = demand_data[demand_data['is_event'] == 1].copy()
    
    if len(event_data) == 0:
        print("âš ï¸ ì¦ê°•í•  ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return demand_data
    
    print(f"  - ì›ë³¸ ì´ë²¤íŠ¸ ë°ì´í„°: {len(event_data):,}ê°œ")
    
    augmented_list = [demand_data.copy()]  # ì›ë³¸ ë°ì´í„° í¬í•¨
    
    for i in range(augmentation_factor - 1):  # ì¶”ê°€ë¡œ (augmentation_factor - 1)ë²ˆ ì¦ê°•
        # ì´ë²¤íŠ¸ ë°ì´í„° ë³µì‚¬
        augmented_event = event_data.copy()
        
        # 1. ë…¸ì´ì¦ˆ ì¶”ê°€ (ìˆ˜ìš” ë³€ë™ì„± ì‹œë®¬ë ˆì´ì…˜)
        noise_factor = 0.1  # 10% ë…¸ì´ì¦ˆ
        demand_noise = np.random.normal(0, noise_factor, len(augmented_event))
        augmented_event['demand'] = augmented_event['demand'] * (1 + demand_noise)
        augmented_event['demand'] = np.maximum(0, augmented_event['demand'])  # ìŒìˆ˜ ë°©ì§€
        
        # 2. ì‹œê³„ì—´ í”¼ì²˜ ì¬ê³„ì‚° (ìˆ˜ìš”ëŠ” ì›ë˜ ìŠ¤ì¼€ì¼ ê¸°ì¤€)
        for col in ['demand']:
            if col in augmented_event.columns:
                # Lag í”¼ì²˜
                for lag in [1, 3, 7, 14]:
                    col_name = f'{col}_lag_{lag}'
                    if col_name in augmented_event.columns:
                        augmented_event[col_name] = augmented_event.groupby(['city', 'sku'])[col].shift(lag)
                
                # Rolling í‰ê· 
                for window in [7, 14]:
                    col_name = f'{col}_rolling_mean_{window}'
                    if col_name in augmented_event.columns:
                        augmented_event[col_name] = augmented_event.groupby(['city', 'sku'])[col].transform(
                            lambda x: x.rolling(window, min_periods=1).mean()
                        )
                
                # Rolling í‘œì¤€í¸ì°¨
                for window in [7, 14]:
                    col_name = f'{col}_rolling_std_{window}'
                    if col_name in augmented_event.columns:
                        augmented_event[col_name] = augmented_event.groupby(['city', 'sku'])[col].transform(
                            lambda x: x.rolling(window, min_periods=1).std()
                        )
        
        # 3. ì™¸ë¶€ ìš”ì¸ì— ì•½ê°„ì˜ ë³€ë™ì„± ì¶”ê°€
        external_cols = ['confidence_index', 'spend_usd', 'avg_temp', 'humidity', 'brent_usd']
        for col in external_cols:
            if col in augmented_event.columns:
                # 5% ì´ë‚´ì˜ ì‘ì€ ë³€ë™ì„± ì¶”ê°€
                external_noise = np.random.normal(0, 0.05, len(augmented_event))
                augmented_event[col] = augmented_event[col] * (1 + external_noise)
        
        # 4. í™˜ìœ¨ ë°ì´í„°ì— ë³€ë™ì„± ì¶”ê°€
        fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']
        for col in fx_cols:
            if col in augmented_event.columns:
                fx_noise = np.random.normal(0, 0.02, len(augmented_event))  # 2% ë³€ë™ì„±
                augmented_event[col] = augmented_event[col] * (1 + fx_noise)
        
        # 6. NaN ê°’ ì²˜ë¦¬
        augmented_event = augmented_event.fillna(0)
        
        # ì¦ê°•ëœ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        augmented_list.append(augmented_event)
        
        print(f"  - ì¦ê°• {i+1} ì™„ë£Œ: {len(augmented_event):,}ê°œ")
    
    # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
    final_augmented = pd.concat(augmented_list, ignore_index=True)
    
    # ì¦ê°•ëœ ë°ì´í„°ì— ê³ ìœ  ì‹ë³„ì ì¶”ê°€ (ì¤‘ë³µ ì œê±° ë°©ì§€)
    final_augmented['augmentation_id'] = range(len(final_augmented))
    
    # ì¤‘ë³µ ì œê±°ëŠ” í•˜ì§€ ì•ŠìŒ (ì¦ê°•ëœ ë°ì´í„°ëŠ” ê°™ì€ ë‚ ì§œ-ë„ì‹œ-SKUë¼ë„ ë‹¤ë¥¸ ê°’)
    # final_augmented = final_augmented.drop_duplicates(
    #     subset=['date', 'city', 'sku'], 
    #     keep='first'
    # )
    
    print(f"  - ìµœì¢… ì¦ê°• ê²°ê³¼: {len(final_augmented):,}ê°œ (ì›ë³¸: {len(demand_data):,}ê°œ)")
    print(f"  - ì´ë²¤íŠ¸ ë°ì´í„° ë¹„ìœ¨: {final_augmented['is_event'].sum() / len(final_augmented) * 100:.2f}%")
    
    return final_augmented

def prepare_lightgbm_features(demand_data):
    """LightGBM ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„"""
    print_progress("ğŸ”§ LightGBM ëª¨ë¸ìš© í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
    start_time = time.time()
    
    # ê¸°ë³¸ í”¼ì²˜
    feature_cols = [
        # ì‹œê°„ ê¸°ë°˜
        'month', 'dayofyear', 'weekday',
        # ì œí’ˆ íŠ¹ì„±
        'storage_gb', 'days_since_launch',
        # ì¹´í…Œê³ ë¦¬ (ì¸ì½”ë”©ëœ ê²ƒ)
        'city_encoded', 'sku_encoded', 'country_encoded', 'family_encoded', 'season_encoded',
        # ê°€ê²© ë° í• ì¸
        'unit_price', 'discount_pct',
        # ë‚ ì”¨
        'avg_temp', 'humidity',
        # ì™¸ë¶€ ìš”ì¸
        'brent_usd', 'pct_change', 'volatility_7d',
        'confidence_index', 'spend_usd',
        # í™˜ìœ¨
        'EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X',
        # ì´ë²¤íŠ¸
        'is_event'
    ]
    
    # ì‹œê³„ì—´ í”¼ì²˜ ì¶”ê°€
    ts_features = [col for col in demand_data.columns if any(x in col for x in ['lag_', 'rolling_mean_', 'rolling_std_'])]
    feature_cols.extend(ts_features)
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
    feature_cols = [col for col in feature_cols if col in demand_data.columns]

    # ì‚¬ìš©ì ìš”ì²­: íŠ¹ì • ì‹œê³„ì—´ í”¼ì²˜ ì œì™¸ (demand ê´€ë ¨ í”¼ì²˜ëŠ” ìœ ì§€)
    remove_features = [
        # ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì¶”ê°€ ì œê±° í”¼ì²˜
        'dayofyear',  # monthì™€ ê±°ì˜ ì™„ë²½í•œ ìƒê´€ê´€ê³„ (r=0.9965)
        # ì¶”ê°€ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°ì„ ìœ„í•œ í”¼ì²˜ ì œê±°
    ]
    before_cnt = len(feature_cols)
    feature_cols = [col for col in feature_cols if col not in remove_features]
    removed_cnt = before_cnt - len(feature_cols)
    if removed_cnt > 0:
        print_progress(f"ğŸ§¹ ì œì™¸í•œ í”¼ì²˜ ìˆ˜: {removed_cnt}ê°œ ({[f for f in remove_features if f in demand_data.columns]})", start_time)
    
    print_progress(f"âœ… í”¼ì²˜ ì¤€ë¹„ ì™„ë£Œ:", start_time)
    print_progress(f"  - ì´ í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ", start_time)
    print_progress(f"  - ì‹œê³„ì—´ í”¼ì²˜: {len(ts_features)}ê°œ", start_time)
    
    return demand_data, feature_cols

def calculate_vif(X, feature_names):
    """VIF(Variance Inflation Factor) ê³„ì‚°"""
    print_progress("ğŸ” VIF ë¶„ì„ ì¤‘...")
    
    vif_data = []
    for i, feature in enumerate(feature_names):
        # í•´ë‹¹ í”¼ì²˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í”¼ì²˜ë“¤ë¡œ íšŒê·€
        X_temp = X.drop(columns=[feature])
        y_temp = X[feature]
        
        # ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
        model = LinearRegression()
        model.fit(X_temp, y_temp)
        
        # RÂ² ê³„ì‚°
        r_squared = model.score(X_temp, y_temp)
        
        # VIF ê³„ì‚° (RÂ²ê°€ 1ì— ê°€ê¹Œìš°ë©´ VIFê°€ ë§¤ìš° ì»¤ì§)
        if r_squared < 0.999:  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì„ê³„ê°’
            vif = 1 / (1 - r_squared)
        else:
            vif = float('inf')
        
        vif_data.append({
            'feature': feature,
            'vif': vif,
            'r_squared': r_squared
        })
    
    vif_df = pd.DataFrame(vif_data)
    vif_df = vif_df.sort_values('vif', ascending=False)
    
    return vif_df

def analyze_multicollinearity(X, feature_names, start_time):
    """ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ (VIF + ìƒê´€ê´€ê³„)"""
    print_progress("ğŸ” ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì‹œì‘...", start_time)
    
    # 1. VIF ë¶„ì„
    vif_df = calculate_vif(X, feature_names)
    
    # VIF ê²°ê³¼ ì¶œë ¥
    print_progress("ğŸ“Š VIF ë¶„ì„ ê²°ê³¼:", start_time)
    print("  - VIF > 10: ì‹¬ê°í•œ ë‹¤ì¤‘ê³µì„ ì„±")
    print("  - VIF > 5: ì£¼ì˜ê°€ í•„ìš”í•œ ë‹¤ì¤‘ê³µì„ ì„±")
    print("  - VIF > 2: ì•½ê°„ì˜ ë‹¤ì¤‘ê³µì„ ì„±")
    print()
    
    high_vif_features = vif_df[vif_df['vif'] > 10]
    moderate_vif_features = vif_df[(vif_df['vif'] > 5) & (vif_df['vif'] <= 10)]
    
    if not high_vif_features.empty:
        print("ğŸš¨ ì‹¬ê°í•œ ë‹¤ì¤‘ê³µì„ ì„± (VIF > 10):")
        for _, row in high_vif_features.iterrows():
            print(f"  - {row['feature']}: VIF={row['vif']:.2f}, RÂ²={row['r_squared']:.4f}")
        print()
    
    if not moderate_vif_features.empty:
        print("âš ï¸ ì£¼ì˜ê°€ í•„ìš”í•œ ë‹¤ì¤‘ê³µì„ ì„± (VIF > 5):")
        for _, row in moderate_vif_features.iterrows():
            print(f"  - {row['feature']}: VIF={row['vif']:.2f}, RÂ²={row['r_squared']:.4f}")
        print()
    
    # VIF ê²°ê³¼ ì €ì¥
    vif_csv_path = DATA_DIR / 'vif_analysis.csv'
    vif_df.to_csv(vif_csv_path, index=False)
    print_progress(f"ğŸ“ VIF ë¶„ì„ ê²°ê³¼ ì €ì¥: {vif_csv_path}", start_time)
    
    # 2. ìƒê´€ê´€ê³„ ë¶„ì„
    print_progress("ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...", start_time)
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    corr_matrix = X.corr()
    
    # ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸° (ì ˆëŒ“ê°’ > 0.8)
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) > 0.8:
                high_corr_pairs.append({
                    'feature1': corr_matrix.columns[i],
                    'feature2': corr_matrix.columns[j],
                    'correlation': corr_val
                })
    
    if high_corr_pairs:
        print("ğŸ”— ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.8):")
        high_corr_pairs.sort(key=lambda x: abs(x['correlation']), reverse=True)
        for pair in high_corr_pairs[:20]:  # ìƒìœ„ 20ê°œë§Œ ì¶œë ¥
            print(f"  - {pair['feature1']} â†” {pair['feature2']}: r={pair['correlation']:.4f}")
        print()
    
    # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì €ì¥
    plt.figure(figsize=(20, 16))
    
    # ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ì´ 0.5 ì´ìƒì¸ ê²ƒë§Œ í‘œì‹œ
    mask = np.abs(corr_matrix) < 0.5
    sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": .8})
    plt.title('Feature Correlation Matrix (|r| >= 0.5)', fontsize=16)
    plt.tight_layout()
    
    corr_png_path = DATA_DIR / 'feature_correlation_heatmap.png'
    plt.savefig(corr_png_path, dpi=300, bbox_inches='tight')
    plt.close()
    print_progress(f"ğŸ“ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì €ì¥: {corr_png_path}", start_time)
    
    # 3. í”¼ì²˜ ê·¸ë£¹ë³„ ë¶„ì„
    print_progress("ğŸ“Š í”¼ì²˜ ê·¸ë£¹ë³„ ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„...", start_time)
    
    # ì‹œê³„ì—´ í”¼ì²˜ ê·¸ë£¹
    lag_features = [f for f in feature_names if 'lag_' in f]
    rolling_mean_features = [f for f in feature_names if 'rolling_mean_' in f]
    rolling_std_features = [f for f in feature_names if 'rolling_std_' in f]
    
    print(f"  - Lag í”¼ì²˜ ìˆ˜: {len(lag_features)}")
    print(f"  - Rolling Mean í”¼ì²˜ ìˆ˜: {len(rolling_mean_features)}")
    print(f"  - Rolling Std í”¼ì²˜ ìˆ˜: {len(rolling_std_features)}")
    
    # ê° ê·¸ë£¹ ë‚´ì—ì„œ ë†’ì€ VIFë¥¼ ê°€ì§„ í”¼ì²˜ë“¤
    for group_name, group_features in [('Lag', lag_features), 
                                      ('Rolling Mean', rolling_mean_features),
                                      ('Rolling Std', rolling_std_features)]:
        if group_features:
            group_vif = vif_df[vif_df['feature'].isin(group_features)]
            high_vif_in_group = group_vif[group_vif['vif'] > 5]
            if not high_vif_in_group.empty:
                print(f"  - {group_name} ê·¸ë£¹ ë‚´ ë†’ì€ VIF í”¼ì²˜:")
                for _, row in high_vif_in_group.iterrows():
                    print(f"    * {row['feature']}: VIF={row['vif']:.2f}")
    
    return vif_df, high_corr_pairs

def train_lightgbm_model(train_data, val_data, feature_cols, start_time):
    """LightGBM ëª¨ë¸ í•™ìŠµ"""
    print_progress("ğŸš€ LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘...", start_time)
        
    # ë°ì´í„° ì¤€ë¹„
    X_train = train_data[feature_cols]
    y_train = train_data['demand']  # íƒ€ê²Ÿì„ ì›ë˜ ìˆ˜ìš”ë¡œ ë³€ê²½
    X_val = val_data[feature_cols]
    y_val = val_data['demand']  # íƒ€ê²Ÿì„ ì›ë˜ ìˆ˜ìš”ë¡œ ë³€ê²½
    
    print_progress(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {X_train.shape}, ê²€ì¦ ë°ì´í„°: {X_val.shape}", start_time)
    
    # ë””ë²„ê¹…: ì´ë²¤íŠ¸ ë°ì´í„° ë¶„ì„
    print_progress("ğŸ” ë””ë²„ê¹…: ì´ë²¤íŠ¸ ë°ì´í„° ë¶„ì„ ì¤‘...", start_time)
    train_event_count = train_data['is_event'].sum()
    train_total_count = len(train_data)
    val_event_count = val_data['is_event'].sum()
    val_total_count = len(val_data)
    
    print(f"  - í›ˆë ¨ ë°ì´í„°: ì´ {train_total_count:,}ê°œ ì¤‘ ì´ë²¤íŠ¸ {train_event_count:,}ê°œ ({train_event_count/train_total_count*100:.2f}%)")
    print(f"  - ê²€ì¦ ë°ì´í„°: ì´ {val_total_count:,}ê°œ ì¤‘ ì´ë²¤íŠ¸ {val_event_count:,}ê°œ ({val_event_count/val_total_count*100:.2f}%)")
    
    # ì´ë²¤íŠ¸ ê¸°ê°„ì˜ ìˆ˜ìš” í†µê³„
    event_demand = train_data[train_data['is_event'] == 1]['demand']
    non_event_demand = train_data[train_data['is_event'] == 0]['demand']
    print(f"  - ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜ìš”: í‰ê· ={event_demand.mean():.1f}, ì¤‘ì•™ê°’={event_demand.median():.1f}, ìµœëŒ€={event_demand.max():.1f}")
    print(f"  - ë¹„ì´ë²¤íŠ¸ ê¸°ê°„ ìˆ˜ìš”: í‰ê· ={non_event_demand.mean():.1f}, ì¤‘ì•™ê°’={non_event_demand.median():.1f}, ìµœëŒ€={non_event_demand.max():.1f}")
    print(f"  - ì´ë²¤íŠ¸/ë¹„ì´ë²¤íŠ¸ ìˆ˜ìš” ë¹„ìœ¨: {event_demand.mean()/non_event_demand.mean():.2f}ë°°")
    print("--------------------------------------------------")
    
    # ë””ë²„ê¹…: í•™ìŠµ/ê²€ì¦ ë°ì´í„° í†µê³„ëŸ‰ ì¶œë ¥
    print_progress("ğŸ” ë””ë²„ê¹…: í•™ìŠµ/ê²€ì¦ ë°ì´í„° í†µê³„ëŸ‰ í™•ì¸ ì¤‘...", start_time)
    print(f"  - X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
    print(f"  - X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
    print(f"  - y_train (demand) stats: Mean={y_train.mean():.4f}, Std={y_train.std():.4f}, Min={y_train.min():.4f}, Max={y_train.max():.4f}")
    print(f"  - y_val (demand) stats: Mean={y_val.mean():.4f}, Std={y_val.std():.4f}, Min={y_val.min():.4f}, Max={y_val.max():.4f}")
    print("--------------------------------------------------")
    
    # LightGBM ë°ì´í„°ì…‹ ìƒì„±
    # ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ìƒì„± (ì´ë²¤íŠ¸ ê¸°ê°„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    # is_eventê°€ 1ì¸ ê²½ìš° ê°€ì¤‘ì¹˜ 200 (ëŒ€í­ ì¦ê°€), 0ì¸ ê²½ìš° ê°€ì¤‘ì¹˜ 1
    event_weight = 100
    weights = np.where(train_data['is_event'] == 1, event_weight, 1)
    
    # ì¶”ê°€ ê°€ì¤‘ì¹˜ ì œê±°: event_intensity ë¯¸ì‚¬ìš©
    final_weights = weights
    
    print(f"  - ì´ë²¤íŠ¸ ê°€ì¤‘ì¹˜: {event_weight}")
    print(f"  - ì´ë²¤íŠ¸ ê¸°ê°„ í‰ê·  ê°€ì¤‘ì¹˜: {final_weights[train_data['is_event'] == 1].mean():.1f}")
    print(f"  - ë¹„ì´ë²¤íŠ¸ ê¸°ê°„ í‰ê·  ê°€ì¤‘ì¹˜: {final_weights[train_data['is_event'] == 0].mean():.1f}")
    print(f"  - ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {final_weights[train_data['is_event'] == 1].mean() / final_weights[train_data['is_event'] == 0].mean():.1f}ë°°")
    # ----------------------------------

    # LightGBM ë°ì´í„°ì…‹ ìƒì„± ì‹œ weight íŒŒë¼ë¯¸í„° ì¶”ê°€
    train_dataset = lgb.Dataset(X_train, label=y_train, weight=final_weights)
    val_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset)
    
    # LightGBM íŒŒë¼ë¯¸í„° - ì´ë²¤íŠ¸ ì˜ˆì¸¡ì— ìµœì í™”
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 128,  # ì¦ê°€: ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
        'learning_rate': 0.005,  # ê°ì†Œ: ë” ì„¸ë°€í•œ í•™ìŠµ
        'feature_fraction': 0.8,  # ê°ì†Œ: ê³¼ì í•© ë°©ì§€
        'bagging_fraction': 0.7,  # ê°ì†Œ: ê³¼ì í•© ë°©ì§€
        'bagging_freq': 3,  # ì¦ê°€: ë” ìì£¼ bagging
        'min_data_in_leaf': 20,  # ì¶”ê°€: ê³¼ì í•© ë°©ì§€
        'min_gain_to_split': 0.1,  # ì¶”ê°€: ì˜ë¯¸ìˆëŠ” ë¶„í• ë§Œ
        'verbose': -1,
        'random_state': 42
    }
    
    # ëª¨ë¸ í•™ìŠµ
    print_progress("ğŸ“š ëª¨ë¸ í•™ìŠµ ì¤‘...", start_time)
    model = lgb.train(
        params,
        train_dataset,
        valid_sets=[train_dataset, val_dataset],
        valid_names=['train', 'valid'],
        num_boost_round=2000,
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=True),
            lgb.log_evaluation(period=100)
        ]
    )
    
    print_progress(f"âœ… LightGBM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {time.time() - start_time:.1f}ì´ˆ", start_time)

    # -----------------------------
    # í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° ë° ì €ì¥/ì‹œê°í™”
    # -----------------------------
    try:
        print_progress("ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° ì¤‘...", start_time)
        gain_importance = model.feature_importance(importance_type='gain')
        split_importance = model.feature_importance(importance_type='split')
        fi_df = pd.DataFrame({
            'feature': feature_cols,
            'gain': gain_importance,
            'split': split_importance
        })
        fi_df['gain_pct'] = fi_df['gain'] / (fi_df['gain'].sum() + 1e-9)
        fi_df['split_pct'] = fi_df['split'] / (fi_df['split'].sum() + 1e-9)
        fi_df = fi_df.sort_values('gain', ascending=False).reset_index(drop=True)

        # is_event í”¼ì²˜ ì¤‘ìš”ë„ í™•ì¸
        is_event_importance = fi_df[fi_df['feature'] == 'is_event']
        if not is_event_importance.empty:
            print(f"ğŸ” is_event í”¼ì²˜ ì¤‘ìš”ë„: gain={is_event_importance['gain'].iloc[0]:.1f}, gain_pct={is_event_importance['gain_pct'].iloc[0]*100:.2f}%")
        else:
            print("âš ï¸ is_event í”¼ì²˜ê°€ í”¼ì²˜ ì¤‘ìš”ë„ì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ")           


        # ì €ì¥
        fi_csv_path = DATA_DIR / 'lightgbm_feature_importance.csv'
        fi_df.to_csv(fi_csv_path, index=False)
        print_progress(f"ğŸ“ í”¼ì²˜ ì¤‘ìš”ë„ CSV ì €ì¥: {fi_csv_path}", start_time)

        # ìƒìœ„ 30ê°œ ì‹œê°í™”
        top_n = min(30, len(fi_df))
        top_df = fi_df.head(top_n).iloc[::-1]  # ê°€ë…ì„±ì„ ìœ„í•´ ì—­ìˆœ í‘œì‹œ
        plt.figure(figsize=(10, max(6, top_n * 0.3)))
        plt.barh(top_df['feature'], top_df['gain'])
        plt.title('LightGBM Feature Importance (gain ê¸°ì¤€)')
        plt.xlabel('Gain')
        plt.tight_layout()
        fi_png_path = DATA_DIR / 'lightgbm_feature_importance_top30.png'
        plt.savefig(fi_png_path, dpi=300, bbox_inches='tight')
        plt.close()
        print_progress(f"ğŸ“ í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„ ì €ì¥: {fi_png_path}", start_time)

        # ì½˜ì†” ìƒìœ„ 20ê°œ ì¶œë ¥
        print("\nìƒìœ„ ì¤‘ìš” í”¼ì²˜ (gain ê¸°ì¤€) Top 20:")
        for i, row in fi_df.head(20).iterrows():
            print(f"  {i+1:2d}. {row['feature']}: gain={row['gain']:.1f}, split={row['split']}, gain_pct={row['gain_pct']*100:.2f}%")
        print()
    except Exception as e:
        print(f"âš ï¸ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°/ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    return model

def create_lightgbm_validation_visualization(val_data, val_pred, start_time):
    """LightGBM ëª¨ë¸ 2022ë…„ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”"""
    print_progress("ğŸ“Š LightGBM ëª¨ë¸ 2022ë…„ ê²€ì¦ ì‹œê°í™” ìƒì„± ì¤‘...", start_time)
    
    # val_predë¥¼ val_dataì— ì¶”ê°€í•˜ì—¬ ì¸ë±ìŠ¤ ë§¤ì¹­ ë¬¸ì œ í•´ê²°
    val_data_with_pred = val_data.copy()
    val_data_with_pred['predicted_demand'] = val_pred
    
    # ë””ë²„ê¹…: ìƒ˜í”Œë§ëœ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ í†µê³„ í™•ì¸
    print_progress("ğŸ” ë””ë²„ê¹…: 2022ë…„ ì‹œê°í™” ìƒ˜í”Œ ë°ì´í„° ì˜ˆì¸¡ê°’ í†µê³„ í™•ì¸ ì¤‘...", start_time)
    sample_combinations = val_data_with_pred[['city', 'sku']].drop_duplicates().head(5)
    for i, (_, combo) in enumerate(sample_combinations.iterrows()):
        city, sku = combo['city'], combo['sku']
        combo_data = val_data_with_pred[(val_data_with_pred['city'] == city) & (val_data_with_pred['sku'] == sku)].copy()
        if not combo_data.empty:
            actual = combo_data['demand'].values
            predicted = combo_data['predicted_demand'].values
            print(f"  - {city}-{sku} (Actual): Mean={np.mean(actual):.2f}, Max={np.max(actual):.2f}")
            print(f"  - {city}-{sku} (Predicted): Mean={np.mean(predicted):.2f}, Max={np.max(predicted):.2f}, Non-zero ratio={np.sum(predicted > 0) / len(predicted) * 100:.2f}%")
    print("--------------------------------------------------")
    
    # ìƒ˜í”Œë§ (í•œêµ­ 4ê°œ ë„ì‹œ ê³ ì •)
    korean_cities = ['Seoul', 'Busan', 'Incheon', 'Gwangju']
    sample_combinations = []
    
    for city in korean_cities:
        city_data = val_data_with_pred[val_data_with_pred['city'] == city]
        if not city_data.empty:
            # í•´ë‹¹ ë„ì‹œì˜ ì²« ë²ˆì§¸ SKU ì„ íƒ
            first_sku = city_data['sku'].iloc[0]
            sample_combinations.append({'city': city, 'sku': first_sku})
    
    if len(sample_combinations) == 0:
        # í•œêµ­ ë„ì‹œê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
        sample_combinations = val_data_with_pred[['city', 'sku']].drop_duplicates().head(5).to_dict('records')
    
    fig, axes = plt.subplots(len(sample_combinations), 1, figsize=(15, 4 * len(sample_combinations)))
    if len(sample_combinations) == 1:
        axes = [axes]
    
    for i, combo in enumerate(sample_combinations):
        city, sku = combo['city'], combo['sku']
        
        # í•´ë‹¹ city-sku ì¡°í•©ì˜ ë°ì´í„° ì¶”ì¶œ
        mask = (val_data_with_pred['city'] == city) & (val_data_with_pred['sku'] == sku)
        combo_data = val_data_with_pred[mask].sort_values('date')
        
        if len(combo_data) > 0:
            # ì‹¤ì œ ìˆ˜ìš” (ì›ë˜ ìŠ¤ì¼€ì¼)
            actual_vals = combo_data['demand'].values
            # ì˜ˆì¸¡ ìˆ˜ìš” (ì›ë˜ ìŠ¤ì¼€ì¼)
            pred_vals = combo_data['predicted_demand'].values
            
            axes[i].plot(actual_vals, label='Actual', linewidth=2)
            axes[i].plot(pred_vals, label='Predicted', linewidth=2, linestyle='--')
            axes[i].set_title(f'2022 Validation - {city} - {sku}')
            axes[i].set_xlabel('Index')
            axes[i].set_ylabel('Demand')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        else:
            axes[i].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[i].transAxes)
            axes[i].set_title(f'2022 Validation - {city} - {sku}')
    
    plt.tight_layout()
    
    # íŒŒì¼ ì €ì¥
    output_path = DATA_DIR / "lightgbm_validation_2022.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print_progress(f"ğŸ“ Saved LightGBM 2022 validation plot: {output_path}", start_time)

def create_lightgbm_full_timeline_visualization(demand_data, result_df, start_time):
    """LightGBM ëª¨ë¸ 2018-2024 ì „ì²´ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œê°í™”"""
    print_progress("ğŸ“Š Creating LightGBM 2018-2024 full timeline plot...", start_time)
    
    # ë””ë²„ê¹…: ì „ì²´ íƒ€ì„ë¼ì¸ ì‹œê°í™” ë°ì´í„° ì˜ˆì¸¡ê°’ í†µê³„ í™•ì¸
    print_progress("ğŸ” Debug: Checking stats for full timeline visualization...", start_time)
    print(f"  - 2023-2024 ì˜ˆì¸¡ê°’ (result_df): Mean={result_df['mean'].mean():.2f}, Max={result_df['mean'].max():.2f}, Non-zero ratio={np.sum(result_df['mean'] > 0) / len(result_df) * 100:.2f}%")
    print("--------------------------------------------------")
    
    # ì‹¤ì œ ë°ì´í„° (2018-2022) - ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    actual_data = demand_data[['date', 'city', 'sku', 'demand']].copy()
    actual_data['type'] = 'actual'
    
    # ì˜ˆì¸¡ ë°ì´í„° (2023-2024)
    pred_vis_data = result_df[['date', 'city', 'sku', 'mean']].copy()
    pred_vis_data = pred_vis_data.rename(columns={'mean': 'demand'})
    pred_vis_data['type'] = 'predicted'
    
    # ë°ì´í„° í†µí•©
    combined_data = pd.concat([actual_data[['date', 'city', 'sku', 'demand', 'type']], pred_vis_data], ignore_index=True)
    combined_data['date'] = pd.to_datetime(combined_data['date'])
    
    # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì¡´ì¬í•˜ëŠ” ë„ì‹œì™€ SKU ìƒ˜í”Œë§
    available_cities = combined_data['city'].unique()
    available_skus = combined_data['sku'].unique()
    
    # ìƒìœ„ 5ê°œ ë„ì‹œì™€ 3ê°œ SKU ì„ íƒ
    sample_cities = available_cities[:5] if len(available_cities) >= 5 else available_cities
    sample_skus = available_skus[:3] if len(available_skus) >= 3 else available_skus
    
    print(f"ğŸ“Š Sample cities for plotting: {sample_cities}")
    print(f"ğŸ“Š Sample SKUs for plotting: {sample_skus}")
    
    fig, axes = plt.subplots(len(sample_cities), len(sample_skus), figsize=(24, 18))
    fig.suptitle('LightGBM: 2018-2024 Full Timeline - Actual vs Predicted', fontsize=16, fontweight='bold')
    
    for i, city in enumerate(sample_cities):
        for j, sku in enumerate(sample_skus):
            city_sku_data = combined_data[(combined_data['city'] == city) & (combined_data['sku'] == sku)]
            
            if len(city_sku_data) > 0:
                city_sku_data = city_sku_data.sort_values('date')
                
                # ì‹¤ì œ ë°ì´í„° (2018-2022)
                actual_mask = city_sku_data['type'] == 'actual'
                actual_plot = city_sku_data[actual_mask]
                
                # ì˜ˆì¸¡ ë°ì´í„° (2023-2024)
                pred_mask = city_sku_data['type'] == 'predicted'
                pred_plot = city_sku_data[pred_mask]
                
                if len(actual_plot) > 0:
                    axes[i, j].plot(actual_plot['date'], actual_plot['demand'], 
                                   label='Actual (2018-2022)', color='blue', linewidth=2)
                
                if len(pred_plot) > 0:
                    axes[i, j].plot(pred_plot['date'], pred_plot['demand'], 
                                   label='Predicted (2023-2024)', color='red', linewidth=2, linestyle='--')
                
                # 2023ë…„ ì‹œì‘ì  í‘œì‹œ
                axes[i, j].axvline(x=pd.Timestamp('2023-01-01'), color='green', linestyle=':', 
                                   alpha=0.7, label='Forecast start')
                
                axes[i, j].set_title(f'{city} - {sku}', fontsize=12, fontweight='bold')
                axes[i, j].set_xlabel('Date')
                axes[i, j].set_ylabel('Demand')
                axes[i, j].legend()
                axes[i, j].grid(True, alpha=0.3)
                
                # xì¶• ë‚ ì§œ í¬ë§·íŒ…
                axes[i, j].tick_params(axis='x', rotation=45)
                
                # yì¶• ë²”ìœ„ ì„¤ì •
                if len(actual_plot) > 0 and len(pred_plot) > 0:
                    y_min = min(actual_plot['demand'].min(), pred_plot['demand'].min())
                    y_max = max(actual_plot['demand'].max(), pred_plot['demand'].max())
                    axes[i, j].set_ylim([y_min * 0.8, y_max * 1.2])
            else:
                axes[i, j].text(0.5, 0.5, 'ë°ì´í„° ì—†ìŒ', ha='center', va='center', 
                               transform=axes[i, j].transAxes)
                axes[i, j].set_title(f'{city} - {sku}', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    
    # íŒŒì¼ ì €ì¥
    output_path = DATA_DIR / "lightgbm_full_timeline_2018_2024.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print_progress(f"ğŸ“ Saved LightGBM full timeline plot: {output_path}", start_time)

def predict_future_lightgbm(model, demand_data, feature_cols, label_encoders, start_time, events_df=None):
    """LightGBM ëª¨ë¸ë¡œ ë¯¸ë˜ ì˜ˆì¸¡"""
    print_progress("ğŸ”® LightGBM ëª¨ë¸ë¡œ ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘...", start_time)
    
    # 2023-2024ë…„ ë‚ ì§œ ìƒì„±
    future_dates = pd.date_range(start='2023-01-01', end='2024-12-31', freq='D')

    # 2023-2024 ì™¸ìƒ/ë³´ì¡° í”¼ì²˜ ì‹¤ì œ ê°’ ë¡œë“œ (í‰ê· ê°’ ì‚¬ìš© ëŒ€ì‹  ì‹¤ì œ ê°’ ì‚¬ìš©)
    oil = pd.read_csv(DATA_DIR / "oil_price_processed.csv", parse_dates=["date"])
    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    currency = pd.read_csv(DATA_DIR / "currency_processed.csv", parse_dates=["date"])
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence_processed.csv", parse_dates=["date"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])

    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']

    # êµ­ê°€ë³„(day, country) ê³µë³€ëŸ‰
    marketing_agg = marketing.groupby(['date', 'country'])['spend_usd'].sum().reset_index()
    country_cov = consumer_conf[['date', 'country', 'confidence_index']]
    country_cov = country_cov.merge(marketing_agg, on=['date', 'country'], how='left')
    country_cov = country_cov.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how='left')
    country_cov = country_cov.merge(calendar[['date', 'country', 'season']], on=['date', 'country'], how='left')

    # ë‚ ì§œ(day) ê³µë³€ëŸ‰
    cov_date = oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']]
    cov_date = cov_date.merge(currency[['date'] + fx_cols], on='date', how='left')

    # ìµœì¢… ë¯¸ë˜ ê³µë³€ëŸ‰ í…Œì´ë¸”
    cov_future = country_cov.merge(cov_date, on='date', how='left')

    # ë””ë²„ê¹…: cov í…Œì´ë¸” êµ¬ì¡° ë¯¸ë¦¬ë³´ê¸°
    try:
        print_progress("ğŸ” cov_future preview:", start_time)
        print(f"  - shape: {cov_future.shape}")
        print(f"  - columns: {list(cov_future.columns)}")
        print(cov_future.head(5))
    except Exception as e:
        print(f"[WARN] cov_future preview failed: {e}")
    
    # 2023-2024 ì´ë²¤íŠ¸ ì‚¬ì „ ì¤€ë¹„ (í•˜ë“œì½”ë”©ëœ ì´ë²¤íŠ¸ ê¸°ê°„ ì‚¬ìš©)
    event_periods = get_hardcoded_event_periods()
    print_progress("ğŸ“¢ 2023-2024 í™•ì • ì´ë²¤íŠ¸ êµ¬ê°„:")
    for (country, year), (start_date, end_date) in event_periods.items():
        if year in [2023, 2024]:
            print(f"  - {country} | {start_date} ~ {end_date}")
    
    # ëª¨ë“  city-sku ì¡°í•©
    city_sku_combinations = demand_data[['city', 'sku']].drop_duplicates()
    
    result = []
    
    print_progress(f"ğŸ“Š ì˜ˆì¸¡ ëŒ€ìƒ: {len(city_sku_combinations)}ê°œ ì¡°í•© Ã— {len(future_dates)}ì¼ = {len(city_sku_combinations) * len(future_dates):,}ê°œ", start_time)
    
    # ê° city-sku ì¡°í•©ë³„ë¡œ ì˜ˆì¸¡
    for idx, (_, combo) in enumerate(tqdm(city_sku_combinations.iterrows(), total=len(city_sku_combinations), desc="ì˜ˆì¸¡ ì§„í–‰")):
        city, sku = combo['city'], combo['sku']
        
        # í•´ë‹¹ ì¡°í•©ì˜ ìµœê·¼ ë°ì´í„° (í”¼ì²˜ ìƒì„±ìš©)
        recent_data = demand_data[(demand_data['city'] == city) & (demand_data['sku'] == sku)].tail(60)
        
        if len(recent_data) == 0:
            continue
        
        # demand ë²„í¼ (ì˜¤í† ë ˆê·¸ë ˆì‹œë¸Œ ì—…ë°ì´íŠ¸ìš©)
        demand_buffer = recent_data['demand'].dropna().tolist()
        if len(demand_buffer) == 0:
            demand_buffer = [0.0]
        # ë””ë²„ê¹…: ì´ˆê¸° ë²„í¼ ìƒíƒœ
        if idx < 3:
            print(f"[DBG] {city}-{sku} initial demand_buffer (last 5): {demand_buffer[-5:]}  size={len(demand_buffer)}")
        
        # ë¯¸ë˜ ë°ì´í„° ìƒì„±
        for date in future_dates:
            # ê¸°ë³¸ í”¼ì²˜ ìƒì„±
            future_row = {
                'date': date,
                'city': city,
                'sku': sku,
                'month': date.month,
                'dayofyear': date.dayofyear,
                'weekday': date.weekday(),
            }

            # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ê°’ ì„¤ì • (ìµœê·¼ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            if len(recent_data) > 0:
                future_row['country'] = recent_data['country'].iloc[0]
                future_row['family'] = recent_data['family'].iloc[0]
                future_row['season'] = recent_data['season'].iloc[0]
            else:
                first_combo = demand_data[(demand_data['city'] == city) & (demand_data['sku'] == sku)]
                if len(first_combo) > 0:
                    future_row['country'] = first_combo['country'].iloc[0]
                    future_row['family'] = first_combo['family'].iloc[0]
                    future_row['season'] = first_combo['season'].iloc[0]
                else:
                    future_row['country'] = demand_data['country'].iloc[0]
                    future_row['family'] = demand_data['family'].iloc[0]
                    future_row['season'] = demand_data['season'].iloc[0]

            # ì™¸ìƒ/ë³´ì¡° í”¼ì²˜ëŠ” ì‹¤ì œ 2018~2024 í…Œì´ë¸”ì—ì„œ ë‹¹ì¼ ê°’ì„ ì¡°íšŒí•˜ì—¬ ì‚¬ìš©
            # countryê°€ ì •í•´ì§„ ì´í›„ì—ë§Œ ê°€ëŠ¥
            lookup_country = future_row['country'] if 'country' in future_row else None
            # country/date ì¡°ì¸ê°’
            cov_row = cov_future[(cov_future['date'] == date) & (cov_future['country'] == lookup_country)]
            if len(cov_row) > 0:
                cov_row = cov_row.iloc[0]
                future_row['confidence_index'] = cov_row.get('confidence_index', 0)
                future_row['spend_usd'] = cov_row.get('spend_usd', 0)
                future_row['avg_temp'] = cov_row.get('avg_temp', 0)
                future_row['humidity'] = cov_row.get('humidity', 0)
                future_row['season'] = cov_row.get('season', future_row.get('season', ''))
            else:
                # fallback: 0
                for col in ['confidence_index', 'spend_usd', 'avg_temp', 'humidity']:
                    future_row[col] = 0

            # ë‚ ì§œ ê³µë³€ëŸ‰
            cov_date_row = cov_date[cov_date['date'] == date]
            if len(cov_date_row) > 0:
                cov_date_row = cov_date_row.iloc[0]
                future_row['brent_usd'] = cov_date_row.get('brent_usd', 0)
                future_row['pct_change'] = cov_date_row.get('pct_change', 0)
                future_row['volatility_7d'] = cov_date_row.get('volatility_7d', 0)
                for fx in fx_cols:
                    future_row[fx] = cov_date_row.get(fx, 0)
            else:
                for col in ['brent_usd', 'pct_change', 'volatility_7d'] + fx_cols:
                    future_row[col] = 0

            # days_since_launch
            if sku in sku_meta['sku'].values:
                sku_launch_date = sku_meta[sku_meta['sku'] == sku]['launch_date'].iloc[0]
                if pd.notna(sku_launch_date):
                    future_row['days_since_launch'] = (date - sku_launch_date).days
                    future_row['days_since_launch'] = max(0, future_row['days_since_launch'])
                else:
                    print(f"sku {sku} has no launch date")
                    future_row['days_since_launch'] = 0
            else:
                future_row['days_since_launch'] = 0
                print(f"sku {sku} not found in sku_meta")

            # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”©
            for col in ['city', 'sku', 'country', 'family', 'season']:
                if col in label_encoders:
                    value_to_encode = str(future_row[col])
                    if value_to_encode not in label_encoders[col].classes_ and len(label_encoders[col].classes_) > 0:
                        value_to_encode = str(label_encoders[col].classes_[0])
                    future_row[f'{col}_encoded'] = label_encoders[col].transform([value_to_encode])[0]

            # is_event ì„¤ì • (í•˜ë“œì½”ë”©ëœ ì´ë²¤íŠ¸ ê¸°ê°„ ì‚¬ìš©)
            event_periods = get_hardcoded_event_periods()
            is_event = 0
            for (country, year), (start_date, end_date) in event_periods.items():
                if (
                    country == future_row['country']
                    and future_row['date'] >= pd.to_datetime(start_date)
                    and future_row['date'] <= pd.to_datetime(end_date)
                ):
                    is_event = 1
                    break
            future_row['is_event'] = is_event

            # ìˆ˜ìš”(demand) ì‹œê³„ì—´ í”¼ì²˜ëŠ” ì§ì „ ê°’ ë²„í¼ì—ì„œ ìƒì„± (ì˜¤í† ë ˆê·¸ë ˆì‹œë¸Œ)
            for lag in [1, 3, 7, 14]:
                if len(demand_buffer) >= lag:
                    future_row[f'demand_lag_{lag}'] = demand_buffer[-lag]
                else:
                    future_row[f'demand_lag_{lag}'] = demand_buffer[0] if len(demand_buffer) > 0 else 0
            for window in [7, 14]:
                if len(demand_buffer) > 0:
                    series = demand_buffer[-window:] if len(demand_buffer) >= window else demand_buffer
                    future_row[f'demand_rolling_mean_{window}'] = float(np.mean(series))
                    future_row[f'demand_rolling_std_{window}'] = float(np.std(series, ddof=1)) if len(series) > 1 else 0.0
                else:
                    future_row[f'demand_rolling_mean_{window}'] = 0.0
                    future_row[f'demand_rolling_std_{window}'] = 0.0

            # í• ì¸/ê°€ê²© ê¸°ë³¸ í”¼ì²˜ëŠ” 0 ëŒ€ì‹  ìµœê·¼ê°’ìœ¼ë¡œ ì±„ì›€ + ì‹œê³„ì—´ ìœ ì§€
            # ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ê°’)
            if 'unit_price' in recent_data.columns and len(recent_data) > 0:
                future_row['unit_price'] = float(recent_data['unit_price'].ffill().iloc[-1])
            if 'discount_pct' in recent_data.columns and len(recent_data) > 0:
                future_row['discount_pct'] = float(recent_data['discount_pct'].ffill().iloc[-1])

            for col in ['discount_pct', 'unit_price']:
                if col in recent_data.columns:
                    for lag in [1, 3, 7, 14]:
                        if len(recent_data) >= lag:
                            future_row[f'{col}_lag_{lag}'] = recent_data[col].iloc[-lag]
                        else:
                            future_row[f'{col}_lag_{lag}'] = recent_data[col].iloc[0] if len(recent_data) > 0 else 0
                    for window in [7, 14]:
                        vals = recent_data[col].tail(window)
                        future_row[f'{col}_rolling_mean_{window}'] = vals.mean() if len(vals) > 0 else (recent_data[col].mean() if len(recent_data) > 0 else 0)
                        # stdëŠ” ì œê±°ëœ ê²½ìš°ê°€ ë§ì•„ ìƒì„± ìƒëµ

            # spend_usdëŠ” 2018~2024 ì‹¤ì œ ê°’ ì¡´ì¬ â†’ ë‹¹ì¼ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œê°’ ê¸°ë°˜ lag/rolling ìƒì„±
            if lookup_country is not None:
                for lag in [1, 3, 7, 14]:
                    lag_date = date - pd.Timedelta(days=lag)
                    v = cov_future.loc[(cov_future['date'] == lag_date) & (cov_future['country'] == lookup_country), 'spend_usd']
                    future_row[f'spend_usd_lag_{lag}'] = float(v.iloc[0]) if len(v) > 0 else 0.0
                for window in [7, 14]:
                    start_d = date - pd.Timedelta(days=window-1)
                    mask = (cov_future['country'] == lookup_country) & (cov_future['date'] >= start_d) & (cov_future['date'] <= date)
                    vals = cov_future.loc[mask, 'spend_usd']
                    future_row[f'spend_usd_rolling_mean_{window}'] = float(vals.mean()) if len(vals) > 0 else 0.0

            # ë‹¨ì¼ í–‰ ì˜ˆì¸¡ ìˆ˜í–‰
            row_df = pd.DataFrame([future_row])
            # ëˆ„ë½ëœ í”¼ì²˜ ë³´ì •
            for col in feature_cols:
                if col not in row_df.columns:
                    row_df[col] = 0
            X_row = row_df[feature_cols]
            # ë””ë²„ê¹…: ì²« 2ê°œ ì¡°í•©, ì²« ì¼ì£¼ì¼ì€ í•µì‹¬ í”¼ì²˜ ë¡œê·¸
            if idx < 2 and date <= pd.Timestamp('2023-01-07'):
                check_cols = [c for c in feature_cols if (
                    c.startswith('demand_lag_') or c.startswith('demand_rolling_mean_') or c.startswith('demand_rolling_std_') or
                    c.startswith('unit_price') or c.startswith('discount_pct') or c.startswith('spend_usd')
                )]
                snap = X_row[check_cols].iloc[0]
                nz_ratio = (snap.replace(0, np.nan).notna().mean()) if len(snap) else 0
                print(f"[DBG] {city}-{sku} {date.date()} nz_ratio={nz_ratio:.2f} demand_lag_1={snap.get('demand_lag_1', np.nan)} spend_usd={snap.get('spend_usd', np.nan)} unit_price={snap.get('unit_price', np.nan)} discount_pct={snap.get('discount_pct', np.nan)}")
            pred_demand = float(model.predict(X_row)[0])
            
            # ì˜ˆì¸¡ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ í›„ ì •ìˆ˜ë¡œ ë³€í™˜ (ìŒìˆ˜ ë°©ì§€)
            pred_demand = int(max(0, round(pred_demand)))

            # ê²°ê³¼ ì €ì¥ ë° ë²„í¼ ì—…ë°ì´íŠ¸
            future_row['mean'] = pred_demand
            result.append(future_row)
            # demand ë²„í¼ì—ëŠ” ì˜ˆì¸¡ê°’ ì¶”ê°€
            demand_buffer.append(pred_demand)
            if len(demand_buffer) > 60:
                demand_buffer = demand_buffer[-60:]
    
    # ê²°ê³¼ ìƒì„±
    result_df = pd.DataFrame(result)[['sku', 'city', 'date', 'mean']]
    
    # ë””ë²„ê¹…: ì˜ˆì¸¡ê°’ ë¶„í¬ í™•ì¸
    print_progress(f"ğŸ“Š ì˜ˆì¸¡ê°’ í†µê³„ - í‰ê· : {result_df['mean'].mean():.2f}, ì¤‘ì•™ê°’: {result_df['mean'].median():.2f}, ìµœëŒ€: {result_df['mean'].max():.2f}, ìµœì†Œ: {result_df['mean'].min():.2f}", start_time)
    print_progress(f"ğŸ“Š 0ì´ ì•„ë‹Œ ì˜ˆì¸¡ê°’ ë¹„ìœ¨: {(result_df['mean'] > 0).mean():.3f}", start_time)
    
    return result_df

def generate_lightgbm_forecast():
    """LightGBM ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡ ìƒì„±"""
    print_progress("=== LightGBM ëª¨ë¸ ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡ ìƒì„± ===")
    total_start_time = time.time()
    
    # 1. ë°ì´í„° ë¡œë“œ
    demand_data, events_df, label_encoders = load_enhanced_training_data()

    # 2018~2024 í™•ì • ì´ë²¤íŠ¸ êµ¬ê°„ ì „ì²´ í”„ë¦°íŠ¸
    if events_df is not None and len(events_df) > 0:
        print("\nğŸ“¢ í™•ì •ëœ ì´ë²¤íŠ¸ êµ¬ê°„ ëª©ë¡ (2018~2024):")
        events_df_sorted = events_df.sort_values(['year','country','start_date']) if 'year' in events_df.columns else events_df
        for _, ev in events_df_sorted.iterrows():
            yr = ev['start_date'].year
            if 2018 <= yr <= 2024:
                print(f"  - {ev['country']} | {ev['start_date'].date()} ~ {ev['end_date'].date()} (year={yr})")
    
    # 2. í”¼ì²˜ ì¤€ë¹„
    demand_data, feature_cols = prepare_lightgbm_features(demand_data)
    
    # 3. ë°ì´í„° ë¶„í•  (ì¦ê°• ì „ì— ë¶„í• )
    print_progress("ğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...", total_start_time)
    train_data = demand_data[demand_data['date'] < '2022-01-01'].copy()
    val_data = demand_data[(demand_data['date'] >= '2022-01-01') & (demand_data['date'] < '2023-01-01')].copy()
    
    # 3.5. í›ˆë ¨ ë°ì´í„°ì—ë§Œ ì¦ê°• ì ìš© (ê²€ì¦ ë°ì´í„°ëŠ” ì›ë³¸ ìœ ì§€)
    #print_progress("ğŸ”„ í›ˆë ¨ ë°ì´í„° ì¦ê°• ë‹¨ê³„ ì‹œì‘...", total_start_time)
    #train_data = augment_event_data(train_data, events_df, augmentation_factor=3)ã„´ã„´
    
    print_progress(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ - Train: {len(train_data):,}ê°œ, Val: {len(val_data):,}ê°œ", total_start_time)
    
    # 4. ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„
    print_progress("ğŸ” ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì¤‘...", total_start_time)
    X_train_sample = train_data[feature_cols].sample(n=min(10000, len(train_data)), random_state=42)  # ìƒ˜í”Œë§ìœ¼ë¡œ ì†ë„ í–¥ìƒ
    vif_df, high_corr_pairs = analyze_multicollinearity(X_train_sample, feature_cols, total_start_time)
    
    # 5. ëª¨ë¸ í•™ìŠµ
    model = train_lightgbm_model(train_data, val_data, feature_cols, total_start_time)
    
    # 6. ê²€ì¦ ì„±ëŠ¥ í‰ê°€
    print_progress("ğŸ“ˆ ê²€ì¦ ì„±ëŠ¥ í‰ê°€ ì¤‘...", total_start_time)
        
    X_val = val_data[feature_cols]
    y_val = val_data['demand']
    val_pred = model.predict(X_val)
    y_val_original = y_val
    
    # ì„±ëŠ¥ ê³„ì‚° (ì›ë˜ ìŠ¤ì¼€ì¼ ê¸°ì¤€)
    val_rmse = np.sqrt(mean_squared_error(y_val_original, val_pred))
    val_r2 = r2_score(y_val_original, val_pred)
    
    print_progress(f"ğŸ“Š ê²€ì¦ ì„±ëŠ¥ - RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}", total_start_time)
    
    # 7. 2022ë…„ ê²€ì¦ ì‹œê°í™”
    print_progress("ğŸ“Š 2022ë…„ ê²€ì¦ ì‹œê°í™” ìƒì„± ì¤‘...", total_start_time)
    
    # ë””ë²„ê¹…: 2022ë…„ ì´ë²¤íŠ¸ ê¸°ê°„ í™•ì¸
    print_progress("ğŸ” ë””ë²„ê¹…: 2022ë…„ ì´ë²¤íŠ¸ ê¸°ê°„ í™•ì¸ ì¤‘...", total_start_time)
    val_events = val_data[val_data['is_event'] == 1]
    if not val_events.empty:
        print(f"  - 2022ë…„ ì´ë²¤íŠ¸ ê¸°ê°„: {len(val_events):,}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
        print(f"  - ì´ë²¤íŠ¸ ê¸°ê°„ ë‚ ì§œ ë²”ìœ„: {val_events['date'].min().date()} ~ {val_events['date'].max().date()}")
        print(f"  - ì´ë²¤íŠ¸ ê¸°ê°„ í‰ê·  ìˆ˜ìš”: {val_events['demand'].mean():.1f}")
    else:
        print("  - âš ï¸ 2022ë…„ì— ì´ë²¤íŠ¸ ê¸°ê°„ì´ ì—†ìŒ!")
    
    create_lightgbm_validation_visualization(val_data, val_pred, total_start_time)
    
    # 8. ë¯¸ë˜ ì˜ˆì¸¡
    print_progress("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘...", total_start_time)
    result_df = predict_future_lightgbm(model, demand_data, feature_cols, label_encoders, total_start_time, events_df=events_df)
    
    # 9. ê²°ê³¼ ì €ì¥
    output_path = DATA_DIR / "lightgbm_forecast_submission.csv"
    result_df.to_csv(output_path, index=False)
    
    print_progress(f"âœ… LightGBM ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ: {time.time() - total_start_time:.1f}ì´ˆ", total_start_time)
    print_progress(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_path}", total_start_time)
    print_progress(f"ğŸ“Š ì´ ì˜ˆì¸¡ ìˆ˜: {len(result_df):,}", total_start_time)
    print_progress(f"ğŸ“ˆ í‰ê·  ìˆ˜ìš”: {result_df['mean'].mean():.1f}", total_start_time)
    print_progress(f"ğŸ“Š ê²€ì¦ RMSE: {val_rmse:.4f}", total_start_time)
    
    # 10. 2018-2024 ì „ì²´ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œê°í™”
    print_progress("ğŸ“Š 2018-2024 ì „ì²´ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œê°í™” ìƒì„± ì¤‘...", total_start_time)
    create_lightgbm_full_timeline_visualization(demand_data, result_df, total_start_time)
    
    return result_df

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print_progress("=== LightGBM ëª¨ë¸ ê¸°ë°˜ ê³ ê¸‰ ì‹œê³„ì—´ ì˜ˆì¸¡ ===")
    result_df = generate_lightgbm_forecast()
    print_progress("âœ… LightGBM ëª¨ë¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 