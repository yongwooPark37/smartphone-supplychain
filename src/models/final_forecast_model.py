# src/models/final_forecast_model.py
# EDA ê¸°ë°˜ ê³ ê¸‰ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ - ì¶œì œì ì ‘ê·¼ë²• ë°˜ì˜

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ê²½ë¡œ ì„¤ì •
SCRIPT = Path(__file__).resolve()
PROJECT_ROOT = SCRIPT.parents[2]
DATA_DIR = PROJECT_ROOT / "data"

def get_country_mapping():
    """êµ­ê°€ ë§¤í•‘"""
    return {
        'Washington_DC':'USA','New_York':'USA','Chicago':'USA','Dallas':'USA',
        'Berlin':'DEU','Munich':'DEU','Frankfurt':'DEU','Hamburg':'DEU',
        'Paris':'FRA','Lyon':'FRA','Marseille':'FRA','Toulouse':'FRA',
        'Seoul':'KOR','Busan':'KOR','Incheon':'KOR','Gwangju':'KOR',
        'Tokyo':'JPN','Osaka':'JPN','Nagoya':'JPN','Fukuoka':'JPN',
        'Manchester':'GBR','London':'GBR','Birmingham':'GBR','Glasgow':'GBR',
        'Ottawa':'CAN','Toronto':'CAN','Vancouver':'CAN','Montreal':'CAN',
        'Canberra':'AUS','Sydney':'AUS','Melbourne':'AUS','Brisbane':'AUS',
        'Brasilia':'BRA','Sao_Paulo':'BRA','Rio_de_Janeiro':'BRA','Salvador':'BRA',
        'Pretoria':'ZAF','Johannesburg':'ZAF','Cape_Town':'ZAF','Durban':'ZAF'
    }

def create_global_confidence_factor(consumer_conf):
    """ê¸€ë¡œë²Œ ì‹ ë¢°ì§€ìˆ˜ ìš”ì¸ ìƒì„± (ì¶œì œì ë°©ì‹)"""
    # í”¼ë²— í…Œì´ë¸” ìƒì„±
    wide = consumer_conf.pivot(index="month", columns="country", values="confidence_index").sort_index()
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    wide = wide.fillna(method='ffill')
    
    # í‘œì¤€í™”
    scaler = StandardScaler()
    Z = scaler.fit_transform(wide)
    
    # PCAë¡œ ê¸€ë¡œë²Œ ìš”ì¸ ì¶”ì¶œ
    pca = PCA(n_components=2)
    global_factors = pca.fit_transform(Z)
    
    # ê¸€ë¡œë²Œ ìš”ì¸ì„ ì‹œê³„ì—´ë¡œ ë³€í™˜
    global_factor_df = pd.DataFrame({
        'year_month': wide.index,
        'global_factor_1': global_factors[:, 0],
        'global_factor_2': global_factors[:, 1]
    })
    
    return global_factor_df, pca, scaler

def detect_events_using_zscore(demand, threshold=2.0):
    """Z-score ê¸°ë°˜ ì´ë²¤íŠ¸ íƒì§€ (ì¶œì œì ë°©ì‹)"""
    # êµ­ê°€ë³„ ì›”ë³„ ìˆ˜ìš” ì§‘ê³„
    demand['year_month'] = demand['date'].dt.to_period('M')
    monthly_country_demand = demand.groupby(['country', 'year_month'])['demand'].sum().reset_index()
    
    events_detected = []
    
    for country in monthly_country_demand['country'].unique():
        country_data = monthly_country_demand[monthly_country_demand['country'] == country].copy()
        country_data = country_data.sort_values('year_month')
        
        # ì´ë™ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
        country_data['demand_mean'] = country_data['demand'].rolling(window=12, min_periods=1).mean()
        country_data['demand_std'] = country_data['demand'].rolling(window=12, min_periods=1).std()
        country_data['z_score'] = (country_data['demand'] - country_data['demand_mean']) / country_data['demand_std']
        
        # ì´ë²¤íŠ¸ ê°ì§€
        events = country_data[country_data['z_score'] > threshold]
        
        for _, event in events.iterrows():
            events_detected.append({
                'country': country,
                'date': event['year_month'],
                'demand': event['demand'],
                'z_score': event['z_score'],
                'normal_demand': event['demand_mean'],
                'multiplier': event['demand'] / event['demand_mean']
            })
    
    return pd.DataFrame(events_detected)

def load_enhanced_training_data():
    """EDA ê¸°ë°˜ ê³ ê¸‰ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    print("=== EDA ê¸°ë°˜ ê³ ê¸‰ ë°ì´í„° ë¡œë“œ ===")
    
    # 1. ìˆ˜ìš” ë°ì´í„°
    conn = sqlite3.connect(DATA_DIR / "demand_train.db")
    demand = pd.read_sql("SELECT * FROM demand_train", conn, parse_dates=['date'])
    conn.close()
    
    # êµ­ê°€ ë§¤í•‘ ì¶”ê°€
    country_map = get_country_mapping()
    demand["country"] = demand["city"].map(country_map)
    
    # 2. ì™¸ë¶€ ë°ì´í„° ë¡œë“œ
    oil = pd.read_csv(DATA_DIR / "oil_price.csv", parse_dates=["date"])
    currency = pd.read_csv(DATA_DIR / "currency.csv", parse_dates=["Date"])
    currency = currency.rename(columns={"Date": "date"})
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence.csv", parse_dates=["month"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])
    ppt = pd.read_csv(DATA_DIR / "price_promo_train.csv", parse_dates=["date"])
    
    # 3. ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
    demand["year"] = demand["date"].dt.year
    demand["month"] = demand["date"].dt.month
    demand["dayofyear"] = demand["date"].dt.dayofyear
    demand["weekday"] = demand["date"].dt.weekday
    demand["quarter"] = demand["date"].dt.quarter
    
    # 4. ê³„ì ˆì„± í”¼ì²˜ (EDAì—ì„œ ë°œê²¬: 9ì›” ìµœê³ ì , 1ì›” ìµœì €ì )
    demand['month_sin'] = np.sin(2 * np.pi * demand['month'] / 12)
    demand['month_cos'] = np.cos(2 * np.pi * demand['month'] / 12)
    demand['dayofyear_sin'] = np.sin(2 * np.pi * demand['dayofyear'] / 365)
    demand['dayofyear_cos'] = np.cos(2 * np.pi * demand['dayofyear'] / 365)
    
    # 5. ê³„ì ˆ ì •ë³´ ì¶”ê°€
    demand = demand.merge(calendar[["date", "country", "season"]], on=["date", "country"], how="left")
    
    # 6. SKU ë©”íƒ€ ì •ë³´
    demand = demand.merge(sku_meta[["sku", "family", "storage_gb", "launch_date"]], on="sku", how="left")
    demand["days_since_launch"] = (demand["date"] - demand["launch_date"]).dt.days.clip(lower=0)
    
    # 7. ì‹œê³„ì—´ í”¼ì²˜ (EDA ê¸°ë°˜ ìµœì í™”)
    demand = demand.sort_values(["city", "sku", "date"])
    
    # ë‹¤ì–‘í•œ ë™ í”¼ì²˜ (EDAì—ì„œ ì¤‘ìš”ë„ í™•ì¸)
    for lag in [1, 3, 7, 14, 30]:
        demand[f"demand_lag_{lag}"] = demand.groupby(["city", "sku"])["demand"].shift(lag).fillna(0)
    
    # ë¡¤ë§ í†µê³„
    for window in [7, 14, 30]:
        demand[f"demand_rolling_mean_{window}"] = demand.groupby(["city", "sku"])["demand"].transform(
            lambda x: x.rolling(window=window, min_periods=1).mean()
        )
        demand[f"demand_rolling_std_{window}"] = demand.groupby(["city", "sku"])["demand"].transform(
            lambda x: x.rolling(window=window, min_periods=1).std()
        )
    
    # 8. ì™¸ë¶€ ìš”ì¸ ì¶”ê°€
    # ìœ ê°€ ë°ì´í„°
    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    demand = demand.merge(oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']], on='date', how='left')
    
    # í™˜ìœ¨ ë°ì´í„° (ì£¼ìš” í™˜ìœ¨ë§Œ)
    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']
    demand = demand.merge(currency[['date'] + fx_cols], on='date', how='left')
    
    # ì†Œë¹„ìì‹ ë¢°ì§€ìˆ˜
    consumer_conf['year_month'] = consumer_conf['month'].dt.to_period('M')
    demand['year_month'] = demand['date'].dt.to_period('M')
    demand = demand.merge(consumer_conf[['year_month', 'country', 'confidence_index']], 
                         on=['year_month', 'country'], how='left')
    
    # ë§ˆì¼€íŒ… ì§€ì¶œ
    demand = demand.merge(marketing[['date', 'country', 'spend_usd']], on=['date', 'country'], how='left')
    
    # ë‚ ì”¨ ë°ì´í„°
    demand = demand.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how='left')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    demand = demand.fillna(0)
    
    # 9. ê¸€ë¡œë²Œ ì‹ ë¢°ì§€ìˆ˜ ìš”ì¸ ìƒì„± (ì¶œì œì ë°©ì‹)
    global_factor_df, pca, scaler = create_global_confidence_factor(consumer_conf)
    demand = demand.merge(global_factor_df, on='year_month', how='left')
    
    # 10. ê°€ê²© ì •ë³´ ì¶”ê°€
    demand = demand.merge(ppt[['date', 'sku', 'city', 'unit_price', 'discount_pct']], 
                         on=['date', 'sku', 'city'], how='left')
    
    # 11. ì§‘ê³„ í”¼ì²˜ (EDA ê¸°ë°˜)
    # ë„ì‹œë³„ í‰ê· 
    city_avg = demand.groupby('city')['demand'].mean().reset_index()
    city_avg = city_avg.rename(columns={'demand': 'city_avg_demand'})
    demand = demand.merge(city_avg, on='city', how='left')
    
    # SKUë³„ í‰ê· 
    sku_avg = demand.groupby('sku')['demand'].mean().reset_index()
    sku_avg = sku_avg.rename(columns={'demand': 'sku_avg_demand'})
    demand = demand.merge(sku_avg, on='sku', how='left')
    
    # êµ­ê°€ë³„ í‰ê· 
    country_avg = demand.groupby('country')['demand'].mean().reset_index()
    country_avg = country_avg.rename(columns={'demand': 'country_avg_demand'})
    demand = demand.merge(country_avg, on='country', how='left')
    
    # 12. ë³€ë™ì„± í”¼ì²˜
    demand['demand_volatility'] = demand.groupby(['city', 'sku'])['demand'].transform(
        lambda x: x.rolling(window=30, min_periods=1).std()
    )
    
    # 13. ì´ë²¤íŠ¸ íƒì§€ ë° í”¼ì²˜ ì¶”ê°€
    events_df = detect_events_using_zscore(demand, threshold=2.0)
    
    # ì´ë²¤íŠ¸ í”Œë˜ê·¸ ì¶”ê°€
    demand['is_event_month'] = 0
    demand['event_multiplier'] = 1.0
    if len(events_df) > 0:
        for _, event in events_df.iterrows():
            mask = (demand['country'] == event['country']) & (demand['year_month'] == event['date'])
            demand.loc[mask, 'is_event_month'] = 1
            demand.loc[mask, 'event_multiplier'] = event['multiplier']
    
    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {demand.shape}")
    print(f"ì´ë²¤íŠ¸ ê°ì§€: {len(events_df)}ê°œ")
    
    return demand, events_df, pca, scaler

def train_enhanced_ensemble_model(train_data):
    """EDA ê¸°ë°˜ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
    print("=== EDA ê¸°ë°˜ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ===")
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    train_mask = train_data['year'] <= 2021
    val_mask = train_data['year'] == 2022
    
    train_set = train_data[train_mask].copy()
    val_set = train_data[val_mask].copy()
    
    # í”¼ì²˜ ì„ íƒ (EDA ê¸°ë°˜ ìµœì í™”)
    feature_cols = [
        # ì‹œê°„ í”¼ì²˜
        'month', 'weekday', 'quarter', 'dayofyear',
        'month_sin', 'month_cos', 'dayofyear_sin', 'dayofyear_cos',
        
        # SKU í”¼ì²˜
        'days_since_launch', 'storage_gb',
        
        # ì‹œê³„ì—´ í”¼ì²˜
        'demand_lag_1', 'demand_lag_3', 'demand_lag_7', 'demand_lag_14', 'demand_lag_30',
        'demand_rolling_mean_7', 'demand_rolling_mean_14', 'demand_rolling_mean_30',
        'demand_rolling_std_7', 'demand_rolling_std_14', 'demand_rolling_std_30',
        
        # ì™¸ë¶€ ìš”ì¸
        'brent_usd', 'pct_change', 'volatility_7d',
        'confidence_index', 'spend_usd', 'avg_temp', 'humidity',
        'global_factor_1', 'global_factor_2',
        
        # ê°€ê²© ì •ë³´
        'unit_price', 'discount_pct',
        
        # ì§‘ê³„ í”¼ì²˜
        'city_avg_demand', 'sku_avg_demand', 'country_avg_demand',
        
        # ì´ë²¤íŠ¸ í”¼ì²˜
        'is_event_month', 'event_multiplier',
        
        # ë³€ë™ì„±
        'demand_volatility'
    ]
    
    # í™˜ìœ¨ í”¼ì²˜ ì¶”ê°€
    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']
    feature_cols.extend(fx_cols)
    
    # ë²”ì£¼í˜• ì¸ì½”ë”©
    label_encoders = {}
    categorical_cols = ['city', 'sku', 'country', 'family', 'season']
    
    for col in categorical_cols:
        le = LabelEncoder()
        train_set[col + '_encoded'] = le.fit_transform(train_set[col].astype(str))
        val_set[col + '_encoded'] = le.transform(val_set[col].astype(str))
        label_encoders[col] = le
        feature_cols.append(col + '_encoded')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in feature_cols:
        if col in train_set.columns:
            train_set[col] = train_set[col].fillna(0)
            val_set[col] = val_set[col].fillna(0)
    
    # ìˆ˜ì¹˜í˜• ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    numeric_features = [col for col in feature_cols if col not in [col + '_encoded' for col in categorical_cols]]
    
    X_train = train_set[feature_cols]
    X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
    X_val = val_set[feature_cols]
    X_val[numeric_features] = scaler.transform(X_val[numeric_features])
    
    y_train = train_set['demand']
    y_val = val_set['demand']
    
    # 3ê°€ì§€ ëª¨ë¸ í•™ìŠµ (EDA ê¸°ë°˜ ìµœì í™”)
    models = {
        'random_forest': RandomForestRegressor(
            n_estimators=200, max_depth=15, min_samples_split=10, 
            min_samples_leaf=5, random_state=42, n_jobs=-1
        ),
        'gradient_boosting': GradientBoostingRegressor(
            n_estimators=150, max_depth=8, learning_rate=0.1, 
            subsample=0.8, random_state=42
        ),
        'linear_regression': LinearRegression()
    }
    
    trained_models = {}
    predictions = {}
    
    for name, model in models.items():
        print(f"í•™ìŠµ ì¤‘: {name}")
        model.fit(X_train, y_train)
        trained_models[name] = model
        
        # ê²€ì¦ ì˜ˆì¸¡
        pred = model.predict(X_val)
        predictions[name] = pred
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
        rmse = np.sqrt(mean_squared_error(y_val, pred))
        r2 = r2_score(y_val, pred)
        print(f"  {name} - RMSE: {rmse:.2f}, RÂ²: {r2:.3f}")
    
    # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
    weights = {}
    total_score = 0
    for name, pred in predictions.items():
        rmse = np.sqrt(mean_squared_error(y_val, pred))
        score = 1 / (1 + rmse)  # RMSEê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        weights[name] = score
        total_score += score
    
    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    for name in weights:
        weights[name] /= total_score
    
    print(f"\në™ì  ê°€ì¤‘ì¹˜: {weights}")
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    ensemble_pred = np.zeros(len(y_val))
    for name, pred in predictions.items():
        ensemble_pred += weights[name] * pred
    
    # ì•™ìƒë¸” ì„±ëŠ¥
    ensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))
    ensemble_r2 = r2_score(y_val, ensemble_pred)
    print(f"\nì•™ìƒë¸” - RMSE: {ensemble_rmse:.2f}, RÂ²: {ensemble_r2:.3f}")
    
    # 2022 ê²€ì¦ ê²°ê³¼ ì‹œê°í™”
    visualize_2022_validation(val_set, ensemble_pred, ensemble_r2)
    
    return trained_models, label_encoders, scaler, feature_cols, weights

def visualize_2022_validation(val_set, val_pred, r2_score):
    """2022 ê²€ì¦ ê²°ê³¼ ì‹œê°í™”"""
    print("=== 2022 ê²€ì¦ ê²°ê³¼ ì‹œê°í™” ===")
    
    # val_predë¥¼ val_setê³¼ ê°™ì€ ìˆœì„œë¡œ ì •ë ¬
    val_set_with_pred = val_set.copy()
    val_set_with_pred["predicted"] = val_pred
    
    # ì¼ë³„ ì´ ìˆ˜ìš” ì§‘ê³„
    daily_actual = val_set_with_pred.groupby("date")["demand"].sum().reset_index()
    daily_pred = val_set_with_pred.groupby("date")["predicted"].sum().reset_index()
    
    # ë°ì´í„° ë³‘í•©
    comparison = daily_actual.merge(daily_pred, on="date", suffixes=("", "_pred"))
    comparison = comparison.rename(columns={"predicted": "predicted"})
    
    # ì‹œê°í™”
    plt.figure(figsize=(15, 8))
    plt.plot(comparison["date"], comparison["demand"], label="Actual", alpha=0.8, linewidth=2, color='blue')
    plt.plot(comparison["date"], comparison["predicted"], label="Predicted", alpha=0.8, linewidth=2, color='red')
    plt.title(f"2022 Daily Demand: Actual vs Predicted (RÂ² = {r2_score:.3f})")
    plt.xlabel("Date")
    plt.ylabel("Daily Total Demand")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # ì„±ëŠ¥ ë¶„ì„
    print(f"ì‹¤ì œ í‰ê· : {comparison['demand'].mean():.1f}")
    print(f"ì˜ˆì¸¡ í‰ê· : {comparison['predicted'].mean():.1f}")
    print(f"í¸í–¥: {comparison['predicted'].mean() - comparison['demand'].mean():.1f}")

def generate_enhanced_forecast():
    """EDA ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡ ìƒì„±"""
    print("=== EDA ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡ ìƒì„± ===")
    
    # 1. ê³ ê¸‰ ë°ì´í„° ë¡œë“œ
    train_data, events_df, pca, scaler = load_enhanced_training_data()
    
    # 2. ê³ ê¸‰ ëª¨ë¸ í•™ìŠµ
    trained_models, label_encoders, scaler, feature_cols, weights = train_enhanced_ensemble_model(train_data)
    
    # 3. ë¯¸ë˜ ë°ì´í„° ìƒì„±
    future_dates = pd.date_range(start="2023-01-01", end="2024-12-31", freq="D")
    future_df = pd.DataFrame({"date": future_dates})
    
    # ê¸°ë³¸ í”¼ì²˜ ì¶”ê°€
    future_df["year"] = future_df["date"].dt.year
    future_df["month"] = future_df["date"].dt.month
    future_df["dayofyear"] = future_df["date"].dt.dayofyear
    future_df["weekday"] = future_df["date"].dt.weekday
    future_df["quarter"] = future_df["date"].dt.quarter
    
    # ê³„ì ˆì„± í”¼ì²˜
    future_df["month_sin"] = np.sin(2 * np.pi * future_df["month"] / 12)
    future_df["month_cos"] = np.cos(2 * np.pi * future_df["month"] / 12)
    future_df["dayofyear_sin"] = np.sin(2 * np.pi * future_df["dayofyear"] / 365)
    future_df["dayofyear_cos"] = np.cos(2 * np.pi * future_df["dayofyear"] / 365)
    
    # ì™¸ë¶€ ë°ì´í„° ë¡œë“œ
    oil = pd.read_csv(DATA_DIR / "oil_price.csv", parse_dates=["date"])
    currency = pd.read_csv(DATA_DIR / "currency.csv", parse_dates=["Date"])
    currency = currency.rename(columns={"Date": "date"})
    consumer_conf = pd.read_csv(DATA_DIR / "consumer_confidence.csv", parse_dates=["month"])
    marketing = pd.read_csv(DATA_DIR / "marketing_spend.csv", parse_dates=["date"])
    weather = pd.read_csv(DATA_DIR / "weather.csv", parse_dates=["date"])
    calendar = pd.read_csv(DATA_DIR / "calendar.csv", parse_dates=["date"])
    sku_meta = pd.read_csv(DATA_DIR / "sku_meta.csv", parse_dates=["launch_date"])
    ppt = pd.read_csv(DATA_DIR / "price_promo_train.csv", parse_dates=["date"])
    
    # ë¯¸ë˜ ë°ì´í„°ì— ì™¸ë¶€ ìš”ì¸ ì¶”ê°€
    oil['pct_change'] = oil['brent_usd'].pct_change()
    oil['volatility_7d'] = oil['pct_change'].rolling(7).std()
    future_df = future_df.merge(oil[['date', 'brent_usd', 'pct_change', 'volatility_7d']], on='date', how='left')
    
    fx_cols = ['EUR=X', 'KRW=X', 'JPY=X', 'GBP=X', 'CAD=X', 'AUD=X', 'BRL=X', 'ZAR=X']
    future_df = future_df.merge(currency[['date'] + fx_cols], on='date', how='left')
    
    future_df = future_df.merge(calendar[["date", "country", "season"]], on="date", how="left")
    future_df = future_df.merge(marketing[['date', 'country', 'spend_usd']], on=['date', 'country'], how='left')
    future_df = future_df.merge(weather[['date', 'country', 'avg_temp', 'humidity']], on=['date', 'country'], how='left')
    
    # ê¸€ë¡œë²Œ ìš”ì¸ ì¶”ê°€
    consumer_conf['year_month'] = consumer_conf['month'].dt.to_period('M')
    future_df['year_month'] = future_df['date'].dt.to_period('M')
    future_df = future_df.merge(consumer_conf[['year_month', 'country', 'confidence_index']], 
                               on=['year_month', 'country'], how='left')
    
    # ê¸€ë¡œë²Œ ìš”ì¸ ìƒì„±
    global_factor_df, _, _ = create_global_confidence_factor(consumer_conf)
    future_df = future_df.merge(global_factor_df, on='year_month', how='left')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    future_df = future_df.fillna(0)
    
    results = []
    
    # ê° ë„ì‹œ-SKU ì¡°í•©ì— ëŒ€í•´ ì˜ˆì¸¡
    cities = train_data["city"].unique()
    skus = train_data["sku"].unique()
    
    for city in cities:
        country = get_country_mapping()[city]
        
        for sku in skus:
            sku_info = sku_meta[sku_meta["sku"] == sku].iloc[0]
            
            # í•´ë‹¹ SKUì˜ ê³¼ê±° ë°ì´í„°
            sku_history = train_data[(train_data["city"] == city) & (train_data["sku"] == sku)].sort_values("date")
            
            if len(sku_history) == 0:
                continue
            
            # ì‹œê³„ì—´ ë°ì´í„° ì´ˆê¸°í™”
            demand_series = sku_history["demand"].tolist()
            
            # ì§‘ê³„ ê°’ë“¤ ê³„ì‚°
            city_avg = sku_history["demand"].mean() if len(sku_history) > 0 else 0
            sku_avg = train_data[train_data["sku"] == sku]["demand"].mean()
            country_avg = train_data[train_data["country"] == country]["demand"].mean()
            
            # ê° ë¯¸ë˜ ë‚ ì§œì— ëŒ€í•´ ë‹¨ê³„ë³„ ì˜ˆì¸¡
            for date in future_dates:
                # í”¼ì²˜ ìƒì„±
                date_features = future_df[future_df["date"] == date].copy()
                date_features["city"] = city
                date_features["country"] = country
                date_features["sku"] = sku
                date_features["family"] = sku_info["family"]
                date_features["storage_gb"] = sku_info["storage_gb"]
                date_features["launch_date"] = sku_info["launch_date"]
                date_features["days_since_launch"] = (date - date_features["launch_date"]).dt.days.clip(lower=0)
                
                # ì‹œê³„ì—´ í”¼ì²˜ ê³„ì‚°
                if len(demand_series) >= 30:
                    lag_1 = demand_series[-1]
                    lag_3 = demand_series[-3]
                    lag_7 = demand_series[-7]
                    lag_14 = demand_series[-14]
                    lag_30 = demand_series[-30]
                    rolling_mean_7 = np.mean(demand_series[-7:])
                    rolling_mean_14 = np.mean(demand_series[-14:])
                    rolling_mean_30 = np.mean(demand_series[-30:])
                    rolling_std_7 = np.std(demand_series[-7:])
                    rolling_std_14 = np.std(demand_series[-14:])
                    rolling_std_30 = np.std(demand_series[-30:])
                    volatility = np.std(demand_series[-30:])
                else:
                    recent_demand = demand_series[-1] if demand_series else 0
                    lag_1 = lag_3 = lag_7 = lag_14 = lag_30 = recent_demand
                    rolling_mean_7 = rolling_mean_14 = rolling_mean_30 = recent_demand
                    rolling_std_7 = rolling_std_14 = rolling_std_30 = 0
                    volatility = 0
                
                # í”¼ì²˜ ì„¤ì •
                date_features["demand_lag_1"] = lag_1
                date_features["demand_lag_3"] = lag_3
                date_features["demand_lag_7"] = lag_7
                date_features["demand_lag_14"] = lag_14
                date_features["demand_lag_30"] = lag_30
                date_features["demand_rolling_mean_7"] = rolling_mean_7
                date_features["demand_rolling_mean_14"] = rolling_mean_14
                date_features["demand_rolling_mean_30"] = rolling_mean_30
                date_features["demand_rolling_std_7"] = rolling_std_7
                date_features["demand_rolling_std_14"] = rolling_std_14
                date_features["demand_rolling_std_30"] = rolling_std_30
                date_features["city_avg_demand"] = city_avg
                date_features["sku_avg_demand"] = sku_avg
                date_features["country_avg_demand"] = country_avg
                date_features["demand_volatility"] = volatility
                
                # ê°€ê²© ì •ë³´ ì¶”ê°€
                sku_city_price = ppt[(ppt['sku'] == sku) & (ppt['city'] == city)]
                if len(sku_city_price) > 0:
                    date_features["unit_price"] = sku_city_price['unit_price'].mean()
                    date_features["discount_pct"] = sku_city_price['discount_pct'].mean()
                else:
                    date_features["unit_price"] = ppt['unit_price'].mean()
                    date_features["discount_pct"] = ppt['discount_pct'].mean()
                
                # ì´ë²¤íŠ¸ í™•ì¸
                date_features["is_event_month"] = 0
                date_features["event_multiplier"] = 1.0
                
                # ê³¼ê±° ì´ë²¤íŠ¸ íŒ¨í„´ ê¸°ë°˜ ë™ì  ë°°ìˆ˜
                for _, event in events_df.iterrows():
                    if event['country'] == country:
                        if date.month == event['date'].month:
                            date_features["is_event_month"] = 1
                            date_features["event_multiplier"] = event['multiplier']
                            break
                
                # ë²”ì£¼í˜• ì¸ì½”ë”©
                for col in ["city", "sku", "country", "family", "season"]:
                    le = label_encoders[col]
                    date_features[col + "_encoded"] = le.transform(date_features[col].astype(str))
                
                # ìŠ¤ì¼€ì¼ë§
                X_pred = date_features[feature_cols]
                numeric_features = [col for col in feature_cols if col not in [col + '_encoded' for col in ["city", "sku", "country", "family", "season"]]]
                X_pred[numeric_features] = scaler.transform(X_pred[numeric_features])
                
                # ì•™ìƒë¸” ì˜ˆì¸¡
                ensemble_pred = 0
                for name, model in trained_models.items():
                    pred = model.predict(X_pred)[0]
                    ensemble_pred += weights[name] * pred
                
                # ì´ë²¤íŠ¸ ë°°ìˆ˜ ì ìš©
                final_pred = int(max(ensemble_pred * date_features["event_multiplier"].iloc[0], 0))
                
                results.append({
                    "date": date.strftime("%Y-%m-%d"),
                    "sku": sku,
                    "city": city,
                    "mean": final_pred
                })
                
                # ì‹œê³„ì—´ ì—…ë°ì´íŠ¸
                demand_series.append(final_pred)
                if len(demand_series) > 100:  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
                    demand_series = demand_series[-100:]
    
    # ê²°ê³¼ ì €ì¥
    result_df = pd.DataFrame(results)
    output_path = DATA_DIR / "enhanced_forecast_submission.csv"
    result_df.to_csv(output_path, index=False)
    
    print(f"âœ… EDA ê¸°ë°˜ ê³ ê¸‰ ì˜ˆì¸¡ ì €ì¥: {output_path}")
    print(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(result_df):,}")
    print(f"í‰ê·  ìˆ˜ìš”: {result_df['mean'].mean():.1f}")
    print(f"ìµœëŒ€ ìˆ˜ìš”: {result_df['mean'].max():,}")
    print(f"ìµœì†Œ ìˆ˜ìš”: {result_df['mean'].min()}")
    
    return result_df

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=== EDA ê¸°ë°˜ ê³ ê¸‰ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ===\n")
    
    result_df = generate_enhanced_forecast()
    
    print(f"\nâœ… EDA ê¸°ë°˜ ê³ ê¸‰ ëª¨ë¸ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: enhanced_forecast_submission.csv")

if __name__ == "__main__":
    main() 